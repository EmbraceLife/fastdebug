# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/lib/groundup_002_get_data_ready.ipynb.

# %% auto 0
__all__ = ['test', 'test_eq', 'match_pct', 'search_data_url', 'check_data_directories', 'get_img_paths', 'get_labels', 'mean_std', 'normalize', 'imgs2tensor', 'get_exp_data', 'chunks', 'chunks_faster', 'Matrix', 'rand']

# %% ../nbs/lib/groundup_002_get_data_ready.ipynb 18
def test(a,b,cmp,cname=None):
    if cname is None: cname=cmp.__name__
    assert cmp(a,b),f"{cname}:\n{a}\n{b}"

# %% ../nbs/lib/groundup_002_get_data_ready.ipynb 19
def test_eq(a,b): test(a,b,operator.eq,'==')

# %% ../nbs/lib/groundup_002_get_data_ready.ipynb 31
def match_pct(query, text):
    "calc the percent of the match between the query string and the text"
    query_keys = query.split(" ")
    total = len(query_keys)
    pct = [key in text.lower() for key in query_keys].count(True)/total
    return pct

# %% ../nbs/lib/groundup_002_get_data_ready.ipynb 33
def search_data_url(dataname):
    from fastai.data.external import URLs
    url = ""
    for k, v in dict(URLs.__dict__).items():
        pct = match_pct(dataname, k)
        if pct == 1.0:
            print(v)    
            url = v            
    return url            

# %% ../nbs/lib/groundup_002_get_data_ready.ipynb 45
def check_data_directories(query): # query of data url
    from fastai.data.external import untar_data
    path = untar_data(search_data_url(query))
    import os
    print(f"cd {str(path)}")
    os.system(f"ls {str(path)}")
    print()
    for p in os.listdir(path):
        if not "." in p:
            print(f"cd {str(path/p)}")
            os.system(f"ls {str(path/p)}")
            print()

# %% ../nbs/lib/groundup_002_get_data_ready.ipynb 56
def get_img_paths(query, train, valid, test):
    from fastai.data.external import untar_data
    from fastai.data.transforms import get_image_files
    path = untar_data(search_data_url(query))
    files_train = get_image_files(path/train)
    files_valid = get_image_files(path/valid)
    files_test = get_image_files(path/test)
    print(f'train: {len(files_train)}, valid: {len(files_valid)}, test: {len(files_test)}')
    return files_train, files_valid, files_test

# %% ../nbs/lib/groundup_002_get_data_ready.ipynb 70
def get_labels(img_files):
    "get labels for each x from a list of file paths"
    df = pd.read_csv(path/"labels.csv")
    labels = []
    for f in img_files:
        for i in df.index:
            if df["name"][i] in str(f):
                labels.append(df["label"][i])
    import random
    from fastcore.foundation import L
    rand = [random.randint(0,len(labels)) for i in range(5)]
    print(f"len: {len(labels)}, random view: {L(labels)[rand]}")
    print(img_files[rand])
    return labels

# %% ../nbs/lib/groundup_002_get_data_ready.ipynb 120
def mean_std(t):
    "check mean and std of a tensor"
    print(f'mean: {t.mean()}, std: {t.std()}')

# %% ../nbs/lib/groundup_002_get_data_ready.ipynb 122
def normalize(t):
    "to normalize a tensor by dividing its maximum value"
    return t/t.max()

# %% ../nbs/lib/groundup_002_get_data_ready.ipynb 125
def imgs2tensor(img_folder:list, # a list of image files path in string
                n=-1, # n == -1 to process all files in the list, otherwise just [:n] files
                size=28 # images to be resized to (size, size)
               ): 
    "convert image folders into a tensor in which images stack on each other, and normalize it"
    lst_t = []
    if n > 0: selected = img_folder[:n]
    else: selected = img_folder
    for f in selected:
        img = PIL.Image.open(f).resize((size,size))
        t = torch.Tensor(np.array(img))
        lst_t.append(t)
    res = torch.stack(lst_t, dim=0)
    res = normalize(res)
    print(res.shape)
    mean_std(res)
    return res

# %% ../nbs/lib/groundup_003_matmul.ipynb 20
def get_exp_data():
    MNIST_URL='https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/data/mnist.pkl.gz?raw=true'
    path_data = Path('data')
    path_data.mkdir(exist_ok=True) # created a data folder in the current directory
    path_gz = path_data/'mnist.pkl.gz'
    from urllib.request import urlretrieve
    if not path_gz.exists(): urlretrieve(MNIST_URL, path_gz)
    with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')
    return x_train, y_train, x_valid, y_valid

# %% ../nbs/lib/groundup_003_matmul.ipynb 25
def chunks(x, sz):
    for i in range(0, len(x), sz): 
        print(i)
        yield x[i:i+sz]

# %% ../nbs/lib/groundup_003_matmul.ipynb 50
def chunks_faster(x, sz):
    "if the data is numpy.ndarray and shape is 1 dimension, then we use chunks to make it a pseudo 2d"
    lst = list(x)
    it = iter(lst)
    img = list(iter(lambda: list(islice(it, sz)), []))
    print(f'len: {len(img)}')
    return img

# %% ../nbs/lib/groundup_003_matmul.ipynb 57
class Matrix:
    "turning a list of list into a maxtrix like object"
    def __init__(self, xs): self.xs = xs
    def __getitem__(self, idxs): return self.xs[idxs[0]][idxs[1]]

# %% ../nbs/lib/groundup_003_matmul.ipynb 90
def rand():
    "create a random number between 0 and 1"
    global rnd_state
    x, y, z = rnd_state
    x = (171 * x) % 30269
    y = (172 * y) % 30307
    z = (170 * z) % 30323
    rnd_state = x,y,z
    return (x/30269 + y/30307 + z/30323) % 1.0