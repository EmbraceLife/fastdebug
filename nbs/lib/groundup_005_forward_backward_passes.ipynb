{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# groundup_005_forward_backward_passes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp delete0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "a = \"to delete\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastdebug.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastdebug.groundup import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %whos function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\n",
    "from pathlib import Path\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastnbs(\"set_printoptions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "np.set_printoptions(precision=2, linewidth=140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get data: get_exp_data, map, tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastnbs(\"idx check\")\n",
    "# check(get_exp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pathlib.Path"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_exp_data??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = map(tensor, get_exp_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 784])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n",
    "type(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exploratory version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastlistnbs(\"groundup\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w1, b1, w2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n,m = x_train.shape\n",
    "# c = y_train.max()+1\n",
    "# n,m,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor(10)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_r,input_c = x_train.shape\n",
    "label_num = y_train.max()+1\n",
    "input_r\n",
    "input_c\n",
    "label_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num hidden activations\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w1 = torch.randn(m,nh)\n",
    "# b1 = torch.zeros(nh)\n",
    "# w2 = torch.randn(nh,1)\n",
    "# b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 50])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = torch.randn(input_c,nh) # weights or coefficients for input\n",
    "b1 = torch.zeros(nh) # biases\n",
    "w2 = torch.randn(nh,1) # weights for hidden activations \n",
    "b2 = torch.zeros(1)\n",
    "w1.shape\n",
    "b1.shape\n",
    "w2.shape\n",
    "b2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lin(x,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export groundup\n",
    "def lin(x, w, b): \n",
    "    \"build a single layer linear model. use torch.matmul (faster version of einsum) to create a linear model\"\n",
    "    return x@w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 50])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_activations = lin(x_train, w1, b1)\n",
    "hidden_layer_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = lin(x_valid, w1, b1)\n",
    "# t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature: None\n",
      "__class__: <class 'builtin_function_or_method'>\n",
      "__repr__: <built-in method clamp_min of type object>\n",
      "\n",
      "__module__: torch\n",
      "__doc__: not exist\n",
      "\n",
      "__dict__: not exist \n",
      "\n",
      "metaclass: False\n",
      "class: False\n",
      "decorator: False\n",
      "function: False\n",
      "method: False\n"
     ]
    }
   ],
   "source": [
    "check(torch.clamp_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export groundup\n",
    "def relu(x): \n",
    "    \"basic relu with max in torch\"\n",
    "    return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.00,  0.00,  1.79,  ...,  0.00,  4.86,  3.40],\n",
       "        [ 0.00,  1.57,  6.56,  ...,  0.00,  6.72,  0.00],\n",
       "        [ 0.25,  4.06,  0.69,  ...,  0.00,  0.00,  0.00],\n",
       "        ...,\n",
       "        [11.80,  0.00, 18.88,  ...,  3.64,  7.60,  0.00],\n",
       "        [ 0.00, 13.53,  7.00,  ...,  5.06,  1.91,  0.00],\n",
       "        [ 4.54,  4.09, 11.58,  ...,  2.35,  0.00,  0.00]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = relu(lin(x_valid, w1, b1)) # add relu unto the linear model\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 50])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq((t >= 0).count_nonzero(), 500000)\n",
    "test_eq((t < 0).count_nonzero(), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb):\n",
    "    \"build a model of 2 layers (1 hidden layer) using lin and relu\"\n",
    "    l1 = lin(xb, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    res = lin(l2, w2, b2)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(x_valid)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE (mean of the squared error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get rid of that trailing (,1), in order to use `mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:,0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Of course, `mse` is not a suitable loss function for multi-class classification; we'll use a better loss function soon. We'll use `mse` for now to keep things simple.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(output, targ): return (output[:,0]-targ).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5, 0, 4,  ..., 8, 4, 8]), tensor([3, 8, 6,  ..., 5, 6, 8]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train,y_valid = y_train.float(),y_valid.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(x_train)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(877.31)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients and backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question: gradients of input, w and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):\n",
    "    # grad of matmul with respect to input\n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### derivaties on scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, diff, Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,b,inp = symbols('w,b,inp', real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Function('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle f{\\left(w,b,inp \\right)}$"
      ],
      "text/plain": [
       "f(w, b, inp)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = f(w,b,inp)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle b + inp w$"
      ],
      "text/plain": [
       "b + inp*w"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr = w*inp + b\n",
    "expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle f{\\left(w,b,inp \\right)} = b + inp w$"
      ],
      "text/plain": [
       "Eq(f(w, b, inp), b + inp*w)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sympy.Eq(f, expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export groundup\n",
    "def print_derivaties(func, expr, *variables):\n",
    "    import sympy\n",
    "    from fastdebug.utils import display_md\n",
    "    display_md(\"$\"+sympy.latex(sympy.Eq(func, expr))+\"$\")\n",
    "    func = expr\n",
    "    lst_derivatives = []\n",
    "    for i in variables:\n",
    "        display_md(\"$\\\\frac{\\\\partial f}{\\\\partial \" + str(i) + \"} =\" + sympy.latex(sympy.simplify(func.diff(i))) + \"$\")\n",
    "        lst_derivatives.append(func.diff(i))\n",
    "    return lst_derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "$f{\\left(w,b,inp \\right)} = b + inp w$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$\\frac{\\partial f}{\\partial inp} =w$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$\\frac{\\partial f}{\\partial w} =inp$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$\\frac{\\partial f}{\\partial b} =1$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = print_derivaties(f, expr, inp, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### derivaties on vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, Matrix, diff, Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle f{\\left(\\left[\\begin{matrix}u_{1}\\\\u_{2}\\\\u_{3}\\end{matrix}\\right],\\left[\\begin{matrix}v_{1}\\\\v_{2}\\\\v_{3}\\end{matrix}\\right],b \\right)}$"
      ],
      "text/plain": [
       "f(Matrix([\n",
       "[u_1],\n",
       "[u_2],\n",
       "[u_3]]), Matrix([\n",
       "[v_1],\n",
       "[v_2],\n",
       "[v_3]]), b)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle g{\\left(f{\\left(\\left[\\begin{matrix}u_{1}\\\\u_{2}\\\\u_{3}\\end{matrix}\\right],\\left[\\begin{matrix}v_{1}\\\\v_{2}\\\\v_{3}\\end{matrix}\\right],b \\right)} \\right)}$"
      ],
      "text/plain": [
       "g(f(Matrix([\n",
       "[u_1],\n",
       "[u_2],\n",
       "[u_3]]), Matrix([\n",
       "[v_1],\n",
       "[v_2],\n",
       "[v_3]]), b))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u1, u2, u3, v1, v2, v3, t, b = symbols('u_1 u_2 u_3 v_1 v_2 v_3  t b', real=True)\n",
    "f = Function('f')\n",
    "g = Function('g')\n",
    "inp = Matrix([u1,u2,u3])\n",
    "inp.shape\n",
    "w = Matrix([v1,v2,v3])\n",
    "f = f(inp, w, b)\n",
    "g = g(f)\n",
    "f\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr = inp.dot(w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle b + u_{1} v_{1} + u_{2} v_{2} + u_{3} v_{3}$"
      ],
      "text/plain": [
       "b + u_1*v_1 + u_2*v_2 + u_3*v_3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "$f{\\left(\\left[\\begin{matrix}u_{1}\\\\u_{2}\\\\u_{3}\\end{matrix}\\right],\\left[\\begin{matrix}v_{1}\\\\v_{2}\\\\v_{3}\\end{matrix}\\right],b \\right)} = b + u_{1} v_{1} + u_{2} v_{2} + u_{3} v_{3}$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$\\frac{\\partial f}{\\partial Matrix([[u_1], [u_2], [u_3]])} =\\left[\\begin{matrix}v_{1}\\\\v_{2}\\\\v_{3}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$\\frac{\\partial f}{\\partial Matrix([[v_1], [v_2], [v_3]])} =\\left[\\begin{matrix}u_{1}\\\\u_{2}\\\\u_{3}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "$\\frac{\\partial f}{\\partial b} =1$"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lst = print_derivaties(f, expr, inp, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # forward pass:\n",
    "    l1 = inp @ w1 + b1\n",
    "    l2 = relu(l1)\n",
    "    out = l2 @ w2 + b2\n",
    "    diff = out[:,0]-targ\n",
    "#     loss = res.pow(2).mean()\n",
    "    loss = diff.pow(2).mean()\n",
    "    \n",
    "    # backward pass:\n",
    "    out.g = 2.*diff[:,None] / inp.shape[0] # d_loss/d_diff\n",
    "    # d_diff/d_out = 1\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    l1.g = (l1>0).float() * l2.g # derivate of relu(l1) with respect to l1\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save for testing against later\n",
    "# w1g = w1.g.clone()\n",
    "# w2g = w2.g.clone()\n",
    "# b1g = b1.g.clone()\n",
    "# b2g = b2.g.clone()\n",
    "# ig  = x_train.g.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cheat a little bit and use PyTorch autograd to check our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt2 = x_train.clone().requires_grad_(True)\n",
    "w12 = w1.clone().requires_grad_(True)\n",
    "w22 = w2.clone().requires_grad_(True)\n",
    "b12 = b1.clone().requires_grad_(True)\n",
    "b22 = b2.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inp, targ):\n",
    "    l1 = inp @ w12 + b12\n",
    "    l2 = relu(l1)\n",
    "    out = l2 @ w22 + b22\n",
    "    return mse(out, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = forward(xt2, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import test_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_close(w22.grad, w2g, eps=0.01)\n",
    "# test_close(b22.grad, b2g, eps=0.01)\n",
    "# test_close(w12.grad, w1g, eps=0.01)\n",
    "# test_close(b12.grad, b1g, eps=0.01)\n",
    "# test_close(xt2.grad, ig , eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers as classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snoop import snoop\n",
    "# https://github.com/alexmojaki/snoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self): self.inp.g = (self.inp>0).float() * self.out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Relu()(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin():\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "        \n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp@self.w + self.b\n",
    "        return self.out\n",
    "\n",
    "    @snoop\n",
    "    def backward(self):\n",
    "        pp(self.inp.shape, self.out.g.shape, self.w.shape, self.w.t().shape)\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = self.inp.t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp = inp\n",
    "        self.targ = targ\n",
    "        self.out = (inp.squeeze() - targ).pow(2).mean()\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
    "        self.loss = Mse()\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 383 ms, sys: 25.7 ms, total: 409 ms\n",
      "Wall time: 60.4 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:05:56.75 >>> Call to Lin.backward in File \"/var/folders/gz/ch3n2mp51m9386sytqf97s6w0000gn/T/ipykernel_49026/878164743.py\", line 10\n",
      "23:05:56.75 .......... self = <__main__.Lin object>\n",
      "23:05:56.75   10 |     def backward(self):\n",
      "23:05:56.75   11 |         pp(self.inp.shape, self.out.g.shape, self.w.shape, self.w.t().shape)\n",
      "23:05:56.75 LOG:\n",
      "23:05:57.11 .... self.inp.shape = torch.Size([50000, 50])\n",
      "23:05:57.11 .... self.out.g.shape = torch.Size([50000, 1])\n",
      "23:05:57.11 .... self.w.shape = torch.Size([50, 1])\n",
      "23:05:57.11 .... self.w.t().shape = torch.Size([1, 50])\n",
      "23:05:57.11   12 |         self.inp.g = self.out.g @ self.w.t()\n",
      "23:05:57.11   13 |         self.w.g = self.inp.t() @ self.out.g\n",
      "23:05:57.14   14 |         self.b.g = self.out.g.sum(0)\n",
      "23:05:57.14 <<< Return value from Lin.backward: None\n",
      "23:05:57.14 >>> Call to Lin.backward in File \"/var/folders/gz/ch3n2mp51m9386sytqf97s6w0000gn/T/ipykernel_49026/878164743.py\", line 10\n",
      "23:05:57.14 .......... self = <__main__.Lin object>\n",
      "23:05:57.14   10 |     def backward(self):\n",
      "23:05:57.14   11 |         pp(self.inp.shape, self.out.g.shape, self.w.shape, self.w.t().shape)\n",
      "23:05:57.14 LOG:\n",
      "23:05:57.14 .... self.inp.shape = torch.Size([50000, 784])\n",
      "23:05:57.14 .... self.out.g.shape = torch.Size([50000, 50])\n",
      "23:05:57.14 .... self.w.shape = torch.Size([784, 50])\n",
      "23:05:57.14 .... self.w.t().shape = torch.Size([50, 784])\n",
      "23:05:57.14   12 |         self.inp.g = self.out.g @ self.w.t()\n",
      "23:05:57.21   13 |         self.w.g = self.inp.t() @ self.out.g\n",
      "23:05:57.30   14 |         self.b.g = self.out.g.sum(0)\n",
      "23:05:57.30 <<< Return value from Lin.backward: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.45 s, sys: 857 ms, total: 2.31 s\n",
      "Wall time: 556 ms\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_close(w2g, w2.g, eps=0.01)\n",
    "# test_close(b2g, b2.g, eps=0.01)\n",
    "# test_close(w1g, w1.g, eps=0.01)\n",
    "# test_close(b1g, b1.g, eps=0.01)\n",
    "# test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "\n",
    "    def forward(self): raise Exception('not implemented')\n",
    "    def bwd(self): raise Exception('not implemented')\n",
    "    def backward(self): self.bwd(self.out, *self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, inp): return inp.clamp_min(0.)\n",
    "    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b): self.w,self.b = w,b\n",
    "    def forward(self, inp): return inp@self.w + self.b\n",
    "    \n",
    "#     @snoop(watch=('self.out.g.shape, self.w.shape, self.w.t().shape, inp.t().shape, self.w.g.shape, self.b.g.shape'))\n",
    "    def bwd(self, out, inp):\n",
    "\n",
    "        from snoop import pp\n",
    "        with snoop: #(watch_explode=('self')):\n",
    "            pp(self.out.g.shape, self.w.t().shape)\n",
    "            inp.g = self.out.g @ self.w.t()\n",
    "            pp(inp.t().shape)        \n",
    "            self.w.g = inp.t() @ self.out.g\n",
    "            pp(self.out.g.sum(0), self.out.g.sum(0).shape)\n",
    "            self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use snoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module):\n",
    "    @snoop\n",
    "    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n",
    "    \n",
    "    @snoop\n",
    "    def bwd(self, out, inp, targ): \n",
    "        from snoop import pp\n",
    "        pp(inp.shape, inp.squeeze().shape, 2*(inp.squeeze()-targ).unsqueeze(-1).shape, targ.shape[0])\n",
    "        inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:05:57.49 >>> Call to Mse.forward in File \"/var/folders/gz/ch3n2mp51m9386sytqf97s6w0000gn/T/ipykernel_49026/1757170846.py\", line 3\n",
      "23:05:57.49 .......... self = <__main__.Mse object>\n",
      "23:05:57.49 .......... inp = tensor([[ 11.52],\n",
      "23:05:57.49                          [ 28.00],\n",
      "23:05:57.49                          [-37...   [-24.99],\n",
      "23:05:57.49                          [ -6.92],\n",
      "23:05:57.49                          [ -5.51]])\n",
      "23:05:57.49 .......... inp.shape = (50000, 1)\n",
      "23:05:57.49 .......... inp.dtype = torch.float32\n",
      "23:05:57.49 .......... targ = tensor([5., 0., 4.,  ..., 8., 4., 8.])\n",
      "23:05:57.49 .......... targ.shape = (50000,)\n",
      "23:05:57.49 .......... targ.dtype = torch.float32\n",
      "23:05:57.49    3 |     def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n",
      "23:05:57.49    3 |     def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n",
      "23:05:57.49 <<< Return value from Mse.forward: tensor(877.31)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 389 ms, sys: 64.6 ms, total: 453 ms\n",
      "Wall time: 61.7 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:05:57.51 >>> Call to Mse.bwd in File \"/var/folders/gz/ch3n2mp51m9386sytqf97s6w0000gn/T/ipykernel_49026/1757170846.py\", line 6\n",
      "23:05:57.51 .......... self = <__main__.Mse object>\n",
      "23:05:57.51 .......... out = tensor(877.31)\n",
      "23:05:57.51 .......... out.shape = ()\n",
      "23:05:57.51 .......... out.dtype = torch.float32\n",
      "23:05:57.51 .......... inp = tensor([[ 11.52],\n",
      "23:05:57.51                          [ 28.00],\n",
      "23:05:57.51                          [-37...   [-24.99],\n",
      "23:05:57.51                          [ -6.92],\n",
      "23:05:57.51                          [ -5.51]])\n",
      "23:05:57.51 .......... inp.shape = (50000, 1)\n",
      "23:05:57.51 .......... inp.dtype = torch.float32\n",
      "23:05:57.51 .......... targ = tensor([5., 0., 4.,  ..., 8., 4., 8.])\n",
      "23:05:57.51 .......... targ.shape = (50000,)\n",
      "23:05:57.51 .......... targ.dtype = torch.float32\n",
      "23:05:57.51    6 |     def bwd(self, out, inp, targ): \n",
      "23:05:57.51    7 |         from snoop import pp\n",
      "23:05:57.51 .............. pp = <snoop.pp_module.PP object>\n",
      "23:05:57.51    8 |         pp(inp.shape, inp.squeeze().shape, 2*(inp.squeeze()-targ).unsqueeze(-1).shape, targ.shape[0])\n",
      "23:05:57.51 LOG:\n",
      "23:05:57.52 .... inp.shape = torch.Size([50000, 1])\n",
      "23:05:57.52 .... inp.squeeze().shape = torch.Size([50000])\n",
      "23:05:57.52 .... 2*(inp.squeeze()-targ).unsqueeze(-1).shape = torch.Size([50000, 1, 50000, 1])\n",
      "23:05:57.52 .... targ.shape[0] = 50000\n",
      "23:05:57.52    9 |         inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]\n",
      "23:05:57.53 <<< Return value from Mse.bwd: None\n",
      "23:05:57.53 >>> Enter with block in Lin.bwd in File \"/var/folders/gz/ch3n2mp51m9386sytqf97s6w0000gn/T/ipykernel_49026/3860237156.py\", line 9\n",
      "23:05:57.53 .............. self = <__main__.Lin object>\n",
      "23:05:57.53 .............. out = tensor([[ 11.52],\n",
      "23:05:57.53                              [ 28.00],\n",
      "23:05:57.53                              [-37...   [-24.99],\n",
      "23:05:57.53                              [ -6.92],\n",
      "23:05:57.53                              [ -5.51]])\n",
      "23:05:57.53 .............. out.shape = (50000, 1)\n",
      "23:05:57.53 .............. out.dtype = torch.float32\n",
      "23:05:57.53 .............. inp = tensor([[10.66,  0.00, 16.42,  ...,  0.00,  2.49... 3.75,  0.00,  0.00,  ...,  0.00,  3.01,  0.00]])\n",
      "23:05:57.53 .............. inp.shape = (50000, 50)\n",
      "23:05:57.53 .............. inp.dtype = torch.float32\n",
      "23:05:57.53 .............. pp = <snoop.pp_module.PP object>\n",
      "23:05:57.53   10 |             pp(self.out.g.shape, self.w.t().shape)\n",
      "23:05:57.53 LOG:\n",
      "23:05:57.54 .... self.out.g.shape = torch.Size([50000, 1])\n",
      "23:05:57.54 .... self.w.t().shape = torch.Size([1, 50])\n",
      "23:05:57.54   11 |             inp.g = self.out.g @ self.w.t()\n",
      "23:05:57.55   12 |             pp(inp.t().shape)        \n",
      "23:05:57.55 LOG:\n",
      "23:05:57.56 .... inp.t().shape = torch.Size([50, 50000])\n",
      "23:05:57.56   13 |             self.w.g = inp.t() @ self.out.g\n",
      "23:05:57.57   14 |             pp(self.out.g.sum(0), self.out.g.sum(0).shape)\n",
      "23:05:57.57 LOG:\n",
      "23:05:57.57 .... self.out.g.sum(0) = tensor([-12.42])\n",
      "23:05:57.57 .... self.out.g.sum(0).shape = torch.Size([1])\n",
      "23:05:57.57   15 |             self.b.g = self.out.g.sum(0)\n",
      "23:05:57.58 <<< Exit with block in Lin.bwd\n",
      "23:05:57.58 >>> Enter with block in Lin.bwd in File \"/var/folders/gz/ch3n2mp51m9386sytqf97s6w0000gn/T/ipykernel_49026/3860237156.py\", line 9\n",
      "23:05:57.58 .............. self = <__main__.Lin object>\n",
      "23:05:57.58 .............. out = tensor([[ 10.66, -10.51,  16.42,  ...,  -1.19,  ...,  -8.48,  -1.09,  ...,  -3.13,   3.01,  -1.09]])\n",
      "23:05:57.58 .............. out.shape = (50000, 50)\n",
      "23:05:57.58 .............. out.dtype = torch.float32\n",
      "23:05:57.58 .............. inp = tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "23:05:57.58                              ...0., 0.],\n",
      "23:05:57.58                              [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "23:05:57.58 .............. inp.shape = (50000, 784)\n",
      "23:05:57.58 .............. inp.dtype = torch.float32\n",
      "23:05:57.58 .............. pp = <snoop.pp_module.PP object>\n",
      "23:05:57.58   10 |             pp(self.out.g.shape, self.w.t().shape)\n",
      "23:05:57.58 LOG:\n",
      "23:05:57.58 .... self.out.g.shape = torch.Size([50000, 50])\n",
      "23:05:57.58 .... self.w.t().shape = torch.Size([50, 784])\n",
      "23:05:57.58   11 |             inp.g = self.out.g @ self.w.t()\n",
      "23:05:57.65   12 |             pp(inp.t().shape)        \n",
      "23:05:57.66 LOG:\n",
      "23:05:57.66 .... inp.t().shape = torch.Size([784, 50000])\n",
      "23:05:57.66   13 |             self.w.g = inp.t() @ self.out.g\n",
      "23:05:57.74   14 |             pp(self.out.g.sum(0), self.out.g.sum(0).shape)\n",
      "23:05:57.74 LOG:\n",
      "23:05:57.74 .... self.out.g.sum(0) = tensor([ -2.48,  18.34,  -3.39,  -3.04,   1.27,  10.89,   3.33,  -1.18,  -0.03,   0.56,  13.46,   6.57,  -7.00, -10.49,  -3.84,  23.47,\n",
      "23:05:57.74                                   -0.33,   6.83,  -0.17,  -2.58,   7.33,  17.98,  10.49,   0.82,   0.63,  -2.49,   2.80,   4.78,  -0.60, -10.30,  -9.16,   8.34,\n",
      "23:05:57.74                                    0.31,   1.92,  -6.47,  -4.49,   0.05,  -3.37,  -1.52,  -1.90,  -1.55,   3.13,  -0.05,  -4.54,  -0.34,   1.22,  13.38,   0.24,\n",
      "23:05:57.74                                   -2.23,  -0.24])\n",
      "23:05:57.74 .... self.out.g.sum(0).shape = torch.Size([50])\n",
      "23:05:57.74   15 |             self.b.g = self.out.g.sum(0)\n",
      "23:05:57.74 <<< Exit with block in Lin.bwd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 958 ms, sys: 474 ms, total: 1.43 s\n",
      "Wall time: 236 ms\n"
     ]
    }
   ],
   "source": [
    "%time model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_close(w2g, w2.g, eps=0.01)\n",
    "# test_close(b2g, b2.g, eps=0.01)\n",
    "# test_close(w1g, w1.g, eps=0.01)\n",
    "# test_close(b1g, b1.g, eps=0.01)\n",
    "# test_close(ig, x_train.g, eps=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, n_in, n_out):\n",
    "        super().__init__()\n",
    "        self.w = torch.randn(n_in,n_out).requires_grad_()\n",
    "        self.b = torch.zeros(n_out).requires_grad_()\n",
    "    def forward(self, inp): return inp@self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [Linear(n_in,nh), nn.ReLU(), Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self, x, targ):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return F.mse_loss(x, targ[:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_c, nh, 1)\n",
    "loss = model(x_train, y_train)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 169.15,   17.12,   23.60,   30.69,   85.86,   27.71,  213.83,  159.29, -199.09,   41.94,    1.28,   58.02,   -1.48,   45.44,\n",
       "          43.16,  134.84,   52.21,  -24.13,   10.31,    3.67,  -77.47,  -43.81,   -3.78, -157.99,  -80.99,  -25.59,    0.67,   70.84,\n",
       "         108.06,   49.26,   -9.78,    6.48,  -51.64,   71.07,  -44.97,   23.96,  -42.41,   56.45,    1.68,  -50.10,    2.48,   27.53,\n",
       "          64.56,  -21.02,   15.59,   -1.81,  -10.04,   87.41,  -13.01,  -72.34])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l0 = model.layers[0]\n",
    "l0.b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
