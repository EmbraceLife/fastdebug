{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp layers\n",
    "#|default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from fastai.imports import *\n",
    "from fastai.torch_imports import *\n",
    "from fastai.torch_core import *\n",
    "from torch.nn.utils import weight_norm, spectral_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *\n",
    "from fastdebug.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai.layers as fl\n",
    "import fastai.torch_core as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastai.torch_core has: \n",
      "85 items in its __all__, and \n",
      "316 user defined functions, \n",
      "137 classes or class objects, \n",
      "4 builtin funcs and methods, and\n",
      "476 callables.\n",
      "\n",
      "None\n",
      "fastai.layers has: \n",
      "61 items in its __all__, and \n",
      "342 user defined functions, \n",
      "172 classes or class objects, \n",
      "4 builtin funcs and methods, and\n",
      "541 callables.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "whatinside(ft)\n",
    "whatinside(fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "> Custom fastai layers and basic functions to grab them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic manipulations and resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```module(*flds, **defaults)```\n",
    "- Decorator to create an `nn.Module` using `f` as `forward` method\n",
    "- create parameters from `flds` and `defaults` and make fileds of args from their keys or names\n",
    "- make the decorated function eg. `Identity` a subclass of ```nn.Module``` and \n",
    "- make `Identity` function itself to be the `forward` function of the subclass of ```nn.Module```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "# @pysnoop()\n",
    "def module(*flds, **defaults):\n",
    "    \"Decorator to create an `nn.Module` using `f` as `forward` method\"\n",
    "    pa = [inspect.Parameter(o, inspect.Parameter.POSITIONAL_OR_KEYWORD) for o in flds]\n",
    "    pb = [inspect.Parameter(k, inspect.Parameter.POSITIONAL_OR_KEYWORD, default=v)\n",
    "          for k,v in defaults.items()]\n",
    "    params = pa+pb\n",
    "    all_flds = [*flds,*defaults.keys()]\n",
    "    \n",
    "#     @snoop\n",
    "#     @pysnoop()\n",
    "    def _f(f):\n",
    "        class c(nn.Module):\n",
    "#             @snoop # to enable debug for Identity()\n",
    "            def __init__(self, *args, **kwargs):\n",
    "                super().__init__()\n",
    "                for i,o in enumerate(args): kwargs[all_flds[i]] = o\n",
    "                kwargs = merge(defaults,kwargs)\n",
    "                for k,v in kwargs.items(): setattr(self,k,v)\n",
    "            __repr__ = basic_repr(all_flds)\n",
    "            forward = f # making Identity's own function to be the forward function?\n",
    "        c.__signature__ = inspect.Signature(params)\n",
    "        c.__name__ = c.__qualname__ = f.__name__\n",
    "        c.__doc__  = f.__doc__\n",
    "        return c\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@module() # running module() and return _f\n",
    "# @snoop\n",
    "def Identity(self, x): # running _f(Identify) and return c, c has __name__ as `Identity` which is a subclass of nn.Module\n",
    "    \"Do nothing at all\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc(module)\n",
    "# doc(Identity)\n",
    "# fastnbs(\"module(*\", \"src\")\n",
    "# module?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:32:14.54 LOG:\n",
      "14:32:14.66 .... module = <function module>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.module(*flds, **defaults)>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp(module)\n",
    "# ic(Identity()(1)) # running Identity's own function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(Identity()(1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Lambda(self, x)```\n",
    "- An easy way to create a pytorch layer for a simple `func`\n",
    "- using ```module``` decorator, and make ```Lambda``` a subclass of ```nn.Module``` and create a parameter `func` for ```Lambda```\n",
    "- run ```Lambda(func)``` to make the `func` a pytorch layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@module('func')\n",
    "# @snoop\n",
    "def Lambda(self, x):\n",
    "    \"An easy way to create a pytorch layer for a simple `func`\"\n",
    "    return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Lambda(func=<function _add2>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _add2(x): return x+2\n",
    "tst = Lambda(_add2)\n",
    "tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10,20)\n",
    "test_eq(tst(x), x+2) # running foward function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Lambda(func=<function _add2>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst2 = pickle.loads(pickle.dumps(tst)) # question: why dumps and then loads again (check the rubostness of the func?)\n",
    "test_eq(tst2(x), x+2)\n",
    "tst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```PartialLambda(Lambda)```\n",
    "- Layer that applies `partial(func, **kwargs)`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastnbs(\"module(*flds\", filter_folder=\"src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```PartialLambda(Lambda)```\n",
    "- a subclass of Lambda, which is a subclass of module, which wrap around nn.Module\n",
    "- Layer that applies `partial(func, **kwargs)` which can custom the `func` of ```Lambda```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PartialLambda(Lambda):\n",
    "    \"Layer that applies `partial(func, **kwargs)`\"\n",
    "    def __init__(self, func, **kwargs):\n",
    "        super().__init__(partial(func, **kwargs))\n",
    "        self.repr = f'{func.__name__}, {kwargs}'\n",
    "\n",
    "    def forward(self, x): return self.func(x)\n",
    "    def __repr__(self): return f'{self.__class__.__name__}({self.repr})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_func(a,b=2): return a+b\n",
    "tst = PartialLambda(test_func, b=5)\n",
    "x.shape\n",
    "test_eq(tst(x), x+5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```view(self:Tensor-1), x.view(x.size(0), -1)```\n",
    "- flatten x into a 1d tensor\n",
    "- flatten x into a 2d tensor, keep the 1dim unchanged, but flatten the rest dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor.view?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "x.size()\n",
    "y = x.view(16)\n",
    "y.size()\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "z.size()\n",
    "z1 = x.view(-1)\n",
    "z1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Flatten(self, x)```\n",
    "- Flatten `x` to a single dimension, e.g. at end of a model. `full` for rank-1 tensor\"\n",
    "Logic: \n",
    "- use decorator ```module(full=False)``` to make ```Flatten``` a layer and create a parameter ```full=False```\n",
    "- ```Flatten(self, x)``` works as the `forward` function\n",
    "- `self.full` can be access in the `forward` function above\n",
    "- ```Flatten(full=True)```: to flatten all dims of a tensor into a 1d tensor\n",
    "- ```Flatten(full=False)```: to keep 1st dim and flatten the rest dims, so only 2 dims remains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@module(full=False)\n",
    "# @snoop\n",
    "def Flatten(self, x):\n",
    "    \"Flatten `x` to a single dimension, e.g. at end of a model. `full` for rank-1 tensor\"\n",
    "#     pp(x.shape, x.view(-1).shape, x.size(0), x.size(), x.view(x.size(0), -1).shape)\n",
    "    return TensorBase(x.view(-1) if self.full else x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = Flatten() # this is running __init__\n",
    "x = torch.randn(10,5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(tst(x).shape, [10,20])\n",
    "tst = Flatten(full=True)\n",
    "test_eq(tst(x).shape, [200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```ToTensorBase(self, x)```\n",
    "- make ```ToTensorBase``` a subclass of `module` which is a subclass of `nn.Module`\n",
    "- ```ToTensorBase(tensor_cls=TensorBase)``` initialize itself with a tensor class (default to TensorBase) as a parameter\n",
    "- after initialization, the output function can take in `x` to turn `x` into an instance of ```TensorBase```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@module(tensor_cls=TensorBase) # the args here are for ToTensorBase.__init__\n",
    "def ToTensorBase(self, x):\n",
    "    \"Convert x to TensorBase\"\n",
    "    return self.tensor_cls(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastnbs(\"def module(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttb = ToTensorBase()\n",
    "timg = TensorImage(torch.rand(1,3,32,32))\n",
    "test_eq(type(ttb(timg)), TensorBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```View(Module)```\n",
    "- ```View``` is a subclass of ```Module```, which inherites from ```nn.Module``` and ```metaclass=PrePostInitMeta```\n",
    "- so, ```View``` is to create a layer for Viewing data\n",
    "- ```View(*size)``` can initialize itself by setting values for `self.size`\n",
    "- ```View.forward(x)``` can run `x.view(self.size)` to create a new tensor based on `x` but with different shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastnbs(\"class Module(\") # to remind me of `Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class View(Module):\n",
    "    \"Reshape `x` to `size`\"\n",
    "    def __init__(self, *size): self.size = size\n",
    "    def forward(self, x): return x.view(self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4,5,10)\n",
    "tst = View(10,5,4)\n",
    "test_eq(tst(x).shape, [10,5,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```ResizeBatch(Module)```\n",
    "- ```ResizeBatch``` is a subclass of nn.Module and no need to run `super().__init__`\n",
    "- ```ResizeBatch(*size)``` can initialize itself with a specific shape/size for tensors\n",
    "- ```rb(x)``` can reshape `x` so that the batch size dim is unchanged but other dims is changed based on `*size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 2, 3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3,) + (1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ResizeBatch(Module):\n",
    "    \"Reshape `x` to `size`, keeping batch dim the same size\"\n",
    "    def __init__(self, *size): self.size = size\n",
    "    def forward(self, x): return x.view((x.size(0),) + self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = ResizeBatch(5,4)\n",
    "x = torch.randn(10,20)\n",
    "test_eq(tst(x).shape, [10,5,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Debugger(self,x)```\n",
    "- ```Debugger``` is made into a layer by decorator `module` using `nn.Module`\n",
    "- after initialization, `db(x)` will run `set_trace()` and return `x` which is a model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastnbs(\"module(*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@module()\n",
    "def Debugger(self,x):\n",
    "    \"A module to debug inside a model.\"\n",
    "    set_trace()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=5, bias=True)\n",
       "  (1): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=5, bias=True)\n",
       "    (1): Linear(in_features=4, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = nn.Sequential(nn.Linear(4,5), nn.Sequential(nn.Linear(4,5), nn.Linear(4,5)))\n",
    "tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugger()(tst) # run this code to activate the ipdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```sigmoid_range(x, low, high)```\n",
    "- calculate sigmoid on tensor `x` and also keep the sigmoid values within `[low, high]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def sigmoid_range(x, low, high):\n",
    "    \"Sigmoid function with range `(low, high)`\"\n",
    "    return torch.sigmoid(x) * (high - low) + low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.5398e-05, 5.0000e-01, 9.9995e-01])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.9999,  0.5000,  1.9999])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tensor([-10.,0.,10.])\n",
    "torch.sigmoid(test)\n",
    "sigmoid_range(test, -1,  2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(sigmoid_range(test, -1,  2), tensor([-1.,0.5, 2.]), atol=1e-4, rtol=1e-4)\n",
    "assert torch.allclose(sigmoid_range(test, -5, -1), tensor([-5.,-3.,-1.]), atol=1e-4, rtol=1e-4)\n",
    "assert torch.allclose(sigmoid_range(test,  2,  4), tensor([2.,  3., 4.]), atol=1e-4, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```SigmoidRange(self, x)```\n",
    "- ```SigmoidRange``` is a subclass of `nn.Module`, and the func defined under `SigmoidRange` is used as `forward` func, thanks to ```module``` decorator\n",
    "- ```sr = SigmoidRange(low, high)``` initialize an instance with the value range `[low, high]`, with `low` and `high` as args for `__init__`\n",
    "- `sr(x)` to calc sigmoid on `x` and put it within the range `[low, high]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@module('low','high')\n",
    "def SigmoidRange(self, x):\n",
    "    \"Sigmoid module with range `(low, high)`\"\n",
    "    return sigmoid_range(x, self.low, self.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastlistnbs(\"src\")\n",
    "# fastnbs(\"module(*f\", \"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = SigmoidRange(-1, 2)\n",
    "assert torch.allclose(tst(test), tensor([-1.,0.5, 2.]), atol=1e-4, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```AdaptiveConcatPool1d(Module)```\n",
    "- becomes a layer, which is a subclass of `Module`, which inherits from `nn.Module` and `PrePostInitMeta` to avoid `super().__init__`\n",
    "- this layer that concats `AdaptiveAvgPool1d` and `AdaptiveMaxPool1d` side by side with each other\n",
    "- ```acp = AdaptiveConcatPool1d(size)``` to initialize the layer with size or num of activations\n",
    "- `acp(x)` is to run tensor `x` through the layer, and if `x.shape` is (5, 10), then the output shape is (5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AdaptiveConcatPool1d(Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool1d` and `AdaptiveMaxPool1d`\"\n",
    "    def __init__(self, size=None):\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool1d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool1d(self.size)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaptiveConcatPool1d(\n",
       "  (ap): AdaptiveAvgPool1d(output_size=1)\n",
       "  (mp): AdaptiveMaxPool1d(output_size=1)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaptiveConcatPool1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.AdaptiveConcatPool1d"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(AdaptiveConcatPool1d())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(AdaptiveConcatPool1d().children())[0](x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaptiveConcatPool1d()(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```torch.max(a, dim, keepdim)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5431,  0.4291,  0.6668, -0.1806],\n",
       "        [ 1.7736, -1.4782, -0.7680, -0.4568],\n",
       "        [-1.7035, -0.4617, -1.7458, -0.2594],\n",
       "        [-1.2322, -1.4792,  0.8141, -0.7532]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([ 0.6668,  1.7736, -0.2594,  0.8141]),\n",
       "indices=tensor([2, 0, 3, 2]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[ 0.6668],\n",
       "        [ 1.7736],\n",
       "        [-0.2594],\n",
       "        [ 0.8141]]),\n",
       "indices=tensor([[2],\n",
       "        [0],\n",
       "        [3],\n",
       "        [2]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a, 1)\n",
    "torch.max(a, 1)[0].shape\n",
    "torch.max(a, 1)[1].shape\n",
    "torch.max(a, dim=1, keepdim=True)\n",
    "torch.max(a, dim=1, keepdim=True)[0].shape\n",
    "torch.max(a, dim=1, keepdim=True)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([ 1.7736,  0.4291,  0.8141, -0.1806]),\n",
       "indices=tensor([1, 0, 3, 0]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([[ 1.7736,  0.4291,  0.8141, -0.1806]]),\n",
       "indices=tensor([[1, 0, 3, 0]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a, 0)\n",
    "torch.max(a, 0)[0].shape\n",
    "torch.max(a, 0)[1].shape\n",
    "torch.max(a, dim=0, keepdim=True)\n",
    "torch.max(a, dim=0, keepdim=True)[0].shape\n",
    "torch.max(a, dim=0, keepdim=True)[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```AdaptiveConcatPool2d(Module)```\n",
    "- it is like ```AdaptiveConcatPoold(Module)```, but deal with 2d\n",
    "- Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`\"\n",
    "- If the input is `bs x nf x h x h`, the output will be `bs x 2*nf x 1 x 1` if no size is passed or `bs x 2*nf x size x size` (nf: num of filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AdaptiveConcatPool2d(Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`\"\n",
    "    def __init__(self, size=None):\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool2d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool2d(self.size)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = AdaptiveConcatPool2d()\n",
    "x = torch.randn(10,5,4,4)\n",
    "test_eq(tst(x).shape, [10,10,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 1, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 1, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 4, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 1, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max1 = torch.max(x,    dim=2, keepdim=True)[0]\n",
    "max2 = torch.max(x,    dim=2, keepdim=False)[0]\n",
    "maxp = torch.max(max1, dim=3, keepdim=True)[0]\n",
    "max1.shape\n",
    "maxp.shape\n",
    "x.shape\n",
    "tst(x).shape\n",
    "test_eq(tst(x)[:,:5], maxp)\n",
    "test_eq(tst(x)[:,5:], x.mean(dim=[2,3], keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 4, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = AdaptiveConcatPool2d(2)\n",
    "x.shape\n",
    "test_eq(tst(x).shape, [10,10,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```PoolType.Avg, PoolType.Max, PoolType.Cat```\n",
    "- they are class properties, which are strings `Avg`, `Max` and `Cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PoolType: Avg,Max,Cat = 'Avg','Max','Cat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```adaptive_pool(pool_type)```\n",
    "- `pool_type` can be `Avg`, `Max` or `Cat`\n",
    "- return `nn.AdaptiveAvgPool2d`, `nn.AdaptiveMaxPool2d`, `nn.AdaptiveConcatPool2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def adaptive_pool(pool_type):\n",
    "    return nn.AdaptiveAvgPool2d if pool_type=='Avg' else nn.AdaptiveMaxPool2d if pool_type=='Max' else AdaptiveConcatPool2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```nn.AdaptiveAvgPool2d((*output_size))```\n",
    "- to initialize the layer using output_size such as `(5,7)`, `7`, `(None, 7)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaptiveAvgPool2d(output_size=(5, 7))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 5, 7])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nn.AdaptiveAvgPool2d??\n",
    "# target output size of 5x7\n",
    "m = nn.AdaptiveAvgPool2d((5,7))\n",
    "m\n",
    "input = torch.randn(1, 64, 8, 9)\n",
    "output = m(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaptiveAvgPool2d(output_size=7)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 7, 7])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target output size of 7x7 (square)\n",
    "m = nn.AdaptiveAvgPool2d(7)\n",
    "m\n",
    "input = torch.randn(1, 64, 10, 9)\n",
    "output = m(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaptiveAvgPool2d(output_size=(None, 7))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 10, 7])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target output size of 10x7\n",
    "m = nn.AdaptiveAvgPool2d((None, 7))\n",
    "m\n",
    "input = torch.randn(1, 64, 10, 9)\n",
    "output = m(input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```PoolFlatten(nn.Sequential)```\n",
    "- it inherits from `nn.Sequential`, so it can be a layer\n",
    "- it combines `nn.AdaptiveAvgPool2d` and `Flatten`\n",
    "- its `nn.AdaptiveAvgPool2d` layer has the last 2 dims to be (1,1)\n",
    "- its `Flatten` layer only keeps two dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PoolFlatten(nn.Sequential):\n",
    "    \"Combine `nn.AdaptiveAvgPool2d` and `Flatten`.\"\n",
    "    def __init__(self, pool_type=PoolType.Avg): super().__init__(adaptive_pool(pool_type)(1), Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastnbs(\"Flatten(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PoolFlatten(\n",
       "  (0): AdaptiveAvgPool2d(output_size=1)\n",
       "  (1): __main__.Flatten(full=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 4, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 1, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = PoolFlatten()\n",
    "tst\n",
    "x.shape\n",
    "nn.AdaptiveAvgPool2d(1)(x).shape\n",
    "Flatten()(nn.AdaptiveAvgPool2d(1)(x)).shape\n",
    "test_eq(tst(x).shape, [10,5])\n",
    "test_eq(tst(x), x.mean(dim=[2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| x: class=<class 'torch.Tensor'>, shape=torch.Size([10, 5, 4, 4]), dtype=torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 7.1276e-02,  6.2517e-01, -2.2132e+00, -1.7874e-01],\n",
       "          [-3.2758e-01, -1.2376e+00,  2.0178e+00, -1.0494e+00],\n",
       "          [-1.8578e+00, -7.9698e-01, -2.1171e+00, -5.7127e-01],\n",
       "          [ 3.5175e-01, -9.9976e-01, -4.4960e-02, -5.3514e-01]],\n",
       "\n",
       "         [[-1.5330e+00, -6.3157e-01, -1.9691e-01,  1.5494e+00],\n",
       "          [-1.8135e-01,  8.2392e-01,  6.2565e-01,  2.6549e+00],\n",
       "          [-1.0391e+00, -7.7068e-01, -4.1978e-01, -8.8519e-01],\n",
       "          [ 6.2824e-01,  7.7295e-03,  1.1718e+00,  1.7979e-01]],\n",
       "\n",
       "         [[ 1.3124e+00, -5.9516e-01, -1.5508e+00,  2.4107e+00],\n",
       "          [ 1.9538e+00, -7.1425e-02, -1.7505e+00, -3.5783e-01],\n",
       "          [ 1.8213e+00,  4.5503e-01,  4.4617e-01, -6.1684e-01],\n",
       "          [ 4.9954e-01, -1.5206e-02,  4.4372e-01,  7.6883e-01]],\n",
       "\n",
       "         [[ 5.1626e-01, -2.0851e+00,  6.3266e-01, -6.9705e-01],\n",
       "          [ 3.8760e-01,  4.9122e-01, -8.1681e-01, -5.4353e-01],\n",
       "          [-3.0987e-01,  8.5079e-01, -1.4253e+00, -1.8715e+00],\n",
       "          [-4.7243e-01,  1.1677e+00,  3.2027e-01, -3.9756e-01]],\n",
       "\n",
       "         [[ 6.6441e-01, -2.7888e-01, -9.1732e-01,  5.2301e-02],\n",
       "          [ 2.4174e-01, -3.8863e-01,  2.5564e+00, -1.8568e+00],\n",
       "          [-1.4144e+00,  1.6601e+00, -6.2815e-01,  2.1165e-01],\n",
       "          [-8.8526e-01,  9.9137e-01, -9.1870e-01,  9.7471e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 8.5411e-01, -1.4079e-01, -4.5996e-01,  1.9226e+00],\n",
       "          [ 4.4002e-01, -4.7346e-01, -6.6095e-01, -1.2353e+00],\n",
       "          [ 2.8805e-01,  2.1752e-01,  1.2827e+00,  2.0792e+00],\n",
       "          [-1.5756e+00,  5.8671e-03,  1.7082e+00, -6.7465e-01]],\n",
       "\n",
       "         [[-1.6744e-02, -5.8505e-01, -9.6304e-01,  9.8751e-01],\n",
       "          [-7.0186e-01,  9.4427e-01, -1.2360e+00, -1.1824e+00],\n",
       "          [-1.8416e-01,  2.0272e+00, -9.7819e-01,  1.2365e+00],\n",
       "          [ 1.6972e+00,  1.3064e+00,  4.6088e-01,  1.1749e+00]],\n",
       "\n",
       "         [[ 1.2804e+00,  1.2733e+00,  9.1404e-01, -5.6902e-01],\n",
       "          [-3.8190e-01, -9.2348e-01,  1.2402e+00,  3.2749e-01],\n",
       "          [ 3.9479e-01, -1.3263e-01,  7.4480e-02,  3.8929e-01],\n",
       "          [ 1.0001e+00,  3.9114e-01,  6.6925e-01, -3.6250e-01]],\n",
       "\n",
       "         [[ 1.4952e+00,  1.4829e-01, -6.7750e-01,  1.3470e+00],\n",
       "          [ 2.8724e-01, -4.1802e-01, -1.0474e+00,  9.9734e-01],\n",
       "          [-4.9104e-01,  1.1030e+00,  1.4318e-01,  9.4340e-01],\n",
       "          [ 4.9610e-01, -9.8606e-01,  5.2903e-01, -1.5625e+00]],\n",
       "\n",
       "         [[-2.0098e-01, -5.0178e-01,  2.1208e+00, -1.9780e-01],\n",
       "          [ 2.3420e+00,  4.0106e-01, -9.6023e-01,  1.4530e-01],\n",
       "          [-9.4389e-01, -4.7905e-01,  8.4070e-02, -7.7703e-01],\n",
       "          [ 6.7524e-01,  6.4435e-01,  1.5233e+00, -1.6391e+00]]],\n",
       "\n",
       "\n",
       "        [[[-1.4528e-01, -1.2144e+00,  1.0056e+00,  2.0253e+00],\n",
       "          [-1.1327e+00,  3.5422e-01, -1.0266e-01, -1.0216e+00],\n",
       "          [-6.0221e-01, -1.0852e-01,  2.0836e-01,  1.7681e+00],\n",
       "          [-1.6425e+00, -5.7871e-01,  3.4348e-02, -5.0475e-01]],\n",
       "\n",
       "         [[ 2.6669e-01,  3.9647e-02,  2.4831e-02,  1.7188e+00],\n",
       "          [-9.4571e-01, -7.1550e-02, -1.7187e+00,  2.1262e+00],\n",
       "          [-2.5101e-01, -4.2545e-01, -8.3281e-01,  1.1361e+00],\n",
       "          [ 8.4980e-01,  1.7948e+00, -9.1904e-01, -4.5934e-01]],\n",
       "\n",
       "         [[ 2.1983e-01, -1.7947e+00, -2.5603e+00, -1.9525e-01],\n",
       "          [-1.3886e+00,  3.7017e-01,  1.6856e-01, -1.1704e+00],\n",
       "          [-2.0337e+00, -9.0179e-01, -1.6478e-02,  1.0710e+00],\n",
       "          [-1.1617e+00, -2.9705e-01, -3.8188e-01, -1.4006e-01]],\n",
       "\n",
       "         [[ 1.7651e+00, -1.5793e-01, -1.6319e-01, -7.5060e-01],\n",
       "          [-3.2650e-01, -7.6616e-01,  1.0208e+00, -1.6736e-01],\n",
       "          [ 1.1899e+00, -1.8894e+00,  1.0354e+00,  1.1029e+00],\n",
       "          [-2.8614e-01,  1.6467e-01, -1.0387e-01, -2.3302e-01]],\n",
       "\n",
       "         [[ 1.2813e-01,  9.1702e-01,  2.0176e+00, -7.2630e-02],\n",
       "          [ 6.4941e-01, -3.0242e-02, -7.6090e-01,  1.3246e+00],\n",
       "          [-8.3618e-01,  1.0300e+00, -7.1378e-01, -9.9908e-01],\n",
       "          [ 1.3054e+00,  8.5191e-01, -1.1092e+00,  4.9509e-01]]],\n",
       "\n",
       "\n",
       "        [[[-8.5098e-01,  2.9827e-02, -8.0978e-01,  8.1100e-01],\n",
       "          [ 7.1080e-03, -5.7866e-01, -9.2739e-01, -1.4993e+00],\n",
       "          [-6.0394e-01, -4.3079e-01,  6.7823e-02,  3.6457e-01],\n",
       "          [ 1.5380e+00,  9.7004e-01, -1.1404e+00, -1.8412e+00]],\n",
       "\n",
       "         [[-4.1824e-01, -1.2558e+00, -7.2759e-01,  3.9580e-01],\n",
       "          [-7.5232e-01,  7.9374e-02, -3.1311e-01,  3.8938e-02],\n",
       "          [-6.2674e-02,  8.4168e-01,  6.2125e-02,  9.0617e-01],\n",
       "          [ 1.0313e+00, -1.2334e+00, -1.8282e+00,  7.1337e-01]],\n",
       "\n",
       "         [[-9.5536e-01,  1.1021e+00, -4.0973e-01, -2.0955e+00],\n",
       "          [-9.6094e-02, -1.3681e+00, -5.6422e-01,  5.7673e-01],\n",
       "          [-6.6747e-02,  5.1652e-01, -2.1394e-01, -7.3456e-01],\n",
       "          [ 5.1635e-01,  3.9295e-01, -2.7970e+00,  1.5529e+00]],\n",
       "\n",
       "         [[-7.2798e-01,  1.1982e+00, -1.2034e+00, -4.4999e-01],\n",
       "          [ 3.4875e-01, -8.5097e-01, -1.6014e-01,  7.4394e-01],\n",
       "          [ 8.3235e-01, -9.6810e-01, -6.6616e-01,  5.3891e-01],\n",
       "          [-1.3793e-01,  1.0814e+00, -1.3625e+00, -3.1790e+00]],\n",
       "\n",
       "         [[ 2.8055e-01, -3.2892e-01,  1.0078e+00,  2.7217e-01],\n",
       "          [-1.1820e+00,  1.3828e+00,  1.8733e-01,  7.5096e-01],\n",
       "          [-4.4867e-01,  5.7199e-01, -9.5139e-01,  1.1595e+00],\n",
       "          [-9.1969e-01, -2.1889e+00, -2.1795e-01, -6.7772e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.4230e+00, -1.2827e+00, -6.4773e-01,  5.3069e-01],\n",
       "          [-9.4603e-01,  1.3846e+00,  1.9679e-01, -1.0207e+00],\n",
       "          [-1.9975e+00,  5.5610e-01,  9.6428e-01, -1.5693e+00],\n",
       "          [-5.8120e-01, -1.6824e+00,  1.3976e+00,  2.1085e+00]],\n",
       "\n",
       "         [[ 1.0263e+00,  1.5669e+00,  9.8732e-02, -4.8245e-01],\n",
       "          [ 6.9305e-01, -2.8643e-01,  1.0906e+00, -1.3857e+00],\n",
       "          [ 8.2051e-01, -1.1270e+00, -6.5998e-01,  8.2600e-01],\n",
       "          [-4.8435e-01,  5.8733e-01, -1.4307e+00,  6.3304e-01]],\n",
       "\n",
       "         [[-1.3710e+00, -2.1436e+00,  1.2375e+00, -5.8922e-01],\n",
       "          [-1.3469e-01, -2.5069e-01,  4.5243e-01,  1.1170e+00],\n",
       "          [ 1.2932e+00, -8.1922e-01,  5.4539e-01, -9.0520e-01],\n",
       "          [ 5.3671e-01, -1.9746e-01, -1.6610e+00,  5.1803e-01]],\n",
       "\n",
       "         [[-7.3704e-01,  8.3018e-02, -9.0494e-01,  5.1647e-01],\n",
       "          [ 8.9332e-01, -6.0252e-01,  1.0689e+00,  3.8105e-01],\n",
       "          [-7.6559e-02,  1.4634e+00, -8.5146e-02,  5.6729e-01],\n",
       "          [-5.8978e-02,  2.9077e-02,  2.8729e-01, -1.1237e-01]],\n",
       "\n",
       "         [[ 9.1113e-01, -4.2713e-01,  1.1678e+00,  5.5829e-02],\n",
       "          [-5.5915e-01, -7.5248e-01,  4.8039e-01, -1.7192e-01],\n",
       "          [ 2.3167e+00, -8.1557e-01,  7.7049e-01, -8.0069e-01],\n",
       "          [ 2.6477e-01,  1.3390e+00, -5.8780e-01,  1.2042e+00]]],\n",
       "\n",
       "\n",
       "        [[[-4.7799e-02,  2.5133e+00, -6.5872e-01,  1.2875e+00],\n",
       "          [-1.4708e-01,  1.6382e-02, -8.0350e-01,  7.2154e-01],\n",
       "          [ 1.0486e+00,  1.4450e+00, -1.1485e+00,  1.1143e+00],\n",
       "          [-8.4803e-01,  1.2372e-02, -1.8934e+00,  6.2387e-01]],\n",
       "\n",
       "         [[-5.8095e-01, -1.1230e+00,  1.1767e+00,  6.7539e-01],\n",
       "          [-1.0221e+00, -8.7211e-01,  2.2970e-01, -5.1538e-01],\n",
       "          [ 2.1181e+00,  1.7756e+00, -6.9463e-01, -1.5532e-01],\n",
       "          [-1.5628e-01,  4.0780e-02, -3.5777e-01, -7.9609e-01]],\n",
       "\n",
       "         [[-3.5896e-01, -7.2146e-01,  5.8672e-01,  5.9493e-01],\n",
       "          [-8.1050e-01, -2.0215e+00, -8.9023e-01, -2.5140e-01],\n",
       "          [-5.9221e-01,  3.2656e-01, -1.9404e+00, -1.1726e+00],\n",
       "          [-7.3787e-01, -2.3600e+00, -1.1365e+00,  1.2152e-01]],\n",
       "\n",
       "         [[ 6.3743e-01,  1.1481e+00,  6.8125e-01,  4.4055e-01],\n",
       "          [ 2.8476e-01, -1.2878e+00,  9.3413e-01, -1.4156e+00],\n",
       "          [ 1.3514e-01,  4.4844e-02, -2.5468e-01,  6.0548e-01],\n",
       "          [ 1.3280e+00, -9.0382e-01,  8.0037e-01, -1.8158e+00]],\n",
       "\n",
       "         [[-4.3236e-01,  9.5112e-01,  2.6078e-02, -8.3347e-01],\n",
       "          [ 1.0782e+00,  1.0826e+00,  4.5012e-01,  1.3466e-01],\n",
       "          [ 1.0359e-01,  5.4636e-01,  1.1700e+00,  1.8953e-01],\n",
       "          [ 4.9144e-01,  1.4636e-01,  1.4520e+00,  9.4950e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.1518e-01,  3.8338e-02, -8.0790e-01,  8.2000e-01],\n",
       "          [ 4.4115e-01,  1.0701e+00,  9.7729e-01,  1.1446e+00],\n",
       "          [ 4.7575e-01,  1.7557e-01, -9.1124e-01,  1.9234e-01],\n",
       "          [-1.8970e+00,  1.9903e-01, -4.7748e-01,  2.4640e+00]],\n",
       "\n",
       "         [[-4.1412e-01,  1.1166e+00, -1.9670e+00, -9.3118e-02],\n",
       "          [ 1.0180e+00,  1.2118e+00,  1.0237e+00, -2.0243e+00],\n",
       "          [-2.8915e-01,  1.8342e+00, -6.9385e-01,  1.0136e+00],\n",
       "          [ 1.7862e-01,  5.0190e-01,  9.1958e-01,  1.7784e-01]],\n",
       "\n",
       "         [[ 8.6724e-01,  5.1904e-01,  1.8337e+00,  2.4936e-01],\n",
       "          [-2.3488e-02, -7.6858e-01,  3.5942e-01,  5.4792e-02],\n",
       "          [ 7.5185e-01, -9.0920e-01, -6.9346e-01, -8.9782e-02],\n",
       "          [-1.8352e+00,  1.1977e+00, -1.5806e-01,  1.0841e+00]],\n",
       "\n",
       "         [[-2.0023e+00,  9.7873e-01, -2.3510e-01, -1.2064e-01],\n",
       "          [ 3.2778e-01,  1.0221e-02, -1.5931e+00, -3.8158e-01],\n",
       "          [-1.4134e+00, -1.8253e+00,  8.3570e-01, -1.9947e-03],\n",
       "          [ 1.4017e+00,  3.1338e-01, -1.3754e+00,  1.1153e+00]],\n",
       "\n",
       "         [[-4.3511e-01, -8.6626e-01,  2.9169e-01, -2.1154e+00],\n",
       "          [-1.3038e+00,  1.2810e+00, -1.0570e+00,  9.8999e-01],\n",
       "          [-1.9927e+00,  1.3580e+00, -1.3337e+00,  9.6701e-01],\n",
       "          [-9.1993e-01, -1.5865e+00, -1.2045e+00, -3.3228e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.6998e-01, -8.2750e-01,  2.8479e-01,  3.1080e-01],\n",
       "          [-4.8005e-01,  1.0459e+00, -3.4289e-01,  8.3390e-01],\n",
       "          [-1.1884e+00, -2.3039e-01,  7.1923e-01, -4.2334e-01],\n",
       "          [-1.1472e+00, -9.1375e-01, -1.2548e+00,  1.2815e+00]],\n",
       "\n",
       "         [[ 1.4915e-01, -9.6142e-02, -4.1310e-01,  4.5889e-02],\n",
       "          [ 5.4780e-01, -3.3337e-01, -7.3116e-01, -2.9561e-01],\n",
       "          [ 1.0882e+00,  6.5868e-02,  1.6538e-01,  3.4795e-01],\n",
       "          [-3.7871e-01,  1.0249e+00,  1.7850e+00,  1.3535e+00]],\n",
       "\n",
       "         [[-1.8688e+00, -1.6023e+00,  9.5732e-01,  2.6037e-01],\n",
       "          [ 4.3183e-01, -1.6706e+00, -4.6861e-01, -2.1245e+00],\n",
       "          [-2.6538e-01,  5.7490e-01, -1.5282e+00,  1.6099e+00],\n",
       "          [ 4.8484e-01, -3.3317e-01, -8.2136e-01, -1.8570e+00]],\n",
       "\n",
       "         [[ 2.3119e+00,  2.0212e-01,  1.2814e-01, -3.7424e-01],\n",
       "          [ 2.6964e-01, -2.1957e-01, -1.2322e+00, -1.4888e-02],\n",
       "          [-2.2166e+00, -1.9315e-01,  2.0875e+00, -9.9234e-01],\n",
       "          [-7.2910e-02,  1.1604e+00, -8.2046e-01,  2.2093e+00]],\n",
       "\n",
       "         [[ 1.4975e+00,  6.5460e-01, -1.6791e+00,  5.7435e-01],\n",
       "          [-9.5246e-01, -2.0844e-01, -7.0477e-01, -1.7094e+00],\n",
       "          [ 1.1892e+00, -1.1758e+00, -5.4757e-03,  5.9981e-01],\n",
       "          [-9.0864e-01,  1.1139e+00,  1.7115e+00,  3.4402e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.0445e-01, -8.7644e-01, -2.0054e-01, -1.2665e+00],\n",
       "          [ 2.2018e+00,  1.5964e+00,  1.6571e+00, -4.7530e-01],\n",
       "          [-1.2392e+00,  2.8490e-01,  3.8575e-01, -9.6754e-01],\n",
       "          [-2.7140e-02, -6.5488e-01, -1.2949e-01,  1.0551e+00]],\n",
       "\n",
       "         [[-9.0379e-01, -1.1746e-01,  8.6639e-03, -1.9275e-02],\n",
       "          [-5.7271e-01,  5.4609e-01,  1.9900e-02, -1.1271e+00],\n",
       "          [-9.3153e-01, -6.4726e-01, -2.9966e-01, -3.0680e-01],\n",
       "          [ 2.0507e-01, -1.0066e+00, -7.9067e-01, -4.8482e-01]],\n",
       "\n",
       "         [[-9.4636e-01, -5.8670e-01,  1.2884e+00, -1.1309e+00],\n",
       "          [-4.2506e-02,  1.2170e-01,  1.1384e+00,  1.3565e+00],\n",
       "          [ 3.1184e-01,  1.6576e-01, -3.6241e-01, -7.3425e-01],\n",
       "          [ 1.1574e-01, -1.6562e+00,  3.8258e-02, -3.4908e-02]],\n",
       "\n",
       "         [[-4.5905e-01,  9.9646e-01,  8.2948e-02,  1.4327e+00],\n",
       "          [ 2.0450e+00, -1.3238e+00,  1.0999e+00,  1.2909e-01],\n",
       "          [ 1.6498e+00,  1.5863e+00,  1.3788e+00, -1.1040e+00],\n",
       "          [ 1.2618e-01,  1.3593e+00,  1.3069e+00,  9.2345e-02]],\n",
       "\n",
       "         [[-9.3038e-01, -5.0516e-01, -8.3015e-01,  1.1183e+00],\n",
       "          [ 9.8178e-01,  1.1917e-02, -3.7628e-01, -2.0041e-02],\n",
       "          [ 9.2492e-01, -1.1911e-01, -1.1774e+00, -5.4928e-03],\n",
       "          [-3.9663e-01, -1.9163e+00,  1.8818e+00, -3.5154e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9737e+00, -9.4061e-03, -1.0948e+00,  4.8829e-01],\n",
       "          [-1.2299e+00,  2.8213e-01, -8.2643e-01, -9.7332e-01],\n",
       "          [-3.7367e-02,  3.0301e+00,  6.9472e-01, -3.7477e-01],\n",
       "          [-2.6270e-01, -6.6064e-01, -6.5184e-01,  1.4470e+00]],\n",
       "\n",
       "         [[-1.0854e-01,  7.7681e-01, -7.2263e-02, -1.5369e+00],\n",
       "          [ 3.9759e-01,  6.7642e-01, -2.0758e+00,  1.1063e-01],\n",
       "          [-1.6925e-01, -2.3881e-02,  9.7442e-02, -5.7205e-01],\n",
       "          [ 3.0789e-01,  5.3282e-01, -6.8828e-01, -1.0591e+00]],\n",
       "\n",
       "         [[ 8.8402e-02, -2.6184e-01,  4.4935e-02, -7.1720e-01],\n",
       "          [ 5.5470e-01, -1.4828e+00,  2.4339e-01, -4.4988e-01],\n",
       "          [-1.0658e+00,  2.1370e-01,  1.9690e-01, -1.3239e+00],\n",
       "          [-4.2803e-01, -1.5254e+00,  7.5737e-02,  9.9495e-01]],\n",
       "\n",
       "         [[-1.2079e+00,  2.0546e-01, -2.7789e+00, -6.1982e-01],\n",
       "          [-6.2037e-01,  1.4004e-01,  8.3062e-01,  3.5284e-01],\n",
       "          [ 1.4910e+00,  1.3343e+00, -7.4336e-01, -8.0683e-01],\n",
       "          [ 3.7078e-01,  3.9941e-01,  1.8679e+00, -1.0093e+00]],\n",
       "\n",
       "         [[ 9.2598e-01,  1.1377e+00, -2.0298e-01, -6.4767e-01],\n",
       "          [ 8.0823e-01, -4.7065e-01,  1.0002e+00, -1.0118e-01],\n",
       "          [-9.3336e-01, -5.2469e-01,  5.1067e-01, -1.9203e-01],\n",
       "          [-3.2204e-01, -7.3693e-01,  2.3474e-01, -1.8345e+00]]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ic(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchNorm layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```NormType``` with `Enum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "NormType = Enum('NormType', 'Batch BatchZero Weight Spectral Instance InstanceZero')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<NormType.Batch: 1>,\n",
       " <NormType.BatchZero: 2>,\n",
       " <NormType.Weight: 3>,\n",
       " <NormType.Spectral: 4>,\n",
       " <NormType.Instance: 5>,\n",
       " <NormType.InstanceZero: 6>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# help(Enum)\n",
    "list(NormType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Batch'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(NormType)[0].name\n",
    "list(NormType)[0].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature: (value, names=None, *, module=None, qualname=None, type=None, start=1)\n",
      "__class__: <class 'enum.EnumMeta'>\n",
      "__repr__: <enum 'NormType'>\n",
      "\n",
      "__module__: __main__\n",
      "__doc__:\n",
      "An enumeration.\n",
      "__dict__: \n",
      "mappingproxy({'Batch': <NormType.Batch: 1>,\n",
      "              'BatchZero': <NormType.BatchZero: 2>,\n",
      "              'Instance': <NormType.Instance: 5>,\n",
      "              'InstanceZero': <NormType.InstanceZero: 6>,\n",
      "              'Spectral': <NormType.Spectral: 4>,\n",
      "              'Weight': <NormType.Weight: 3>,\n",
      "              '__doc__': 'An enumeration.',\n",
      "              '__module__': '__main__',\n",
      "              '__new__': <function Enum.__new__>,\n",
      "              '_generate_next_value_': <function Enum._generate_next_value_>,\n",
      "              '_member_map_': {'Batch': <NormType.Batch: 1>,\n",
      "                               'BatchZero': <NormType.BatchZero: 2>,\n",
      "                               'Instance': <NormType.Instance: 5>,\n",
      "                               'InstanceZero': <NormType.InstanceZero: 6>,\n",
      "                               'Spectral': <NormType.Spectral: 4>,\n",
      "                               'Weight': <NormType.Weight: 3>},\n",
      "              '_member_names_': ['Batch',\n",
      "                                 'BatchZero',\n",
      "                                 'Weight',\n",
      "                                 'Spectral',\n",
      "                                 'Instance',\n",
      "                                 'InstanceZero'],\n",
      "              '_member_type_': <class 'object'>,\n",
      "              '_value2member_map_': {1: <NormType.Batch: 1>,\n",
      "                                     2: <NormType.BatchZero: 2>,\n",
      "                                     3: <NormType.Weight: 3>,\n",
      "                                     4: <NormType.Spectral: 4>,\n",
      "                                     5: <NormType.Instance: 5>,\n",
      "                                     6: <NormType.InstanceZero: 6>}})\n",
      "metaclass: False\n",
      "class: True\n",
      "decorator: False\n",
      "function: False\n",
      "method: False\n"
     ]
    }
   ],
   "source": [
    "check(NormType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signature: None\n",
      "__class__: <enum 'NormType'>\n",
      "__repr__: NormType.Batch\n",
      "\n",
      "__module__: __main__\n",
      "__doc__:\n",
      "An enumeration.\n",
      "__dict__: \n",
      "{'__objclass__': <enum 'NormType'>, '_name_': 'Batch', '_value_': 1}\n",
      "metaclass: False\n",
      "class: False\n",
      "decorator: False\n",
      "function: False\n",
      "method: False\n"
     ]
    }
   ],
   "source": [
    "check(list(NormType)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```_get_norm(prefix, nf, ndim=2, zero=False, **kwargs)```\n",
    "official doc: Norm layer with `nf` features and `ndim` initialized depending on `norm_type`.\n",
    "\n",
    "My doc: to create a `nn.BatchNorm` between 1d to 3d, and output `nf` activation, and can set `weight.data` to either 0 or 1\n",
    "- to get normalization layer\n",
    "- `prefix`: tell which type of normalization layer, like 'BatchNorm'\n",
    "- `ndim=2`: default to 2d, so we get `BatchNorm2d`\n",
    "- `nf`: like 15, to return 15 output or activation at the end of the BatchNorm2d layer\n",
    "- `zero`: True or False, to set BatchNorm layer's weight to be either 0 or 1\n",
    "- `bn.affine`: when it is False, then weight and bias will be None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop(watch=('bn.bias.data', 'bn.weight.data'))\n",
    "def _get_norm(prefix, nf, ndim=2, zero=False, **kwargs):\n",
    "    \"Norm layer with `nf` features and `ndim` initialized depending on `norm_type`.\"\n",
    "    assert 1 <= ndim <= 3\n",
    "#     pp.deep(lambda: getattr(nn, f\"{prefix}{ndim}d\")(nf, **kwargs))\n",
    "    bn = getattr(nn, f\"{prefix}{ndim}d\")(nf, **kwargs)\n",
    "    if bn.affine:\n",
    "        bn.bias.data.fill_(1e-3)\n",
    "        bn.weight.data.fill_(0. if zero else 1.)\n",
    "    return bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```BatchNorm(nf, ndim=2, norm_type=NormType.Batch, **kwargs)```\n",
    "Official doc:  BatchNorm layer with `nf` features and `ndim` initialized depending on `norm_type`.\n",
    "\n",
    "My doc: create a BatchNorm layer (2d, by default) by wrapping around `_get_norm`\n",
    "- use kwargs from `nn.BatchNorm2d`\n",
    "- `ndim=2`: by default to create a `nn.BatchNorm2d`\n",
    "- `nf`: like 15, to output 15 activations\n",
    "- `norm_type`: if not `NormType.BatchZero`, then make `wegith.data` all equals 1; otherwise, equals 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NormType.Batch: 1>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<NormType.BatchZero: 2>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NormType.Batch\n",
    "NormType.BatchZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@delegates(nn.BatchNorm2d) # pass its args to BatchNorm\n",
    "def BatchNorm(nf, ndim=2, norm_type=NormType.Batch, **kwargs):\n",
    "    \"BatchNorm layer with `nf` features and `ndim` initialized depending on `norm_type`.\"\n",
    "    return _get_norm('BatchNorm', nf, ndim, zero=norm_type==NormType.BatchZero, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(torch.nn.modules.batchnorm.BatchNorm2d) # to check the meaning of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.BatchNorm(nf, ndim=2, norm_type=<NormType.Batch: 1>, *, eps: float = 1e-05, momentum: float = 0.1, affine: bool = True, track_running_stats: bool = True, device=None, dtype=None)>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BatchNorm # receive kwargs from nn.BatchNorm2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = BatchNorm(15)\n",
    "assert isinstance(tst, nn.BatchNorm2d)\n",
    "test_eq(tst.weight, torch.ones(15))\n",
    "tst = BatchNorm(15, norm_type=NormType.BatchZero)\n",
    "test_eq(tst.weight, torch.zeros(15))\n",
    "tst = BatchNorm(15, ndim=1)\n",
    "assert isinstance(tst, nn.BatchNorm1d)\n",
    "tst = BatchNorm(15, ndim=3)\n",
    "assert isinstance(tst, nn.BatchNorm3d)\n",
    "test_eq(BatchNorm(15, affine=False).weight, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```InstanceNorm(nf, ndim=2, norm_type=NormType.Instance, affine=True, **kwargs)```\n",
    "official doc: InstanceNorm layer with `nf` features and `ndim` initialized depending on `norm_type`.\n",
    "\n",
    "mydoc: to create a InstanceNorm layer (1d-3d), any num of activations, set weight.data to 0 or 1, set `affine` True by default\n",
    "- wrapping around `_get_norm`\n",
    "- using kwargs from `nn.InstanceNorm2d`; \n",
    "- default to `NormType.Instance` and `weight.data` will be set to 1; if `NormType.InstanceZero` then `weight.data` is set to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@delegates(nn.InstanceNorm2d)\n",
    "def InstanceNorm(nf, ndim=2, norm_type=NormType.Instance, affine=True, **kwargs):\n",
    "    \"InstanceNorm layer with `nf` features and `ndim` initialized depending on `norm_type`.\"\n",
    "    return _get_norm('InstanceNorm', nf, ndim, zero=norm_type==NormType.InstanceZero, affine=affine, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kwargs` are passed to `nn.BatchNorm` and can be `eps`, `momentum`, `affine` and `track_running_stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = InstanceNorm(15)\n",
    "assert isinstance(tst, nn.InstanceNorm2d)\n",
    "test_eq(tst.weight, torch.ones(15))\n",
    "tst = InstanceNorm(15, norm_type=NormType.InstanceZero)\n",
    "test_eq(tst.weight, torch.zeros(15))\n",
    "tst = InstanceNorm(15, ndim=1)\n",
    "assert isinstance(tst, nn.InstanceNorm1d)\n",
    "tst = InstanceNorm(15, ndim=3)\n",
    "assert isinstance(tst, nn.InstanceNorm3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `affine` is false the weight should be `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(BatchNorm(15, affine=False).weight, None)\n",
    "test_eq(InstanceNorm(15, affine=False).weight, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```BatchNorm1dFlat(nn.BatchNorm1d)```, `running_mean`, `running_var`, `contiguous`\n",
    "official doc: `nn.BatchNorm1d`, but first flattens leading dimensions\n",
    "\n",
    "mydoc: allow high dim `x` to run through `nn.BatchNorm1d` by flattening leading dims first, and return `x` in its original shape\n",
    "- how to use `torch.Tensor.contiguous`: stackoverflow [answer](https://stackoverflow.com/questions/48915810/what-does-contiguous-do-in-pytorch)\n",
    "- how to access `bn.running_mean` and `bn.running_var`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class BatchNorm1dFlat(nn.BatchNorm1d):\n",
    "    \"`nn.BatchNorm1d`, but first flattens leading dimensions\"\n",
    "#     @snoop(watch=('snp.shape', 'help(x.contiguous)'))\n",
    "    def forward(self, x):\n",
    "        if x.dim()==2: \n",
    "            return super().forward(x)\n",
    "        *f,l = x.shape\n",
    "#         snp = x.contiguous()\n",
    "#         snp = snp.view(-1,1)\n",
    "        x = x.contiguous().view(-1,l)\n",
    "        return super().forward(x).view(*f,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check(BatchNorm1dFlat)\n",
    "# help(BatchNorm1dFlat)\n",
    "# help(torch.nn.modules.batchnorm._NormBase)\n",
    "# help(torch.nn.modules.module.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm1dFlat(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = BatchNorm1dFlat(15)\n",
    "tst\n",
    "tst.running_mean\n",
    "tst.running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 15])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 3.4016e-03,  8.1919e-04, -9.7062e-05, -2.5036e-03,  3.2423e-04,\n",
       "        -2.1983e-03, -1.6188e-03,  2.6421e-04, -2.3959e-03,  1.6011e-03,\n",
       "         1.6295e-03, -3.4547e-03,  1.4706e-03,  1.4990e-03, -6.8275e-04])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9978, 0.9982, 1.0006, 1.0035, 1.0039, 0.9943, 0.9998, 1.0038, 0.9978,\n",
       "        1.0001, 1.0034, 1.0029, 1.0004, 1.0056, 0.9969])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(32, 64, 15)\n",
    "y = tst(x)\n",
    "y.shape\n",
    "tst.running_mean\n",
    "tst.running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = x.mean(dim=[0,1])\n",
    "test_close(tst.running_mean, 0*0.9 + mean*0.1)\n",
    "var = (x-mean).pow(2).mean(dim=[0,1])\n",
    "test_close(tst.running_var, 1*0.9 + var*0.1, eps=1e-4)\n",
    "test_close(y, (x-mean)/torch.sqrt(var+1e-5) * tst.weight + tst.bias, eps=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```LinBnDrop(nn.Sequential)```\n",
    "official doc: Module grouping `BatchNorm1d`, `Dropout` and `Linear` layers\"\n",
    "\n",
    "mydoc: create a block of layers (BatchNorm1d, Dropout, Linear) together \n",
    "- `lin_first=False`: default to put linear layer to the end of the block\n",
    "- `act=None`: default to None, adding a something (None, or a layer like nn.ReLu, maybe) behind linear layer\n",
    "- `p=0.`: default to 0., as num of dropouts\n",
    "- `bn=True`: default to True, to have a BatchNorm layer or not; if True, the linear layer removes bias\n",
    "- `n_in, n_out`: num of input and output activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class LinBnDrop(nn.Sequential):\n",
    "    \"Module grouping `BatchNorm1d`, `Dropout` and `Linear` layers\"\n",
    "    def __init__(self, n_in, n_out, bn=True, p=0., act=None, lin_first=False):\n",
    "        layers = [BatchNorm(n_out if lin_first else n_in, ndim=1)] if bn else []\n",
    "        if p != 0: layers.append(nn.Dropout(p))\n",
    "        lin = [nn.Linear(n_in, n_out, bias=not bn)]\n",
    "        if act is not None: lin.append(act)\n",
    "        layers = lin+layers if lin_first else layers+lin\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BatchNorm` layer is skipped if `bn=False`, as is the dropout if `p=0.`. Optionally, you can add an activation for after the linear layer with `act`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinBnDrop(\n",
       "  (0): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): Linear(in_features=10, out_features=20, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " Linear(in_features=10, out_features=20, bias=False)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = LinBnDrop(10, 20)\n",
    "tst\n",
    "mods = list(tst.children())\n",
    "mods\n",
    "test_eq(len(mods), 2)\n",
    "assert isinstance(mods[0], nn.BatchNorm1d)\n",
    "assert isinstance(mods[1], nn.Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinBnDrop(\n",
       "  (0): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (1): Dropout(p=0.1, inplace=False)\n",
       "  (2): Linear(in_features=10, out_features=20, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " Dropout(p=0.1, inplace=False),\n",
       " Linear(in_features=10, out_features=20, bias=False)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = LinBnDrop(10, 20, p=0.1)\n",
    "tst\n",
    "mods = list(tst.children())\n",
    "mods\n",
    "test_eq(len(mods), 3)\n",
    "assert isinstance(mods[0], nn.BatchNorm1d)\n",
    "assert isinstance(mods[1], nn.Dropout)\n",
    "assert isinstance(mods[2], nn.Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinBnDrop(\n",
       "  (0): Linear(in_features=10, out_features=20, bias=False)\n",
       "  (1): ReLU()\n",
       "  (2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=10, out_features=20, bias=False),\n",
       " ReLU(),\n",
       " BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = LinBnDrop(10, 20, act=nn.ReLU(), lin_first=True)\n",
    "tst\n",
    "mods = list(tst.children())\n",
    "mods\n",
    "test_eq(len(mods), 3)\n",
    "assert isinstance(mods[0], nn.Linear)\n",
    "assert isinstance(mods[1], nn.ReLU)\n",
    "assert isinstance(mods[2], nn.BatchNorm1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinBnDrop(\n",
       "  (0): Linear(in_features=10, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=10, out_features=20, bias=True)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = LinBnDrop(10, 20, bn=False)\n",
    "tst\n",
    "mods = list(tst.children())\n",
    "mods\n",
    "test_eq(len(mods), 1)\n",
    "assert isinstance(mods[0], nn.Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```clamp(min, max)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function clamp:\n",
      "\n",
      "clamp(...) method of torch.Tensor instance\n",
      "    clamp(min=None, max=None) -> Tensor\n",
      "    \n",
      "    See :func:`torch.clamp`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(x.clamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5912,  0.3023, -0.6173],\n",
       "        [-0.2088,  0.6322,  0.3258]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.8308, 0.5750, 0.3504],\n",
       "        [0.4480, 0.6530, 0.5807]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000, 0.3504],\n",
       "        [0.4480, 0.5000, 0.5000]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "x\n",
    "x.sigmoid()\n",
    "x.sigmoid().clamp(0,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```sigmoid(input, eps=1e-7)```\n",
    "official docs: Same as `torch.sigmoid`, plus clamping to `(eps,1-eps)`\n",
    "\n",
    "mydoc: wrap around `torch.sigmoid` and clamping values to be within `[eps, 1-eps]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def sigmoid(input, eps=1e-7):\n",
    "    \"Same as `torch.sigmoid`, plus clamping to `(eps,1-eps)\"\n",
    "    return input.sigmoid().clamp(eps,1-eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2419, -1.7405, -0.4980],\n",
       "        [ 1.6586, -0.2702,  1.0093]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4398, 0.1493, 0.3780],\n",
       "        [0.8401, 0.4329, 0.7329]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4398, 0.1493, 0.3780],\n",
       "        [0.5000, 0.4329, 0.5000]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "x\n",
    "x.sigmoid()\n",
    "x.sigmoid().clamp(0,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4398, 0.1493, 0.3780],\n",
       "        [0.8401, 0.4329, 0.7329]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2419, -1.7405, -0.4980],\n",
       "        [ 1.6586, -0.2702,  1.0093]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```sigmoid_(input, eps=1e-7)```\n",
    "official docs: Same as `torch.sigmoid_`, plus clamping to `(eps,1-eps)`\n",
    "\n",
    "mydoc: inplace version of `sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def sigmoid_(input, eps=1e-7):\n",
    "    \"Same as `torch.sigmoid_`, plus clamping to `(eps,1-eps)\"\n",
    "    return input.sigmoid_().clamp_(eps,1-eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3758, -0.4757, -0.3201],\n",
       "        [ 1.6543, -2.6505, -1.0499]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5929, 0.3833, 0.4206],\n",
       "        [0.8395, 0.0660, 0.2592]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5929, 0.3833, 0.4206],\n",
       "        [0.8395, 0.0660, 0.2592]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "x\n",
    "sigmoid_(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```kaiming_uniform_,uniform_,xavier_uniform_,normal_``` from `torch.nn.init`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from torch.nn.init import kaiming_uniform_,uniform_,xavier_uniform_,normal_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```vleaky_relu(input, inplace=True)```\n",
    "original docs: `F.leaky_relu` with 0.3 slope\n",
    "\n",
    "mydoc: wrap `F.leaky_rely` and set `negative_slop` to 0.3 and set `inplace` True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def vleaky_relu(input, inplace=True):\n",
    "    \"`F.leaky_relu` with 0.3 slope\"\n",
    "    return F.leaky_relu(input, negative_slope=0.3, inplace=inplace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6327,  0.6887,  0.2330],\n",
       "        [ 0.7055, -0.4074, -0.8898]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0063,  0.6887,  0.2330],\n",
       "        [ 0.7055, -0.0041, -0.0089]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1898,  0.6887,  0.2330],\n",
       "        [ 0.7055, -0.1222, -0.2669]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "x\n",
    "F.leaky_relu(x)\n",
    "vleaky_relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```__default_init__``` of all ReLus are set to ```kaiming_uniform_```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "for o in F.relu,nn.ReLU,F.relu6,nn.ReLU6,F.leaky_relu,nn.LeakyReLU:\n",
    "    o.__default_init__ = kaiming_uniform_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```__default_init__``` of all sigmoid are set to ```xavier_uniform_```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "for o in F.sigmoid,nn.Sigmoid,F.tanh,nn.Tanh,sigmoid,sigmoid_:\n",
    "    o.__default_init__ = xavier_uniform_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```nested_callable(m, 'bias.fill_')```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1619, -0.1003])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Linear(3,2)\n",
    "m.bias.data\n",
    "with torch.no_grad(): nested_callable(m, 'bias.fill_')(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```init_default(m, func=nn.init.kaiming_normal_)```\n",
    "official docs:Initialize `m` weights with `func` and set `bias` to 0.\n",
    "\n",
    "mydoc: \n",
    "- initialize a model `m.weight` with `func` which default to `kaiming_normal_`\n",
    "- initialize `m.bias` with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def init_default(m, func=nn.init.kaiming_normal_):\n",
    "    \"Initialize `m` weights with `func` and set `bias` to 0.\"\n",
    "    if func and hasattr(m, 'weight'): func(m.weight)\n",
    "    with torch.no_grad(): nested_callable(m, 'bias.fill_')(0.)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4545,  0.3245, -0.0279],\n",
       "        [-0.2206, -0.4405,  0.5584]], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.3999,  0.1045], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3, out_features=2, bias=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.0997, -0.6984,  0.2521],\n",
       "        [-0.0327,  0.4805,  0.2035]], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Linear(3,2)\n",
    "m.weight\n",
    "m.bias\n",
    "init_default(m)\n",
    "m.weight\n",
    "m.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```init_linear(m, act_func=None, init='auto', bias_std=0.01)```\n",
    "mydoc: initialize a linear layer or any layer's weight and bias\n",
    "- normalize bias with 0 mean and bias_std=0.01 by default; if bias is not available or bias_std is None, then set biase to be 0\n",
    "- normalize weight with `kaiming_uniform_`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def init_linear(m, act_func=None, init='auto', bias_std=0.01):\n",
    "    if getattr(m,'bias',None) is not None and bias_std is not None:\n",
    "        if bias_std != 0: normal_(m.bias, 0, bias_std)\n",
    "        else: m.bias.data.zero_()\n",
    "    if init=='auto':\n",
    "        if act_func in (F.relu_,F.leaky_relu_): init = kaiming_uniform_\n",
    "        else: init = nested_callable(act_func, '__class__.__default_init__')\n",
    "        if init == noop: init = getcallable(act_func, '__default_init__')\n",
    "    if callable(init): init(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.nn.init.normal_(tensor: torch.Tensor, mean: float = 0.0, std: float = 1.0) -> torch.Tensor>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```_conv_func(ndim=2, transpose=False)```\n",
    "official: Return the proper conv `ndim` function, potentially a `transposed`\n",
    "\n",
    "mydoc: return a conv layer with 1d to 3d, can be transposed if set True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _conv_func(ndim=2, transpose=False):\n",
    "    \"Return the proper conv `ndim` function, potentially `transposed`.\"\n",
    "    assert 1 <= ndim <=3\n",
    "    return getattr(nn, f'Conv{\"Transpose\" if transpose else \"\"}{ndim}d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "test_eq(_conv_func(ndim=1),torch.nn.modules.conv.Conv1d)\n",
    "test_eq(_conv_func(ndim=2),torch.nn.modules.conv.Conv2d)\n",
    "test_eq(_conv_func(ndim=3),torch.nn.modules.conv.Conv3d)\n",
    "test_eq(_conv_func(ndim=1, transpose=True),torch.nn.modules.conv.ConvTranspose1d)\n",
    "test_eq(_conv_func(ndim=2, transpose=True),torch.nn.modules.conv.ConvTranspose2d)\n",
    "test_eq(_conv_func(ndim=3, transpose=True),torch.nn.modules.conv.ConvTranspose3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```defaults.activation``` is set to `nn.ReLU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "defaults.activation=nn.ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```weight_norm```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=20, out_features=40, bias=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=20, out_features=40, bias=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 20])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# help(weight_norm)\n",
    "nn.Linear(20, 40)\n",
    "m = weight_norm(nn.Linear(20, 40), name='weight')\n",
    "m\n",
    "m.weight_g.size()\n",
    "m.weight_v.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```ConvLayer(nn.Sequential)```\n",
    "official:    Create a sequence of convolutional (`ni` to `nf`), ReLU (if `use_activ`) and `norm_type` layers.\n",
    "\n",
    "mydoc: create a block/sequence of layers including convolutional, ReLU and norm_type layers\n",
    "- use `padding` and `transpose` to set padding to be `(ks-1)/2` or 0\n",
    "- set `bn` True if either `NormType.Batch` or `NormType.BatchZero`\n",
    "- set `inn` True if either `NormType.Instance` or `NormType.InstanceZero`\n",
    "- set `bias` True, if `bn` or `inn` is False and `bias` is given as None\n",
    "- `conv_func` is assigned to a conv layer class created by `_conv_func` with `ndim` dimension and `transpose` or not\n",
    "- `conv` is assigned to an actual conv layer object by running `conv_func(ni, nf, kernel_size=ks, bias=bias, stride=stride, padding=padding, **kwargs)`\n",
    "- `act` is assigned to None or a layer class by calling `act_cls()` which gives us ReLU\n",
    "- use `init_linear(conv, act, init=init, bias_std=bias_std)` to initialize weight and bias of `conv` \n",
    "- use `weight_norm` or `spectral_norm` to normalize the weight of `conv` if `norm_type == NormType.Weight` or `==NormType.Spectral`\n",
    "- create a list `act_bn` to store `act` layer, `BatchNorm(nf, norm_type=norm_type, ndim=ndim)`, `InstanceNorm(nf, norm_type=norm_type, ndim=ndim)` if `act is not None`, `bn, inn` are True respectively; and reverse the list order if `bn_1st` True\n",
    "- put `conv` layer in the front of the `act_bn` list and assign the new list to `layers`\n",
    "- if there is `xtra` layer, then add it to the end of the list `layers`\n",
    "- finally asking the `super()` i.e., `nn.Sequential` initialze all the layers inside `layers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ConvLayer(nn.Sequential):\n",
    "    \"Create a sequence of convolutional (`ni` to `nf`), ReLU (if `use_activ`) and `norm_type` layers.\"\n",
    "    @delegates(nn.Conv2d)\n",
    "    def __init__(self, ni, nf, ks=3, stride=1, padding=None, bias=None, ndim=2, norm_type=NormType.Batch, bn_1st=True,\n",
    "                 act_cls=defaults.activation, transpose=False, init='auto', xtra=None, bias_std=0.01, **kwargs):\n",
    "        if padding is None: padding = ((ks-1)//2 if not transpose else 0)\n",
    "        bn = norm_type in (NormType.Batch, NormType.BatchZero)\n",
    "        inn = norm_type in (NormType.Instance, NormType.InstanceZero)\n",
    "        if bias is None: bias = not (bn or inn)\n",
    "        conv_func = _conv_func(ndim, transpose=transpose)\n",
    "        conv = conv_func(ni, nf, kernel_size=ks, bias=bias, stride=stride, padding=padding, **kwargs)\n",
    "        act = None if act_cls is None else act_cls()\n",
    "        init_linear(conv, act, init=init, bias_std=bias_std)\n",
    "        if   norm_type==NormType.Weight:   conv = weight_norm(conv)\n",
    "        elif norm_type==NormType.Spectral: conv = spectral_norm(conv)\n",
    "        layers = [conv]\n",
    "        act_bn = []\n",
    "        if act is not None: act_bn.append(act)\n",
    "        if bn: act_bn.append(BatchNorm(nf, norm_type=norm_type, ndim=ndim))\n",
    "        if inn: act_bn.append(InstanceNorm(nf, norm_type=norm_type, ndim=ndim))\n",
    "        if bn_1st: act_bn.reverse()\n",
    "        layers += act_bn\n",
    "        if xtra: layers.append(xtra)\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution uses `ks` (kernel size) `stride`, `padding` and `bias`. `padding` will default to the appropriate value (`(ks-1)//2` if it's not a transposed conv) and `bias` will default to `True` the `norm_type` is `Spectral` or `Weight`, `False` if it's `Batch` or `BatchZero`. Note that if you don't want any normalization, you should pass `norm_type=None`.\n",
    "\n",
    "This defines a conv layer with `ndim` (1,2 or 3) that will be a ConvTranspose if `transpose=True`. `act_cls` is the class of the activation function to use (instantiated inside). Pass `act=None` if you don't want an activation function. If you quickly want to change your default activation, you can change the value of `defaults.activation`.\n",
    "\n",
    "`init` is used to initialize the weights (the bias are initialized to 0) and `xtra` is an optional layer to add at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvLayer(\n",
       "  (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = ConvLayer(16, 32)\n",
    "tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
       " BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " ReLU()]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mods = list(tst.children())\n",
    "mods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(mods), 3)\n",
    "test_eq(mods[1].weight, torch.ones(32))\n",
    "test_eq(mods[0].padding, (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64, 16, 8, 8)#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding is selected to make the shape the same if stride=1\n",
    "test_eq(tst(x).shape, [64,32,8,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding is selected to make the shape half if stride=2\n",
    "tst = ConvLayer(16, 32, stride=2)\n",
    "test_eq(tst(x).shape, [64,32,4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#But you can always pass your own padding if you want\n",
    "tst = ConvLayer(16, 32, padding=0)\n",
    "test_eq(tst(x).shape, [64,32,6,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No bias by default for Batch NormType\n",
    "assert mods[0].bias is None\n",
    "#But can be overridden with `bias=True`\n",
    "tst = ConvLayer(16, 32, bias=True)\n",
    "assert first(tst.children()).bias is not None\n",
    "#For no norm, or spectral/weight, bias is True by default\n",
    "for t in [None, NormType.Spectral, NormType.Weight]:\n",
    "    tst = ConvLayer(16, 32, norm_type=t)\n",
    "    assert first(tst.children()).bias is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Various n_dim/tranpose\n",
    "tst = ConvLayer(16, 32, ndim=3)\n",
    "assert isinstance(list(tst.children())[0], nn.Conv3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = ConvLayer(16, 32, ndim=1, transpose=True)\n",
    "assert isinstance(list(tst.children())[0], nn.ConvTranspose1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvLayer(\n",
       "  (0): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "  (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False),\n",
       " BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#No activation/leaky\n",
    "tst = ConvLayer(16, 32, ndim=3, act_cls=None)\n",
    "tst\n",
    "mods = list(tst.children())\n",
    "mods\n",
    "test_eq(len(mods), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvLayer(\n",
       "  (0): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "  (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): LeakyReLU(negative_slope=0.1)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False),\n",
       " BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " LeakyReLU(negative_slope=0.1)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = ConvLayer(16, 32, ndim=3, act_cls=partial(nn.LeakyReLU, negative_slope=0.1))\n",
    "tst\n",
    "mods = list(tst.children())\n",
    "mods\n",
    "test_eq(len(mods), 3)\n",
    "assert isinstance(mods[2], nn.LeakyReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# def linear(in_features, out_features, bias=True, act_cls=None, init='auto'):\n",
    "#     \"Linear layer followed by optional activation, with optional auto-init\"\n",
    "#     res = nn.Linear(in_features, out_features, bias=bias)\n",
    "#     if act_cls: act_cls = act_cls()\n",
    "#     init_linear(res, act_cls, init=init)\n",
    "#     if act_cls: res = nn.Sequential(res, act_cls)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# @delegates(ConvLayer)\n",
    "# def conv1d(ni, nf, ks, stride=1, ndim=1, norm_type=None, **kwargs):\n",
    "#     \"Convolutional layer followed by optional activation, with optional auto-init\"\n",
    "#     return ConvLayer(ni, nf, ks, stride=stride, ndim=ndim, norm_type=norm_type, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# @delegates(ConvLayer)\n",
    "# def conv2d(ni, nf, ks, stride=1, ndim=2, norm_type=None, **kwargs):\n",
    "#     \"Convolutional layer followed by optional activation, with optional auto-init\"\n",
    "#     return ConvLayer(ni, nf, ks, stride=stride, ndim=ndim, norm_type=norm_type, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# @delegates(ConvLayer)\n",
    "# def conv3d(ni, nf, ks, stride=1, ndim=3, norm_type=None, **kwargs):\n",
    "#     \"Convolutional layer followed by optional activation, with optional auto-init\"\n",
    "#     return ConvLayer(ni, nf, ks, stride=stride, ndim=ndim, norm_type=norm_type, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```AdaptiveAvgPool(sz=1, ndim=2)```\n",
    "official: nn.AdaptiveAvgPool layer for `ndim`\n",
    "\n",
    "instantiate an AdaptiveAvgPool2d layer object with 1 activation output by default\n",
    "- it can be 1d to 3d\n",
    "- it can output any number of activations with `sz` arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def AdaptiveAvgPool(sz=1, ndim=2):\n",
    "    \"nn.AdaptiveAvgPool layer for `ndim`\"\n",
    "    assert 1 <= ndim <= 3\n",
    "    return getattr(nn, f\"AdaptiveAvgPool{ndim}d\")(sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaptiveAvgPool3d(output_size=3)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaptiveAvgPool(3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```MaxPool(ks=2, stride=None, padding=0, ndim=2, ceil_mode=False)```\n",
    "official: nn.MaxPool layer for `ndim`\n",
    "\n",
    "instantiate an nn.MaxPool2d layer object with kernel size 2, stride 2, padding 0, no ceil_mode by default\n",
    "- it can be 1d to 3d\n",
    "- according to `nn.MaxPool2d`, by default `stride` is equal to `ks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def MaxPool(ks=2, stride=None, padding=0, ndim=2, ceil_mode=False):\n",
    "    \"nn.MaxPool layer for `ndim`\"\n",
    "    assert 1 <= ndim <= 3\n",
    "    return getattr(nn, f\"MaxPool{ndim}d\")(ks, stride=stride, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(nn.MaxPool2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MaxPool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxPool3d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MaxPool(3, ndim=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```AvgPool(ks=2, stride=None, padding=0, ndim=2, ceil_mode=False)```\n",
    "official: nn.AvgPool layer for `ndim`\n",
    "\n",
    "instantiate an nn.AvgPool2d layer object with kernel size 2, stride 2, padding 0, no ceil_mode by default\n",
    "- it can be 1d to 3d\n",
    "- according to `nn.AvgPool2d`, by default `stride` is equal to `ks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def AvgPool(ks=2, stride=None, padding=0, ndim=2, ceil_mode=False):\n",
    "    \"nn.AvgPool layer for `ndim`\"\n",
    "    assert 1 <= ndim <= 3\n",
    "    return getattr(nn, f\"AvgPool{ndim}d\")(ks, stride=stride, padding=padding, ceil_mode=ceil_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AvgPool2d(kernel_size=2, stride=2, padding=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "AvgPool3d(kernel_size=3, stride=5, padding=2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AvgPool()\n",
    "AvgPool(3, 5, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```trunc_normal_(x, mean=0., std=1.)```\n",
    "official: Truncated normal initialization (approximation)\n",
    "\n",
    "This is to implement a finding from a paper. There is discussion on how to implement it. https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def trunc_normal_(x, mean=0., std=1.):\n",
    "    \"Truncated normal initialization (approximation)\"\n",
    "    # From https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12\n",
    "    return x.normal_().fmod_(2).mul_(std).add_(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Embedding(nn.Embedding)```\n",
    "official: Embedding layer with truncated normal initialization\n",
    "\n",
    "- is a subclass of `nn.Embedding`\n",
    "- instantiate an embedding layer with `nn.Embedding(num_input, n_features, std=0.01)`\n",
    "- then apply truncated normalization on the weight using std=0.01 by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Embedding(nn.Embedding):\n",
    "    \"Embedding layer with truncated normal initialization\"\n",
    "    def __init__(self, ni, nf, std=0.01):\n",
    "        super().__init__(ni, nf)\n",
    "        trunc_normal_(self.weight.data, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(10, 5)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Embedding(10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncated normal initialization bounds the distribution to avoid large value. For a given standard deviation `std`, the bounds are roughly `-2*std`, `2*std`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = 0.02\n",
    "tst = Embedding(10, 30, std)\n",
    "assert tst.weight.min() > -2*std\n",
    "assert tst.weight.max() < 2*std\n",
    "test_close(tst.weight.mean(), 0, 1e-2)\n",
    "test_close(tst.weight.std(), std, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```SelfAttention(Module)```\n",
    "official: Self attention layer for `n_channels`.\n",
    "\n",
    "To build SelfAttention from scratch, key implementation details is discussed below\n",
    "- `sa = SelfAttention(n_channels)` to instantiate a SelfAttention layer\n",
    "- during instantiation, 3 conv1d layers are created with `n_in`, `n_out` calculated based on `n_channels`\n",
    "- the forward function is to implement the paper in the link below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class SelfAttention(Module):\n",
    "    \"Self attention layer for `n_channels`.\"\n",
    "    def __init__(self, n_channels):\n",
    "        self.query,self.key,self.value = [self._conv(n_channels, c) for c in (n_channels//8,n_channels//8,n_channels)]\n",
    "        self.gamma = nn.Parameter(tensor([0.]))\n",
    "\n",
    "    def _conv(self,n_in,n_out):\n",
    "        return ConvLayer(n_in, n_out, ks=1, ndim=1, norm_type=NormType.Spectral, act_cls=None, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Notation from the paper.\n",
    "        size = x.size()\n",
    "        x = x.view(*size[:2],-1)\n",
    "        f,g,h = self.query(x),self.key(x),self.value(x)\n",
    "        beta = F.softmax(torch.bmm(f.transpose(1,2), g), dim=1)\n",
    "        o = self.gamma * torch.bmm(h, beta) + x\n",
    "        return o.view(*size).contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention layer as introduced in [Self-Attention Generative Adversarial Networks](https://arxiv.org/abs/1805.08318).\n",
    "\n",
    "Initially, no change is done to the input. This is controlled by a trainable parameter named `gamma` as we return `x + gamma * out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelfAttention(\n",
       "  (query): ConvLayer(\n",
       "    (0): Conv1d(16, 2, kernel_size=(1,), stride=(1,), bias=False)\n",
       "  )\n",
       "  (key): ConvLayer(\n",
       "    (0): Conv1d(16, 2, kernel_size=(1,), stride=(1,), bias=False)\n",
       "  )\n",
       "  (value): ConvLayer(\n",
       "    (0): Conv1d(16, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst = SelfAttention(16)\n",
    "tst\n",
    "tst.gamma.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(32, 16, 8, 8)\n",
    "test_eq(tst(x),x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then during training `gamma` will probably change since it's a trainable parameter. Let's see what's happening when it gets a nonzero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst.gamma.data.fill_(1.)\n",
    "y = tst(x)\n",
    "test_eq(y.shape, [32,16,8,8])\n",
    "test_ne(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mechanism requires three matrix multiplications (here represented by 1x1 convs). The multiplications are done on the channel level (the second dimension in our tensor) and we flatten the feature map (which is 8x8 here). As in the paper, we note `f`, `g` and `h` the results of those multiplications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvLayer(\n",
       "  (0): Conv1d(16, 2, kernel_size=(1,), stride=(1,), bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Conv1d(16, 2, kernel_size=(1,), stride=(1,), bias=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst.query\n",
    "tst.query[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q,k,v = tst.query[0].weight.data,tst.key[0].weight.data,tst.value[0].weight.data\n",
    "test_eq([q.shape, k.shape, v.shape], [[2, 16, 1], [2, 16, 1], [16, 16, 1]])\n",
    "f,g,h = map(lambda m: x.view(32, 16, 64).transpose(1,2) @ m.squeeze().t(), [q,k,v])\n",
    "test_eq([f.shape, g.shape, h.shape], [[32,64,2], [32,64,2], [32,64,16]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key part of the attention layer is to compute attention weights for each of our location in the feature map (here 8x8 = 64). Those are positive numbers that sum to 1 and tell the model to pay attention to this or that part of the picture. We make the product of `f` and the transpose of `g` (to get something of size bs by 64 by 64) then apply a softmax on the first dimension (to get the positive numbers that sum up to 1). The result can then be multiplied with `h` transposed to get an output of size bs by channels by 64, which we can then be viewed as an output the same size as the original input. \n",
    "\n",
    "The final result is then `x + gamma * out` as we saw before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = F.softmax(torch.bmm(f, g.transpose(1,2)), dim=1)\n",
    "test_eq(beta.shape, [32, 64, 64])\n",
    "out = torch.bmm(h.transpose(1,2), beta)\n",
    "test_eq(out.shape, [32, 16, 64])\n",
    "test_close(y, x + out.view(32, 16, 8, 8), eps=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```PooledSelfAttention2d(Module)```\n",
    "official: Pooled self attention layer for 2d.\n",
    "\n",
    "Implemented from scratch and build with the template of `SelfAttention`, and the difference between `SelfAttention` is discussed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PooledSelfAttention2d(Module):\n",
    "    \"Pooled self attention layer for 2d.\"\n",
    "    def __init__(self, n_channels):\n",
    "        self.n_channels = n_channels\n",
    "        self.query,self.key,self.value = [self._conv(n_channels, c) for c in (n_channels//8,n_channels//8,n_channels//2)]\n",
    "        self.out   = self._conv(n_channels//2, n_channels)\n",
    "        self.gamma = nn.Parameter(tensor([0.]))\n",
    "\n",
    "    def _conv(self,n_in,n_out):\n",
    "        return ConvLayer(n_in, n_out, ks=1, norm_type=NormType.Spectral, act_cls=None, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_ftrs = x.shape[2]*x.shape[3]\n",
    "        f = self.query(x).view(-1, self.n_channels//8, n_ftrs)\n",
    "        g = F.max_pool2d(self.key(x),   [2,2]).view(-1, self.n_channels//8, n_ftrs//4)\n",
    "        h = F.max_pool2d(self.value(x), [2,2]).view(-1, self.n_channels//2, n_ftrs//4)\n",
    "        beta = F.softmax(torch.bmm(f.transpose(1, 2), g), -1)\n",
    "        o = self.out(torch.bmm(h, beta.transpose(1,2)).view(-1, self.n_channels//2, x.shape[2], x.shape[3]))\n",
    "        return self.gamma * o + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention layer used in the [Big GAN paper](https://arxiv.org/abs/1809.11096).\n",
    "\n",
    "It uses the same attention as in `SelfAttention` but adds a max pooling of stride 2 before computing the matrices `g` and `h`: the attention is ported on one of the 2x2 max-pooled window, not the whole feature map. There is also a final matrix product added at the end to the output, before retuning `gamma * out + x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PooledSelfAttention2d(\n",
       "  (query): ConvLayer(\n",
       "    (0): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (key): ConvLayer(\n",
       "    (0): Conv2d(8, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (value): ConvLayer(\n",
       "    (0): Conv2d(8, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (out): ConvLayer(\n",
       "    (0): Conv2d(4, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PooledSelfAttention2d(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```_conv1d_spect(ni:int, no:int, ks:int=1, stride:int=1, padding:int=0, bias:bool=False)```\n",
    "official : Create and initialize a `nn.Conv1d` layer with spectral normalization.\n",
    "\n",
    "- create a conv1d layer with `nn.Conv1d(ni, no, ks, stride=stride, padding=padding, bias=bias)`\n",
    "- initialize it with `nn.init.kaiming_normal_(conv.weight)`\n",
    "- if `bias=True`, make them zero\n",
    "- run spectral normalization on this conv layer and return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _conv1d_spect(ni:int, no:int, ks:int=1, stride:int=1, padding:int=0, bias:bool=False):\n",
    "    \"Create and initialize a `nn.Conv1d` layer with spectral normalization.\"\n",
    "    conv = nn.Conv1d(ni, no, ks, stride=stride, padding=padding, bias=bias)\n",
    "    nn.init.kaiming_normal_(conv.weight)\n",
    "    if bias: conv.bias.data.zero_()\n",
    "    return spectral_norm(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1d(3, 2, kernel_size=(1,), stride=(1,), bias=False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_conv1d_spect(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```SimpleSelfAttention(self, n_in:int, ks=1, sym=False)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class SimpleSelfAttention(Module):\n",
    "    def __init__(self, n_in:int, ks=1, sym=False):\n",
    "        self.sym,self.n_in = sym,n_in\n",
    "        self.conv = _conv1d_spect(n_in, n_in, ks, padding=ks//2, bias=False)\n",
    "        self.gamma = nn.Parameter(tensor([0.]))\n",
    "\n",
    "    def forward(self,x):\n",
    "        if self.sym:\n",
    "            c = self.conv.weight.view(self.n_in,self.n_in)\n",
    "            c = (c + c.t())/2\n",
    "            self.conv.weight = c.view(self.n_in,self.n_in,1)\n",
    "\n",
    "        size = x.size()\n",
    "        x = x.view(*size[:2],-1)\n",
    "\n",
    "        convx = self.conv(x)\n",
    "        xxT = torch.bmm(x,x.permute(0,2,1).contiguous())\n",
    "        o = torch.bmm(xxT, convx)\n",
    "        o = self.gamma * o + x\n",
    "        return o.view(*size).contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PixelShuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PixelShuffle introduced in [this article](https://arxiv.org/pdf/1609.05158.pdf) to avoid checkerboard artifacts when upsampling images. If we want an output with `ch_out` filters, we use a convolution with `ch_out * (r**2)` filters, where `r` is the upsampling factor. Then we reorganize those filters like in the picture below:\n",
    "\n",
    "<img src=\"images/pixelshuffle.png\" alt=\"Pixelshuffle\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```icnr_init(x, scale=2, init=nn.init.kaiming_normal_)```\n",
    "official: ICNR init of `x`, with `scale` and `init` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "def icnr_init(x, scale=2, init=nn.init.kaiming_normal_):\n",
    "    \"ICNR init of `x`, with `scale` and `init` function\"\n",
    "    ni,nf,h,w = x.shape\n",
    "    ni2 = int(ni/(scale**2))\n",
    "#     pp(x.new_zeros([ni2,nf,h,w]).shape, init(x.new_zeros([ni2,nf,h,w])).shape)\n",
    "    k = init(x.new_zeros([ni2,nf,h,w])).transpose(0, 1)\n",
    "    k = k.contiguous().view(ni2, nf, -1)\n",
    "    k = k.repeat(1, 1, scale**2)\n",
    "    return k.contiguous().view([nf,ni,h,w]).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICNR init was introduced in [this article](https://arxiv.org/abs/1707.02937). It suggests to initialize the convolution that will be used in PixelShuffle so that each of the `r**2` channels get the same weight (so that in the picture above, the 9 colors in a 3 by 3 window are initially the same).\n",
    "\n",
    "> Note: This is done on the first dimension because PyTorch stores the weights of a convolutional layer in this format: `ch_out x ch_in x ks x ks`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:27:44.89 >>> Call to icnr_init in File \"/var/folders/gz/ch3n2mp51m9386sytqf97s6w0000gn/T/ipykernel_57140/3594885830.py\", line 3\n",
      "08:27:44.89 ...... x = tensor([[[[-0.0068]],\n",
      "08:27:44.89            \n",
      "08:27:44.89                     [[-0.8325]],\n",
      "08:27:44.89            \n",
      "08:27:44.89              ...,\n",
      "08:27:44.89            \n",
      "08:27:44.89                     [[-0.0070]],\n",
      "08:27:44.89            \n",
      "08:27:44.89                     [[ 0.4225]]]])\n",
      "08:27:44.89 ...... type(x) = <class 'torch.Tensor'>\n",
      "08:27:44.89 ...... x.shape = (64, 32, 1, 1)\n",
      "08:27:44.89 ...... scale = 2\n",
      "08:27:44.89 ...... type(scale) = <class 'int'>\n",
      "08:27:44.89 ...... init = <function kaiming_normal_>\n",
      "08:27:44.89 ...... type(init) = <class 'function'>\n",
      "08:27:44.89 ...... sig(init) = <Signature (tensor: torch.Tensor, a: float = 0, ...tr = 'fan_in', nonlinearity: str = 'leaky_relu')>\n",
      "08:27:44.89    3 | def icnr_init(x, scale=2, init=nn.init.kaiming_normal_):\n",
      "08:27:44.89    5 |     ni,nf,h,w = x.shape\n",
      "08:27:44.89 .......... ni = 64\n",
      "08:27:44.89 .......... type(ni) = <class 'int'>\n",
      "08:27:44.89 .......... nf = 32\n",
      "08:27:44.89 .......... type(nf) = <class 'int'>\n",
      "08:27:44.89 .......... h = 1\n",
      "08:27:44.89 .......... type(h) = <class 'int'>\n",
      "08:27:44.89 .......... w = 1\n",
      "08:27:44.89 .......... type(w) = <class 'int'>\n",
      "08:27:44.89    6 |     ni2 = int(ni/(scale**2))\n",
      "08:27:44.90 .......... ni2 = 16\n",
      "08:27:44.90 .......... type(ni2) = <class 'int'>\n",
      "08:27:44.90    7 |     pp(x.new_zeros([ni2,nf,h,w]).shape, init(x.new_zeros([ni2,nf,h,w])).shape)\n",
      "08:27:44.90 LOG:\n",
      "08:27:44.91 .... x.new_zeros([ni2,nf,h,w]).shape = torch.Size([16, 32, 1, 1])\n",
      "08:27:44.91 .... init(x.new_zeros([ni2,nf,h,w])).shape = torch.Size([16, 32, 1, 1])\n",
      "08:27:44.92    8 |     k = init(x.new_zeros([ni2,nf,h,w])).transpose(0, 1)\n",
      "08:27:44.92 .......... k = tensor([[[[ 0.1750]],\n",
      "08:27:44.92                \n",
      "08:27:44.92                         [[ 0.4088]],\n",
      "08:27:44.92                \n",
      "08:27:44.92                  ...,\n",
      "08:27:44.92                \n",
      "08:27:44.92                         [[ 0.1177]],\n",
      "08:27:44.92                \n",
      "08:27:44.92                         [[ 0.3008]]]])\n",
      "08:27:44.92 .......... type(k) = <class 'torch.Tensor'>\n",
      "08:27:44.92 .......... k.shape = (32, 16, 1, 1)\n",
      "08:27:44.92    9 |     k = k.contiguous().view(ni2, nf, -1)\n",
      "08:27:44.93 .......... k = tensor([[[ 0.1750],\n",
      "08:27:44.93                         [ 0.4088],\n",
      "08:27:44.93                        ....1537],\n",
      "08:27:44.93                         [ 0.1177],\n",
      "08:27:44.93                         [ 0.3008]]])\n",
      "08:27:44.93 .......... k.shape = (16, 32, 1)\n",
      "08:27:44.93   10 |     k = k.repeat(1, 1, scale**2)\n",
      "08:27:44.93 .......... k = tensor([[[ 0.1750,  0.1750,  0.1750,  0.1750],\n",
      "08:27:44.93                 ...\n",
      "08:27:44.93                         [ 0.3008,  0.3008,  0.3008,  0.3008]]])\n",
      "08:27:44.93 .......... k.shape = (16, 32, 4)\n",
      "08:27:44.93   11 |     return k.contiguous().view([nf,ni,h,w]).transpose(0, 1)\n",
      "08:27:44.93 <<< Return value from icnr_init: tensor([[[[ 0.1750]],\n",
      "08:27:44.93                                  \n",
      "08:27:44.93                                           [[ 0.0070]],\n",
      "08:27:44.93                                  \n",
      "08:27:44.93                                    ...,\n",
      "08:27:44.93                                  \n",
      "08:27:44.93                                           [[ 0.4090]],\n",
      "08:27:44.93                                  \n",
      "08:27:44.93                                           [[ 0.3008]]]])\n"
     ]
    }
   ],
   "source": [
    "tst = torch.randn(16*4, 32, 1, 1)\n",
    "tst = icnr_init(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,16*4,4):\n",
    "    test_eq(tst[i],tst[i+1])\n",
    "    test_eq(tst[i],tst[i+2])\n",
    "    test_eq(tst[i],tst[i+3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PixelShuffle_ICNR(nn.Sequential):\n",
    "    \"Upsample by `scale` from `ni` filters to `nf` (default `ni`), using `nn.PixelShuffle`.\"\n",
    "    def __init__(self, ni, nf=None, scale=2, blur=False, norm_type=NormType.Weight, act_cls=defaults.activation):\n",
    "        super().__init__()\n",
    "        nf = ifnone(nf, ni)\n",
    "        layers = [ConvLayer(ni, nf*(scale**2), ks=1, norm_type=norm_type, act_cls=act_cls, bias_std=0),\n",
    "                  nn.PixelShuffle(scale)]\n",
    "        if norm_type == NormType.Weight:\n",
    "            layers[0][0].weight_v.data.copy_(icnr_init(layers[0][0].weight_v.data))\n",
    "            layers[0][0].weight_g.data.copy_(((layers[0][0].weight_v.data**2).sum(dim=[1,2,3])**0.5)[:,None,None,None])\n",
    "        else:\n",
    "            layers[0][0].weight.data.copy_(icnr_init(layers[0][0].weight.data))\n",
    "        if blur: layers += [nn.ReplicationPad2d((1,0,1,0)), nn.AvgPool2d(2, stride=1)]\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional layer is initialized with `icnr_init` and passed `act_cls` and `norm_type` (the default of weight normalization seemed to be what's best for super-resolution problems, in our experiments). \n",
    "\n",
    "The `blur` option comes from [Super-Resolution using Convolutional Neural Networks without Any Checkerboard Artifacts](https://arxiv.org/abs/1806.02658) where the authors add a little bit of blur to completely get rid of checkerboard artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psfl = PixelShuffle_ICNR(16)\n",
    "x = torch.randn(64, 16, 8, 8)\n",
    "y = psfl(x)\n",
    "test_eq(y.shape, [64, 16, 16, 16])\n",
    "#ICNR init makes every 2x2 window (stride 2) have the same elements\n",
    "for i in range(0,16,2):\n",
    "    for j in range(0,16,2):\n",
    "        test_eq(y[:,:,i,j],y[:,:,i+1,j])\n",
    "        test_eq(y[:,:,i,j],y[:,:,i  ,j+1])\n",
    "        test_eq(y[:,:,i,j],y[:,:,i+1,j+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psfl = PixelShuffle_ICNR(16, norm_type=None)\n",
    "x = torch.randn(64, 16, 8, 8)\n",
    "y = psfl(x)\n",
    "test_eq(y.shape, [64, 16, 16, 16])\n",
    "#ICNR init makes every 2x2 window (stride 2) have the same elements\n",
    "for i in range(0,16,2):\n",
    "    for j in range(0,16,2):\n",
    "        test_eq(y[:,:,i,j],y[:,:,i+1,j])\n",
    "        test_eq(y[:,:,i,j],y[:,:,i  ,j+1])\n",
    "        test_eq(y[:,:,i,j],y[:,:,i+1,j+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psfl = PixelShuffle_ICNR(16, norm_type=NormType.Spectral)\n",
    "x = torch.randn(64, 16, 8, 8)\n",
    "y = psfl(x)\n",
    "test_eq(y.shape, [64, 16, 16, 16])\n",
    "#ICNR init makes every 2x2 window (stride 2) have the same elements\n",
    "for i in range(0,16,2):\n",
    "    for j in range(0,16,2):\n",
    "        test_eq(y[:,:,i,j],y[:,:,i+1,j])\n",
    "        test_eq(y[:,:,i,j],y[:,:,i  ,j+1])\n",
    "        test_eq(y[:,:,i,j],y[:,:,i+1,j+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def sequential(*args):\n",
    "    \"Create an `nn.Sequential`, wrapping items with `Lambda` if needed\"\n",
    "    if len(args) != 1 or not isinstance(args[0], OrderedDict):\n",
    "        args = list(args)\n",
    "        for i,o in enumerate(args):\n",
    "            if not isinstance(o,nn.Module): args[i] = Lambda(o)\n",
    "    return nn.Sequential(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class SequentialEx(Module):\n",
    "    \"Like `nn.Sequential`, but with ModuleList semantics, and can access module input\"\n",
    "    def __init__(self, *layers): self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        for l in self.layers:\n",
    "            res.orig = x\n",
    "            nres = l(res)\n",
    "            # We have to remove res.orig to avoid hanging refs and therefore memory leaks\n",
    "            res.orig, nres.orig = None, None\n",
    "            res = nres\n",
    "        return res\n",
    "\n",
    "    def __getitem__(self,i): return self.layers[i]\n",
    "    def append(self,l):      return self.layers.append(l)\n",
    "    def extend(self,l):      return self.layers.extend(l)\n",
    "    def insert(self,i,l):    return self.layers.insert(i,l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful to write layers that require to remember the input (like a resnet block) in a sequential way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MergeLayer(Module):\n",
    "    \"Merge a shortcut with the result of the module by adding them or concatenating them if `dense=True`.\"\n",
    "    def __init__(self, dense:bool=False): self.dense=dense\n",
    "    def forward(self, x): return torch.cat([x,x.orig], dim=1) if self.dense else (x+x.orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_block = SequentialEx(ConvLayer(16, 16), ConvLayer(16,16))\n",
    "res_block.append(MergeLayer()) # just to test append - normally it would be in init params\n",
    "x = torch.randn(32, 16, 8, 8)\n",
    "y = res_block(x)\n",
    "test_eq(y.shape, [32, 16, 8, 8])\n",
    "test_eq(y, x + res_block[1](res_block[0](x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = TensorBase(torch.randn(32, 16, 8, 8))\n",
    "y = res_block(x)\n",
    "test_is(y.orig, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent to keras.layers.Concatenate, it will concat the outputs of a ModuleList over a given dimension (default the filter dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "class Cat(nn.ModuleList):\n",
    "    \"Concatenate layers outputs over a given dim\"\n",
    "    def __init__(self, layers, dim=1):\n",
    "        self.dim=dim\n",
    "        super().__init__(layers)\n",
    "    def forward(self, x): return torch.cat([l(x) for l in self], dim=self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [ConvLayer(2,4), ConvLayer(2,4), ConvLayer(2,4)] \n",
    "x = torch.rand(1,2,8,8) \n",
    "cat = Cat(layers) \n",
    "test_eq(cat(x).shape, [1,12,8,8]) \n",
    "test_eq(cat(x), torch.cat([l(x) for l in layers], dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready-to-go models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class SimpleCNN(nn.Sequential):\n",
    "    \"Create a simple CNN with `filters`.\"\n",
    "    def __init__(self, filters, kernel_szs=None, strides=None, bn=True):\n",
    "        nl = len(filters)-1\n",
    "        kernel_szs = ifnone(kernel_szs, [3]*nl)\n",
    "        strides    = ifnone(strides   , [2]*nl)\n",
    "        layers = [ConvLayer(filters[i], filters[i+1], kernel_szs[i], stride=strides[i],\n",
    "                  norm_type=(NormType.Batch if bn and i<nl-1 else None)) for i in range(nl)]\n",
    "        layers.append(PoolFlatten())\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a succession of convolutional layers from `(filters[0],filters[1])` to `(filters[n-2],filters[n-1])` (if `n` is the length of the `filters` list) followed by a `PoolFlatten`. `kernel_szs` and `strides` defaults to a list of 3s and a list of 2s. If `bn=True` the convolutional layers are successions of conv-relu-batchnorm, otherwise conv-relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = SimpleCNN([8,16,32])\n",
    "mods = list(tst.children())\n",
    "test_eq(len(mods), 3)\n",
    "test_eq([[m[0].in_channels, m[0].out_channels] for m in mods[:2]], [[8,16], [16,32]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test kernel sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = SimpleCNN([8,16,32], kernel_szs=[1,3])\n",
    "mods = list(tst.children())\n",
    "test_eq([m[0].kernel_size for m in mods[:2]], [(1,1), (3,3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = SimpleCNN([8,16,32], strides=[1,2])\n",
    "mods = list(tst.children())\n",
    "test_eq([m[0].stride for m in mods[:2]], [(1,1),(2,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ProdLayer(Module):\n",
    "    \"Merge a shortcut with the result of the module by multiplying them.\"\n",
    "    def forward(self, x): return x * x.orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "inplace_relu = partial(nn.ReLU, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def SEModule(ch, reduction, act_cls=defaults.activation):\n",
    "    nf = math.ceil(ch//reduction/8)*8\n",
    "    return SequentialEx(nn.AdaptiveAvgPool2d(1),\n",
    "                        ConvLayer(ch, nf, ks=1, norm_type=None, act_cls=act_cls),\n",
    "                        ConvLayer(nf, ch, ks=1, norm_type=None, act_cls=nn.Sigmoid),\n",
    "                        ProdLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ResBlock(Module):\n",
    "    \"Resnet block from `ni` to `nh` with `stride`\"\n",
    "    @delegates(ConvLayer.__init__)\n",
    "    def __init__(self, expansion, ni, nf, stride=1, groups=1, reduction=None, nh1=None, nh2=None, dw=False, g2=1,\n",
    "                 sa=False, sym=False, norm_type=NormType.Batch, act_cls=defaults.activation, ndim=2, ks=3,\n",
    "                 pool=AvgPool, pool_first=True, **kwargs):\n",
    "        norm2 = (NormType.BatchZero if norm_type==NormType.Batch else\n",
    "                 NormType.InstanceZero if norm_type==NormType.Instance else norm_type)\n",
    "        if nh2 is None: nh2 = nf\n",
    "        if nh1 is None: nh1 = nh2\n",
    "        nf,ni = nf*expansion,ni*expansion\n",
    "        k0 = dict(norm_type=norm_type, act_cls=act_cls, ndim=ndim, **kwargs)\n",
    "        k1 = dict(norm_type=norm2, act_cls=None, ndim=ndim, **kwargs)\n",
    "        convpath  = [ConvLayer(ni,  nh2, ks, stride=stride, groups=ni if dw else groups, **k0),\n",
    "                     ConvLayer(nh2,  nf, ks, groups=g2, **k1)\n",
    "        ] if expansion == 1 else [\n",
    "                     ConvLayer(ni,  nh1, 1, **k0),\n",
    "                     ConvLayer(nh1, nh2, ks, stride=stride, groups=nh1 if dw else groups, **k0),\n",
    "                     ConvLayer(nh2,  nf, 1, groups=g2, **k1)]\n",
    "        if reduction: convpath.append(SEModule(nf, reduction=reduction, act_cls=act_cls))\n",
    "        if sa: convpath.append(SimpleSelfAttention(nf,ks=1,sym=sym))\n",
    "        self.convpath = nn.Sequential(*convpath)\n",
    "        idpath = []\n",
    "        if ni!=nf: idpath.append(ConvLayer(ni, nf, 1, act_cls=None, ndim=ndim, **kwargs))\n",
    "        if stride!=1: idpath.insert((1,0)[pool_first], pool(stride, ndim=ndim, ceil_mode=True))\n",
    "        self.idpath = nn.Sequential(*idpath)\n",
    "        self.act = defaults.activation(inplace=True) if act_cls is defaults.activation else act_cls()\n",
    "\n",
    "    def forward(self, x): return self.act(self.convpath(x) + self.idpath(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a resnet block (normal or bottleneck depending on `expansion`, 1 for the normal block and 4 for the traditional bottleneck) that implements the tweaks from [Bag of Tricks for Image Classification with Convolutional Neural Networks](https://arxiv.org/abs/1812.01187). In particular, the last batchnorm layer (if that is the selected `norm_type`) is initialized with a weight (or gamma) of zero to facilitate the flow from the beginning to the end of the network. It also implements optional [Squeeze and Excitation](https://arxiv.org/abs/1709.01507) and grouped convs for [ResNeXT](https://arxiv.org/abs/1611.05431) and similar models (use `dw=True` for depthwise convs).\n",
    "\n",
    "The `kwargs` are passed to `ConvLayer` along with `norm_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def SEBlock(expansion, ni, nf, groups=1, reduction=16, stride=1, **kwargs):\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, reduction=reduction, nh1=nf*2, nh2=nf*expansion, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def SEResNeXtBlock(expansion, ni, nf, groups=32, reduction=16, stride=1, base_width=4, **kwargs):\n",
    "    w = math.floor(nf * (base_width / 64)) * groups\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, groups=groups, reduction=reduction, nh2=w, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def SeparableBlock(expansion, ni, nf, reduction=16, stride=1, base_width=4, **kwargs):\n",
    "    return ResBlock(expansion, ni, nf, stride=stride, reduction=reduction, nh2=nf*2, dw=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Distributed Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent to Keras `TimeDistributed` Layer, enables computing pytorch `Module` over an axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _stack_tups(tuples, stack_dim=1):\n",
    "    \"Stack tuple of tensors along `stack_dim`\"\n",
    "    return tuple(torch.stack([t[i] for t in tuples], dim=stack_dim) for i in range_of(tuples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TimeDistributed(Module):\n",
    "    \"Applies `module` over `tdim` identically for each step, use `low_mem` to compute one at a time.\" \n",
    "    def __init__(self, module, low_mem=False, tdim=1):\n",
    "        store_attr()\n",
    "        \n",
    "    def forward(self, *tensors, **kwargs):\n",
    "        \"input x with shape:(bs,seq_len,channels,width,height)\"\n",
    "        if self.low_mem or self.tdim!=1: \n",
    "            return self.low_mem_forward(*tensors, **kwargs)\n",
    "        else:\n",
    "            #only support tdim=1\n",
    "            inp_shape = tensors[0].shape\n",
    "            bs, seq_len = inp_shape[0], inp_shape[1]   \n",
    "            out = self.module(*[x.view(bs*seq_len, *x.shape[2:]) for x in tensors], **kwargs)\n",
    "        return self.format_output(out, bs, seq_len)\n",
    "    \n",
    "    def low_mem_forward(self, *tensors, **kwargs):                                           \n",
    "        \"input x with shape:(bs,seq_len,channels,width,height)\"\n",
    "        seq_len = tensors[0].shape[self.tdim]\n",
    "        args_split = [torch.unbind(x, dim=self.tdim) for x in tensors]\n",
    "        out = []\n",
    "        for i in range(seq_len):\n",
    "            out.append(self.module(*[args[i] for args in args_split]), **kwargs)\n",
    "        if isinstance(out[0], tuple):\n",
    "            return _stack_tups(out, stack_dim=self.tdim)\n",
    "        return torch.stack(out, dim=self.tdim)\n",
    "    \n",
    "    def format_output(self, out, bs, seq_len):\n",
    "        \"unstack from batchsize outputs\"\n",
    "        if isinstance(out, tuple):\n",
    "            return tuple(out_i.view(bs, seq_len, *out_i.shape[1:]) for out_i in out)\n",
    "        return out.view(bs, seq_len,*out.shape[1:])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'TimeDistributed({self.module})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, seq_len = 2, 5\n",
    "x, y = torch.rand(bs,seq_len,3,2,2), torch.rand(bs,seq_len,3,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tconv = TimeDistributed(nn.Conv2d(3,4,1))\n",
    "test_eq(tconv(x).shape, (2,5,4,2,2))\n",
    "tconv.low_mem=True\n",
    "test_eq(tconv(x).shape, (2,5,4,2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mod(Module):\n",
    "    def __init__(self):\n",
    "        self.conv = nn.Conv2d(3,4,1)\n",
    "    def forward(self, x, y):\n",
    "        return self.conv(x) + self.conv(y)\n",
    "tmod = TimeDistributed(Mod())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tmod(x,y)\n",
    "test_eq(out.shape, (2,5,4,2,2))\n",
    "tmod.low_mem=True\n",
    "out_low_mem = tmod(x,y)\n",
    "test_eq(out_low_mem.shape, (2,5,4,2,2))\n",
    "test_eq(out, out_low_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mod2(Module):\n",
    "    def __init__(self):\n",
    "        self.conv = nn.Conv2d(3,4,1)\n",
    "    def forward(self, x, y):\n",
    "        return self.conv(x), self.conv(y)\n",
    "tmod2 = TimeDistributed(Mod2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tmod2(x,y)\n",
    "test_eq(len(out), 2)\n",
    "test_eq(out[0].shape, (2,5,4,2,2))\n",
    "tmod2.low_mem=True\n",
    "out_low_mem = tmod2(x,y)\n",
    "test_eq(out_low_mem[0].shape, (2,5,4,2,2))\n",
    "test_eq(out, out_low_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### TimeDistributed\n",
       "\n",
       ">      TimeDistributed (module, low_mem=False, tdim=1)\n",
       "\n",
       "Applies `module` over `tdim` identically for each step, use `low_mem` to compute one at a time."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### TimeDistributed\n",
       "\n",
       ">      TimeDistributed (module, low_mem=False, tdim=1)\n",
       "\n",
       "Applies `module` over `tdim` identically for each step, use `low_mem` to compute one at a time."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TimeDistributed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module is equivalent to [Keras TimeDistributed Layer](https://keras.io/api/layers/recurrent_layers/time_distributed/). This wrapper allows to apply a layer to every temporal slice of an input. By default it is assumed the time axis (`tdim`) is the 1st one (the one after the batch size). A typical usage would be to encode a sequence of images using an image encoder.\n",
    "\n",
    "The `forward` function of `TimeDistributed` supports `*args` and `**kkwargs` but only `args` will be split and passed to the underlying module independently for each timestep, `kwargs` will be passed as they are. This is useful when you have module that take multiple arguments as inputs, this way, you can put all tensors you need spliting as `args` and other arguments that don't need split as `kwargs`.\n",
    "\n",
    "> This module is heavy on memory, as it will try to pass mutiple timesteps at the same time on the batch dimension, if you get out of memorey errors, try first reducing your batch size by the number of timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = create_body(resnet18())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A resnet18 will encode a feature map of 512 channels. Height and Width will be divided by 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_resnet = TimeDistributed(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a synthetic batch of 2 image-sequences of lenght 5. `(bs, seq_len, ch, w, h)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sequence = torch.rand(2, 5, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 512, 2, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_resnet(image_sequence).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, one can encode a sequence of images on feature space.\n",
    "There is also a `low_mem_forward` that will pass images one at a time to reduce GPU memory consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 512, 2, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_resnet.low_mem_forward(image_sequence).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swish and Mish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from torch.jit import script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@script\n",
    "def _swish_jit_fwd(x): return x.mul(torch.sigmoid(x))\n",
    "\n",
    "@script\n",
    "def _swish_jit_bwd(x, grad_output):\n",
    "    x_sigmoid = torch.sigmoid(x)\n",
    "    return grad_output * (x_sigmoid * (1 + x * (1 - x_sigmoid)))\n",
    "\n",
    "class _SwishJitAutoFn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return _swish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_variables[0]\n",
    "        return _swish_jit_bwd(x, grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def swish(x, inplace=False): return _SwishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Swish(Module):\n",
    "    def forward(self, x): return _SwishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@script\n",
    "def _mish_jit_fwd(x): return x.mul(torch.tanh(F.softplus(x)))\n",
    "\n",
    "@script\n",
    "def _mish_jit_bwd(x, grad_output):\n",
    "    x_sigmoid = torch.sigmoid(x)\n",
    "    x_tanh_sp = F.softplus(x).tanh()\n",
    "    return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\n",
    "\n",
    "class MishJitAutoFn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return _mish_jit_fwd(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_variables[0]\n",
    "        return _mish_jit_bwd(x, grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def mish(x): return F.mish(x) if torch.__version__ >= '1.9' else MishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Mish(Module):\n",
    "    def forward(self, x): return MishJitAutoFn.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "if ismin_torch('1.9'): Mish = nn.Mish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "for o in swish,Swish,mish,Mish: o.__default_init__ = kaiming_uniform_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for submodules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to get the list of all parameters of a given model. For when you want all submodules (like linear/conv layers) without forgetting lone parameters, the following class wraps those in fake modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ParameterModule(Module):\n",
    "    \"Register a lone parameter `p` in a module.\"\n",
    "    def __init__(self, p): self.val = p\n",
    "    def forward(self, x): return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def children_and_parameters(m):\n",
    "    \"Return the children of `m` and its direct parameters not registered in modules.\"\n",
    "    children = list(m.children())\n",
    "    children_p = sum([[id(p) for p in c.parameters()] for c in m.children()],[])\n",
    "    for p in m.parameters():\n",
    "        if id(p) not in children_p: children.append(ParameterModule(p))\n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TstModule(Module):\n",
    "    def __init__(self): self.a,self.lin = nn.Parameter(torch.randn(1)),nn.Linear(5,10)\n",
    "\n",
    "tst = TstModule()\n",
    "children = children_and_parameters(tst)\n",
    "test_eq(len(children), 2)\n",
    "test_eq(children[0], tst.lin)\n",
    "assert isinstance(children[1], ParameterModule)\n",
    "test_eq(children[1].val, tst.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def has_children(m):\n",
    "    try: next(m.children())\n",
    "    except StopIteration: return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(Module): pass\n",
    "assert not has_children(A())\n",
    "assert has_children(TstModule())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def flatten_model(m):\n",
    "    \"Return the list of all submodules and parameters of `m`\"\n",
    "    return sum(map(flatten_model,children_and_parameters(m)),[]) if has_children(m) else [m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = nn.Sequential(TstModule(), TstModule())\n",
    "children = flatten_model(tst)\n",
    "test_eq(len(children), 4)\n",
    "assert isinstance(children[1], ParameterModule)\n",
    "assert isinstance(children[3], ParameterModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class NoneReduce():\n",
    "    \"A context manager to evaluate `loss_func` with none reduce.\"\n",
    "    def __init__(self, loss_func): self.loss_func,self.old_red = loss_func,None\n",
    "\n",
    "    def __enter__(self):\n",
    "        if hasattr(self.loss_func, 'reduction'):\n",
    "            self.old_red = self.loss_func.reduction\n",
    "            self.loss_func.reduction = 'none'\n",
    "            return self.loss_func\n",
    "        else: return partial(self.loss_func, reduction='none')\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        if self.old_red is not None: self.loss_func.reduction = self.old_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = torch.randn(5),torch.randn(5)\n",
    "loss_fn = nn.MSELoss()\n",
    "with NoneReduce(loss_fn) as loss_func:\n",
    "    loss = loss_func(x,y)\n",
    "test_eq(loss.shape, [5])\n",
    "test_eq(loss_fn.reduction, 'mean')\n",
    "\n",
    "loss_fn = F.mse_loss\n",
    "with NoneReduce(loss_fn) as loss_func:\n",
    "    loss = loss_func(x,y)\n",
    "test_eq(loss.shape, [5])\n",
    "test_eq(loss_fn, F.mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def in_channels(m):\n",
    "    \"Return the shape of the first weight layer in `m`.\"\n",
    "    try: return next(l.weight.shape[1] for l in flatten_model(m) if nested_attr(l,'weight.ndim',-1)==4)\n",
    "    except StopIteration as e: e.args = [\"No weight layer\"]; raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(in_channels(nn.Sequential(nn.Conv2d(5,4,3), nn.Conv2d(4,3,3))), 5)\n",
    "test_eq(in_channels(nn.Sequential(nn.AvgPool2d(4), nn.Conv2d(4,3,3))), 4)\n",
    "test_eq(in_channels(nn.Sequential(BatchNorm(4), nn.Conv2d(4,3,3))), 4)\n",
    "test_eq(in_channels(nn.Sequential(InstanceNorm(4), nn.Conv2d(4,3,3))), 4)\n",
    "test_eq(in_channels(nn.Sequential(InstanceNorm(4, affine=False), nn.Conv2d(4,3,3))), 4)\n",
    "test_fail(lambda : in_channels(nn.Sequential(nn.AvgPool2d(4))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Natsume/mambaforge/lib/python3.9/site-packages/nbdev/export.py:54: UserWarning: Notebook '/Users/Natsume/Documents/fastai/nbs/09c_vision.widgets.ipynb' uses `#|export` without `#|default_exp` cell.\n",
      "Note nbdev2 no longer supports nbdev1 syntax. Run `nbdev_migrate` to upgrade.\n",
      "See https://nbdev.fast.ai/getting_started.html for more information.\n",
      "  warn(f\"Notebook '{nbname}' uses `#|export` without `#|default_exp` cell.\\n\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'/Users/Natsume/Documents/fastai/fastai/nbs/09c_vision.widgets.ipynb' is not in the subpath of '/Users/Natsume/Documents/fastai/nbs' OR one path is relative and the other is absolute.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [140]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#|hide\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnbdev\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnbdev_export\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/fastcore/script.py:110\u001b[0m, in \u001b[0;36mcall_parse.<locals>._f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    109\u001b[0m     mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(inspect\u001b[38;5;241m.\u001b[39mcurrentframe()\u001b[38;5;241m.\u001b[39mf_back)\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mod: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SCRIPT_INFO\u001b[38;5;241m.\u001b[39mfunc \u001b[38;5;129;01mand\u001b[39;00m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m: SCRIPT_INFO\u001b[38;5;241m.\u001b[39mfunc \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sys\u001b[38;5;241m.\u001b[39margv)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m: sys\u001b[38;5;241m.\u001b[39margv\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/nbdev/doclinks.py:135\u001b[0m, in \u001b[0;36mnbdev_export\u001b[0;34m(path, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files: nb_export(f)\n\u001b[1;32m    134\u001b[0m add_init(get_config()\u001b[38;5;241m.\u001b[39mlib_path)\n\u001b[0;32m--> 135\u001b[0m \u001b[43m_build_modidx\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/nbdev/doclinks.py:97\u001b[0m, in \u001b[0;36m_build_modidx\u001b[0;34m(dest, nbs_path, skip_exists)\u001b[0m\n\u001b[1;32m     95\u001b[0m code_root \u001b[38;5;241m=\u001b[39m dest\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mresolve()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m globtastic(dest, file_glob\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.py\u001b[39m\u001b[38;5;124m\"\u001b[39m, skip_file_re\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^_\u001b[39m\u001b[38;5;124m'\u001b[39m, skip_folder_re\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.ipynb_checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 97\u001b[0m     res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msyms\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(\u001b[43m_get_modidx\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbs_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnbs_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     98\u001b[0m idxfile\u001b[38;5;241m.\u001b[39mwrite_text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# Autogenerated by nbdev\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124md = \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mpformat(res, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m140\u001b[39m, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, compact\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/nbdev/doclinks.py:71\u001b[0m, in \u001b[0;36m_get_modidx\u001b[0;34m(py_path, code_root, nbs_path)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m _iter_py_cells(py_path):\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cell\u001b[38;5;241m.\u001b[39mnb \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m     loc \u001b[38;5;241m=\u001b[39m _nbpath2html(\u001b[43mcell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnb_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelative_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbs_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stor\u001b[39m(nm):\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m L(nm): d[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmod_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloc\u001b[38;5;241m.\u001b[39mas_posix()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,rel_name\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/pathlib.py:939\u001b[0m, in \u001b[0;36mPurePath.relative_to\u001b[0;34m(self, *other)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (root \u001b[38;5;129;01mor\u001b[39;00m drv) \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m cf(abs_parts[:n]) \u001b[38;5;241m!=\u001b[39m cf(to_abs_parts):\n\u001b[1;32m    938\u001b[0m     formatted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_parsed_parts(to_drv, to_root, to_parts)\n\u001b[0;32m--> 939\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m is not in the subpath of \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    940\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m OR one path is relative and the other is absolute.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    941\u001b[0m                      \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m), \u001b[38;5;28mstr\u001b[39m(formatted)))\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_parsed_parts(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, root \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    943\u001b[0m                                abs_parts[n:])\n",
      "\u001b[0;31mValueError\u001b[0m: '/Users/Natsume/Documents/fastai/fastai/nbs/09c_vision.widgets.ipynb' is not in the subpath of '/Users/Natsume/Documents/fastai/nbs' OR one path is relative and the other is absolute."
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "from nbdev import *\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
