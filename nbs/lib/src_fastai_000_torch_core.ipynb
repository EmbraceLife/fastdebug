{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#| eval: false\n",
    "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp to_delete_torch_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from __future__ import annotations\n",
    "from fastai.imports import *\n",
    "from fastai.torch_imports import *\n",
    "from packaging.version import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastdebug.utils import *\n",
    "from fastdebug.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastai.imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai.imports as fi\n",
    "import fastai.torch_imports as fti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi\n",
    "fti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cat /Users/Natsume/mambaforge/lib/python3.9/site-packages/fastai/imports.py\n",
    "# %cat /Users/Natsume/mambaforge/lib/python3.9/site-packages/fastai/torch_imports.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \\_all_, defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "_all_ = ['progress_bar','master_bar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai.torch_core as ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft._all_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "defaults.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup_cuda\n",
    "not used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def setup_cuda(benchmark=defaults.benchmark):\n",
    "    \"Sets the main cuda device and sets `cudnn.benchmark` to `benchmark`\"\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.current_device()==0:\n",
    "            def_gpu = int(os.environ.get('DEFAULT_GPU') or 0)\n",
    "            if torch.cuda.device_count()>=def_gpu: torch.cuda.set_device(def_gpu)\n",
    "        torch.backends.cudnn.benchmark = benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Core\n",
    "\n",
    "> Basic pytorch functions used in the fastai library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrays and show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `subplots(nrows, ncols,...)`\n",
    "A wrapper around plt.subplots(...); only used in show_images; not in show_image, nor in show_image_batch;to create/display and return a fig with specified size and a specified num of empty subplots;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastview(\"subplots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "@delegates(plt.subplots, keep=True)\n",
    "def subplots(\n",
    "    nrows:int=1, # Number of rows in returned axes grid\n",
    "    ncols:int=1, # Number of columns in returned axes grid\n",
    "    figsize:tuple=None, # Width, height in inches of the returned figure \n",
    "    imsize:int=3, # Size (in inches) of images that will be displayed in the returned figure\n",
    "    suptitle:str=None, # Title to be set to returned figure\n",
    "    **kwargs\n",
    ") -> (plt.Figure, plt.Axes): # Returns both fig and ax as a tuple \n",
    "    \"Returns a figure and set of subplots to display images of `imsize` inches\"\n",
    "    if figsize is None: \n",
    "#         pp.deep(lambda: nrows*imsize if suptitle is None or imsize>2 else nrows*imsize+0.6)\n",
    "        h=nrows*imsize if suptitle is None or imsize>2 else nrows*imsize+0.6 #https://github.com/matplotlib/matplotlib/issues/5355\n",
    "        figsize=(ncols*imsize, h)\n",
    "    fig,ax = plt.subplots(nrows, ncols, figsize=figsize, **kwargs)\n",
    "    if suptitle is not None: fig.suptitle(suptitle)\n",
    "    if nrows*ncols==1: ax = array([ax])\n",
    "    return fig,ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used in `get_grid`. `suptitle`, `sharex`, `sharey`, `squeeze`, `subplot_kw` and `gridspec_kw` are all passed down to [plt.subplots](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html#matplotlib-pyplot-subplots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check(subplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdb = Fastdb(subplots)\n",
    "# fdb.eg = \"\"\"\n",
    "# _,axs = subplots()\n",
    "# test_eq(axs.shape,[1])\n",
    "# plt.close()\n",
    "# \"\"\"\n",
    "# fdb.print()\n",
    "# fdb.docsrc(10, \"A wrapper around plt.subplots(...); only used in show_images; not in show_image, nor in show_image_batch;\\\n",
    "# to create/display and return a fig with specified size and a specified num of empty subplots\")\n",
    "# fdb.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastview('subplots')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "_,axs = subplots()\n",
    "test_eq(axs.shape,[1])\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,axs = subplots(2,3)\n",
    "test_eq(axs.shape,[2,3])\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `_fig_bounds(x)`\n",
    "- make sure figsize is properly defined; \n",
    "- the bounds (either width or height) is between 1 and 5; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastview(\"_fig_bounds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[min(5, max(1,r)) for r in range(10)] # only allow value from 1 to 5; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "def _fig_bounds(x):\n",
    "#     pp.deep(lambda: x//32)\n",
    "    r = x//32\n",
    "#     pp.deep(lambda: min(5, max(1,r)))\n",
    "    return min(5, max(1,r))\n",
    "# to be used in show_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdb = Fastdb(_fig_bounds)\n",
    "# fdb.print()\n",
    "# fdb.docsrc(0, \"make sure figsize is properly defined; the bounds (either width or height) is between 1 and 5\")\n",
    "# fdb.docsrc(5, \"only allow value from 1 to 5\")\n",
    "# fdb.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastcodes(\"allow value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastview(\"_fig_bounds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```show_image(im, ax...)```\n",
    "- im can be a pytorch tensor, np.array or pure a PIL image object which will then be turned into np.array; \n",
    "- if im is a pytorch tensor and 1st dim is < 5, meaning if color channel is not the last dim, then the dims will be permuted; \n",
    "- if im has only one channel, remove the dim for the channel; \n",
    "- ax and its options are used to display; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastview(\"show_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.permute: to reorganise a tensor's dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "@delegates(plt.Axes.imshow, keep=True, but=['shape', 'imlim'])\n",
    "def show_image(im, ax=None, figsize=None, title=None, ctx=None, **kwargs):\n",
    "    \"Show a PIL or PyTorch image on `ax`.\"\n",
    "    # Handle pytorch axis order\n",
    "    if hasattrs(im, ('data','cpu','permute')):\n",
    "        im = im.data.cpu()\n",
    "        if im.shape[0]<5: im=im.permute(1,2,0)\n",
    "    elif not isinstance(im,np.ndarray): im=array(im)\n",
    "    # Handle 1-channel images\n",
    "    if im.shape[-1]==1: im=im[...,0]\n",
    "\n",
    "    ax = ifnone(ax,ctx)\n",
    "    if figsize is None: figsize = (_fig_bounds(im.shape[0]), _fig_bounds(im.shape[1]))\n",
    "    if ax is None: _,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im, **kwargs)\n",
    "    if title is not None: ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdb = Fastdb(show_image)\n",
    "# fdb.eg = \"\"\"\n",
    "# # pp.deep(lambda: Image.open(TEST_IMAGE_BW))\n",
    "# im = Image.open(TEST_IMAGE_BW)\n",
    "# ax = show_image(im, cmap=\"Greys\")\n",
    "# \"\"\"\n",
    "# fdb.docsrc(3, \"im can be a pytorch tensor, np.array or pure a PIL image object which will then be turned into np.array; \\\n",
    "# if im is a pytorch tensor and 1st dim is < 5, meaning if color channel is not the last dim, then the dims will be permuted; \\\n",
    "# if im has only one channel, remove the dim for the channel; ax and its options are used to display\")\n",
    "# fdb.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastview(\"show_image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`show_image` can show PIL images..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check(show_image) # to see its signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%snoop # activated by %load_ext snoop, but pp, snoop, @snoop are not available\n",
    "# pp.deep(lambda: Image.open(TEST_IMAGE_BW))\n",
    "im = Image.open(TEST_IMAGE_BW)\n",
    "# im = array(im)\n",
    "# pp.deep(lambda: show_image(im, cmap=\"Greys\"))\n",
    "ax = show_image(im, cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and color images with standard `CHW` dim order..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%snoop\n",
    "# pp.deep(lambda: np.array(Image.open(TEST_IMAGE)))\n",
    "im2 = np.array(Image.open(TEST_IMAGE))\n",
    "im2.shape\n",
    "ax = show_image(im2, figsize=(5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and color images with `HWC` dim order..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.as_tensor(im2).shape\n",
    "im3 = torch.as_tensor(im2).permute(2,0,1) # how to turn an array to a tensor and permute its dims\n",
    "im3.shape\n",
    "ax = show_image(im3, figsize=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastview(\"show_image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `show_titled_image(o, ...)`\n",
    "use a tuple o which is (img, title) to make show_image to display the image with title printed on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastview(\"show_titled_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "@delegates(show_image, keep=True)\n",
    "def show_titled_image(o, **kwargs):\n",
    "    \"Call `show_image` destructuring `o` to `(img,title)`\"\n",
    "    show_image(o[0], title=str(o[1]), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdb = Fastdb(show_titled_image)\n",
    "# fdb.eg = \"\"\"\n",
    "# %%snoop\n",
    "# show_titled_image((im3,'A puppy'), figsize=(2,2))\n",
    "# \"\"\"\n",
    "# fdb.docsrc(3, \"use a tuple o which is (img, title) to make show_image to display the image with title printed on top\")\n",
    "# fdb.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%snoop\n",
    "show_titled_image((im3,'A puppy'), figsize=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `show_images(ims, nrows, ncols, ...)`\n",
    "- to display multiple images with/wihtout titles in rows and cols; \n",
    "- first input 'ims' is a tuple of imgs; \n",
    "- 'titles' should be in tuple (e.g., a tuple of None, if not available); \n",
    "- ncols is calc by total imgs and nrows;\n",
    "- empty fig and subplots are created by subplots; \n",
    "- finally loop through each img, title, ax to show_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastview(\"show_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show all images `ims` as subplots with `rows` using `titles`. `suptitle` provides a way to create a figure title for all images. If you use `suptitle`, `constrained_layout` is used unless you set `constrained_layout` to `False`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "@delegates(subplots)\n",
    "def show_images(ims, nrows=1, ncols=None, titles=None, **kwargs):\n",
    "    \"Show all images `ims` as subplots with `rows` using `titles`.\"\n",
    "    if ncols is None: ncols = int(math.ceil(len(ims)/nrows))\n",
    "    if titles is None: titles = [None]*len(ims)\n",
    "#     pp.deep(lambda: subplots(nrows, ncols, **kwargs)[1].flat)\n",
    "    axs = subplots(nrows, ncols, **kwargs)[1].flat\n",
    "    for im,t,ax in zip(ims, titles, axs): show_image(im, ax=ax, title=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdb = Fastdb(show_images)\n",
    "# fdb.eg = \"\"\"\n",
    "# %%snoop\n",
    "# show_images((im,im3),titles=('number','puppy'),suptitle='Number Puppy',  imsize=3)\n",
    "# \"\"\"\n",
    "# fdb.docsrc(3, \"to display multiple images with/wihtout titles in rows and cols; first input 'ims' is a tuple of imgs; 'titles' should be in tuple if not None; ncols is calc by total imgs and nrows;\\\n",
    "# titles are given as None for all images if not available; empty fig and subplots are created by subplots; \\\n",
    "# finally loop through each img, title, ax to show_image; \")\n",
    "# fdb.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastview(\"show_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%snoop\n",
    "show_images((im,im3),titles=('number','puppy'),suptitle='Number Puppy',  imsize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArrayBase, ArrayImageBase, ArrayImage, ArrayImageBW, ArrayMask\n",
    "`ArrayImage`, `ArrayImageBW` and `ArrayMask` are subclasses of `ndarray`, through which images can be turned arrays and get displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ArrayBase(ndarray):\n",
    "    \"An `ndarray` that can modify casting behavior\"\n",
    "    @classmethod\n",
    "#     @snoop\n",
    "    def _before_cast(cls, x): \n",
    "#         pp(x, array(x).shape)\n",
    "#         n = array(x)\n",
    "        return x if isinstance(x,ndarray) else array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ArrayImageBase(ArrayBase):\n",
    "    \"Base class for arrays representing images\"\n",
    "    _show_args = {'cmap':'viridis'}\n",
    "#     @snoop\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        return show_image(self, ctx=ctx, **{**self._show_args, **kwargs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ArrayImage(ArrayImageBase):\n",
    "    \"An array representing an image\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ArrayImageBW(ArrayImage):\n",
    "    \"An array representing an image\"\n",
    "    _show_args = {'cmap':'Greys'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ArrayMask(ArrayImageBase):\n",
    "    \"An array representing an image mask\"\n",
    "    _show_args = {'alpha':0.5, 'cmap':'tab20', 'interpolation':'nearest'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(TEST_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%snoop\n",
    "im_t = cast(im, ArrayImage) # how to use cast to turn PIL image object into an ArrayImage object\n",
    "test_eq(type(im_t), ArrayImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = im_t.show(figsize=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastview(show_image)\n",
    "# fastnotes(\"use cast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fig_exists(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor.`__array_eq__(self:Tensor, b)`\n",
    "make 0 dim and 1 dim tensor comparison possible: `test_eq(tensor(1), tensor([1]))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before __array_eq__, we have to choose between torch.equal or ==; after, we don't need to\n",
    "tensor(100).dim()\n",
    "assert not torch.equal(tensor(100), tensor([100])) \n",
    "assert tensor(100) == tensor([100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tensor([True]): print(\"yes\") # tensor([True]) is equivalent to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "# @snoop\n",
    "def __array_eq__(self:Tensor,b):\n",
    "#     pp.deep(lambda: torch.equal(self,b) if self.dim() else self==b)\n",
    "    return torch.equal(self,b) if self.dim() else self==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastcore.test import equals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor.__array_eq__ is actually used inside equals which is from fastcore.test\n",
    "# test_eq??\n",
    "# equals??\n",
    "# test??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%snoop\n",
    "test_eq(tensor(1), torch.tensor([1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastnbs(\"__array_eq__\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor._array2tensor(x)\n",
    "wrap around `torch.from_numpy` to turn np.array into torch tensor for special cases (np.uint16=> np.float32; np.int=>np.int64 win32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastnbs(\"__array_eq__\")\n",
    "# fastlistnbs(\"src_fastai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _array2tensor(x):\n",
    "    if x.dtype==np.uint16: x = x.astype(np.float32)\n",
    "    # windows default numpy int dytpe is int32, while torch tensor default int dtype is int64\n",
    "    # https://github.com/numpy/numpy/issues/9464\n",
    "    if sys.platform == \"win32\":\n",
    "        if x.dtype==np.int: x = x.astype(np.int64)\n",
    "    return torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(array([1,2,3]).dtype, 'int64')\n",
    "test_eq(array([1.,2,3]).dtype, 'float64')\n",
    "test_eq(torch.from_numpy(array([1,2,3])).dtype, torch.int64)\n",
    "test_eq(torch.from_numpy(array([1.,2,3])).dtype, torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```tensor(x, *rest, **kwargs)```\n",
    "- fastai.tensor can convert tuple, list, ndarray, pd.Series, pd.DataFrame and pure free flowing elements into torch.tensor\n",
    "- it has potential to turn data.__array__ or iterable data into torch.tensor (commented out at the moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@use_kwargs_dict(dtype=None, device=None, requires_grad=False, pin_memory=False)\n",
    "def tensor(x, *rest, **kwargs):\n",
    "    \"Like `torch.as_tensor`, but handle lists too, and can pass multiple vector elements directly.\"\n",
    "    if len(rest): x = (x,)+rest\n",
    "    # There was a Pytorch bug in dataloader using num_workers>0. Haven't confirmed if fixed\n",
    "    # if isinstance(x, (tuple,list)) and len(x)==0: return tensor(0)\n",
    "    res = (x if isinstance(x, Tensor)\n",
    "           else torch.tensor(x, **kwargs) if isinstance(x, (tuple,list))\n",
    "           else _array2tensor(x) if isinstance(x, ndarray)\n",
    "           else as_tensor(x.values, **kwargs) if isinstance(x, (pd.Series, pd.DataFrame))\n",
    "#            else as_tensor(array(x, **kwargs)) if hasattr(x, '__array__') or is_iter(x)\n",
    "           else _array2tensor(array(x), **kwargs))\n",
    "    if res.dtype is torch.float64: return res.float()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to make a copy of a tensor properly using pytorch.tensor or fastai.tensor\n",
    "test_eq(torch.tensor([1,2,3]) is torch.tensor(torch.tensor([1,2,3])), False) # being False, meaning they are copies not reference\n",
    "# The above way of making a tensor copy will throw a warning, the pytorch recommend the following way\n",
    "test_eq(torch.tensor([1,2,3]) is torch.tensor([1,2,3]).clone().detach(), False) \n",
    "# Fastai has the following intuitive way\n",
    "test_eq(tensor(torch.tensor([1,2,3])) is torch.tensor([1,2,3]), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both fastai.tensor and torch.tensor can turn an array or a list into a tensor like below\n",
    "test_eq(torch.tensor(array([1,2,3])), tensor(array([1,2,3])))\n",
    "test_eq(torch.tensor([1,2,3]), tensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastai.tensor can handle multiple elements directly, but not torch.tensor\n",
    "test_eq(tensor(1.), torch.tensor(1.))\n",
    "try:\n",
    "    torch.tensor(1,2,3)\n",
    "except: \n",
    "    print(\"torch.tensor can't handle (1,2,3)\")\n",
    "test_eq(tensor(1,2,3), torch.tensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(tensor(torch.tensor([1,2,3])), torch.tensor([1,2,3]))\n",
    "test_eq(tensor(array([1,2,3])), torch.tensor([1,2,3]))\n",
    "test_eq(tensor(1,2,3), torch.tensor([1,2,3]))\n",
    "test_eq_type(tensor(1.0), torch.tensor(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```set_seed(s, reproducible=False)```\n",
    "set random seed for either torch.tensor, ndarray, or just pure python random, and set True or False for ```torch.backends.cudnn.deterministic/benchmark```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```set_seed``` is useful for reproducibility between runs. It is important to remember that certain classes such as ```Dataloaders``` have internal random number generators that is not effected by this function, so this must be run before such objects are created in order to guarantee reproducibility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(22 % 3, 1)\n",
    "test_eq(22 // 3, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def set_seed(s, reproducible=False):\n",
    "    \"Set random seed for `random`, `torch`, and `numpy` (where available)\"\n",
    "    try: torch.manual_seed(s)\n",
    "    except NameError: pass\n",
    "    try: torch.cuda.manual_seed_all(s)\n",
    "    except NameError: pass\n",
    "    try: np.random.seed(s%(2**32-1))\n",
    "    except NameError: pass\n",
    "    random.seed(s)\n",
    "    if reproducible:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how ```set_seed``` can be used to reset the state of random number generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed(2*33)\n",
    "set_seed(12)\n",
    "a1 = np.random.random()\n",
    "a2 = torch.rand(())\n",
    "a3 = random.random()\n",
    "# set_seed(2*33)\n",
    "set_seed(12)\n",
    "b1 = np.random.random()\n",
    "b2 = torch.rand(())\n",
    "b3 = random.random()\n",
    "print('a\\'s: {0:3.3f} {1:3.3f} {2:3.3f}'.format(a1,a2,a3))\n",
    "print('b\\'s: {0:3.3f} {1:3.3f} {2:3.3f}'.format(b1,b2,b3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(a1,b1)\n",
    "test_eq(a2,b2)\n",
    "test_eq(a3,b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```get_random_states``` and ```set_random_states```\n",
    "get random states for pure python random, ndarray, torch.tensor... and set values for those random_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```get_random_states```\n",
    "return a dict of states for python_random_state, numpy_state, torch_state, torch_cuda_state, torch_deterministic, torch_benchmark\n",
    "\n",
    "```get_random_states``` and ```set_random_states``` are useful for storing a state so you can go back to it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_random_states():\n",
    "    \"Gets states for `random`, `torch`, and `numpy` random number generators\"\n",
    "    return {'random_state':random.getstate(),\n",
    "            'numpy_state':np.random.get_state(),\n",
    "            'torch_state':torch.get_rng_state(),\n",
    "            'torch_cuda_state':torch.cuda.get_rng_state_all(),\n",
    "            'torch_deterministic':torch.backends.cudnn.deterministic,\n",
    "            'torch_benchmark':torch.backends.cudnn.benchmark}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```set_random_states```\n",
    "set values for random_state,numpy_state,torch_state,torch_cuda_state,torch_deterministic,torch_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def set_random_states(random_state,numpy_state,torch_state,torch_cuda_state,torch_deterministic,torch_benchmark):\n",
    "    \"Set states for `random`, `torch`, and `numpy` random number generators\"\n",
    "    random.setstate(random_state)\n",
    "    np.random.set_state(numpy_state)\n",
    "    torch.set_rng_state(torch_state)\n",
    "    torch.cuda.set_rng_state_all(torch_cuda_state)\n",
    "    torch.backends.cudnn.deterministic=torch_deterministic\n",
    "    torch.backends.cudnn.benchmark=torch_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below notice that the old values and rewinded values are the same because we were able to return to the previous state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_states = get_random_states()\n",
    "olds = (random.random(),np.random.random(),torch.rand(()))\n",
    "news = (random.random(),np.random.random(),torch.rand(()))\n",
    "set_random_states(**old_states)\n",
    "rewinds = (random.random(),np.random.random(),torch.rand(()))\n",
    "\n",
    "print('olds:    {0:3.3f} {1:3.3f} {2:3.3f}'.format(*olds))\n",
    "print('news:    {0:3.3f} {1:3.3f} {2:3.3f}'.format(*news))\n",
    "print('rewinds: {0:3.3f} {1:3.3f} {2:3.3f}'.format(*rewinds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ne(olds,news)\n",
    "test_eq(olds,rewinds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```no_random(seed=42, reproducible=True)```\n",
    "```with no_random(): ``` can create a random context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ```no_random``` we combine the ideas of rewinding state with ```get_random_states``` and ```set_random_states``` with the ability to ```set_seed``` and create a context manager that can allow us to control randomness in a portion of our code. \n",
    "\n",
    "Note: Similar to ```torch.random.fork_rng```, but also with ```numpy``` and ```random```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@contextmanager\n",
    "def no_random(seed=42,reproducible=True):\n",
    "    \"Stores and retrieves state of random number generators. Sets random seed for `random`, `torch`, and `numpy`.\"\n",
    "    states = get_random_states()\n",
    "    set_seed(seed,reproducible=reproducible)\n",
    "    try:\n",
    "        yield #we are managing global variables\n",
    "    finally:\n",
    "        set_random_states(**states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples on how we can use ```no_random``` to control the randomness within a block of code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states=get_random_states()\n",
    "olds = (random.random(),np.random.random(),torch.rand(()))\n",
    "set_random_states(**states) #rewinding above random calls\n",
    "\n",
    "with no_random():\n",
    "    new1 = (random.random(),np.random.random(),torch.rand(()))\n",
    "with no_random():\n",
    "    new2 = (random.random(),np.random.random(),torch.rand(()))\n",
    "with no_random(seed=100):\n",
    "    seeded1 = (random.random(),np.random.random(),torch.rand(()))\n",
    "with no_random(seed=100):\n",
    "    seeded2 = (random.random(),np.random.random(),torch.rand(()))\n",
    "        \n",
    "rewinds = (random.random(),np.random.random(),torch.rand(()))\n",
    "\n",
    "print('olds:    {0:3.3f} {1:3.3f} {2:3.3f}'.format(*olds))\n",
    "print('new1:    {0:3.3f} {1:3.3f} {2:3.3f}'.format(*new1))\n",
    "print('new2:    {0:3.3f} {1:3.3f} {2:3.3f}'.format(*new2))\n",
    "print('seeded1: {0:3.3f} {1:3.3f} {2:3.3f}'.format(*seeded1))\n",
    "print('seeded2: {0:3.3f} {1:3.3f} {2:3.3f}'.format(*seeded2))\n",
    "print('rewinds: {0:3.3f} {1:3.3f} {2:3.3f}'.format(*rewinds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that olds, and rewinds are also both equal to each other. From this  we can see that everything in the ```with``` blocks did not update the state outside of the block. Inside of the block, the state is reset for any particular seed, so for the same seed you should get the same random number generator results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It is important to remember that classes like ``` Dataloader``` have internal random number generators, and ```no_random``` will have no effect on those random number generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ne(olds,new1)\n",
    "test_eq(new1,new2)\n",
    "test_ne(new1,seeded1)\n",
    "test_eq(seeded1,seeded2)\n",
    "test_eq(olds,rewinds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```unsqueeze(x, dim=-1,n=1)```\n",
    "unsqueeze or expand a tensor another n dimensions, if current x is 1 dim, and n=2, then x will be 3 dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "def unsqueeze(x, dim=-1, n=1):\n",
    "    \"Same as `torch.unsqueeze` but can add `n` dims\"\n",
    "    for _ in range(n): x = x.unsqueeze(dim)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tensor([1])\n",
    "t2 = unsqueeze(t, n=2)\n",
    "test_eq(t2,t[:,None,None])\n",
    "test_eq(t.dim() + 2, t2.dim()) # dim == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```unsqueeze_(x, dim=-1, n=1)```\n",
    "It can do unsqueeze above but inplace (without making another copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def unsqueeze_(x, dim=-1, n=1):\n",
    "    \"Same as `torch.unsqueeze_` but can add `n` dims\"\n",
    "    for _ in range(n): x.unsqueeze_(dim)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tensor([1])\n",
    "unsqueeze_(t, n=2)\n",
    "test_eq(t, tensor([1]).view(1,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```_fa_rebuild_tensor``` and ```_fa_rebuild_qtensor```\n",
    "not used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _fa_rebuild_tensor (cls, *args, **kwargs): return cls(torch._utils._rebuild_tensor_v2(*args, **kwargs))\n",
    "def _fa_rebuild_qtensor(cls, *args, **kwargs): return cls(torch._utils._rebuild_qtensor  (*args, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```apply(func, x, *args, **kwargs)```\n",
    "apply func to x, and x can be a list, tuple, L, dict or a scalar object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain_type?\n",
    "# is_listy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def apply(func, x, *args, **kwargs):\n",
    "    \"Apply `func` recursively to `x`, passing on args\"\n",
    "    if is_listy(x): return type(x)([apply(func, o, *args, **kwargs) for o in x])\n",
    "    if isinstance(x,dict):  return {k: apply(func, v, *args, **kwargs) for k,v in x.items()}\n",
    "    res = func(x, *args, **kwargs)\n",
    "    return res if x is None else retain_type(res, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastnotes(\"slice(\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type([])([1,2,3]) # is equivalent to list([1,2,3])\n",
    "type(slice(1,3))(1,2,3) # is equivalent to slice(1,2,3)\n",
    "[0,1,2,3,4,5,6,7][slice(0,-1,2)] # how to use slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(lambda x: x + 1, [1,2,3])\n",
    "apply(lambda x: x + 1, (1,2,3))\n",
    "apply(lambda x: x + 1, L(1,2,3))\n",
    "apply(lambda x: x + 1, {'a':1, 'b':2,'c':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```maybe_gather(x, axis=0)```\n",
    "Gather copies of `x` on `axis` (if training is distributed). used in ```to_detach```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_distrib(): Return the number of processes in distributed training (if applicable). defined below in this notebook\n",
    "# num_distrib? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(tensor(0).ndim, 0)\n",
    "test_eq(tensor(1,2,3).ndim, 1)\n",
    "test_eq(tensor([[[1],[2],[3]]]).ndim, 3)\n",
    "test_eq(tensor([[[1],[2],[3]]]).shape, (1,3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_distrib():\n",
    "    \"Return the number of processes in distributed training (if applicable).\"\n",
    "    return int(os.environ.get('WORLD_SIZE', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@snoop\n",
    "def maybe_gather(x, axis=0):\n",
    "    \"Gather copies of `x` on `axis` (if training is distributed)\"\n",
    "    if num_distrib()<=1: return x\n",
    "    ndim = x.ndim\n",
    "    res = [x.new_zeros(*x.shape if ndim > 0 else (1,)) for _ in range(num_distrib())]\n",
    "    torch.distributed.all_gather(res, x.contiguous() if ndim > 0 else x[None])\n",
    "    return torch.cat(res, dim=axis) if ndim > 0 else torch.cat(res, dim=axis).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```to_detach(b, cpu=True, gather=True)```\n",
    "Recursively detach lists of tensors in `b ` (from graph and no more gradients); put them on the CPU if `cpu=True`. (non-tensor is dealt too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t.detach(): return a new tensor detached from graph so there will be no gradient\n",
    "# Tensor.detach? \n",
    "# t.cpu(): return a new tensor stored in cpu memory\n",
    "# Tensor.cpu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@snoop\n",
    "def to_detach(b, cpu=True, gather=True):\n",
    "    \"Recursively detach lists of tensors in `b `; put them on the CPU if `cpu=True`.\"\n",
    "    @snoop\n",
    "    def _inner(x, cpu=True, gather=True):\n",
    "        if not isinstance(x,Tensor): return x\n",
    "        x = x.detach()\n",
    "        if gather: x = maybe_gather(x)\n",
    "        return x.cpu() if cpu else x\n",
    "    return apply(_inner, b, cpu=cpu, gather=gather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gather` only applies during distributed training and the result tensor will be the one gathered across processes if `gather=True` (as a result, the batch size will be multiplied by the number of processes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstold = [tensor(1.,2, requires_grad=True), tensor([3.,4], requires_grad=True)]\n",
    "test_eq(lstold[0].requires_grad, True)\n",
    "lstnew = to_detach([tensor(1,2), tensor([3,4])])\n",
    "test_eq(lstnew[0].requires_grad, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```to_half(b)```\n",
    "Recursively map lists of tensors in `b ` to FP16, if b is not floating point then return unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check(torch.is_floating_point)\n",
    "# check(Tensor.half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(torch.is_floating_point(tensor(1.,2)), True)\n",
    "test_eq(torch.is_floating_point(tensor(1,2)), False)\n",
    "test_eq(tensor(1.,2.).dtype, torch.float32)\n",
    "test_eq(tensor(1.,2.).half().dtype, torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def to_half(b):\n",
    "    \"Recursively map lists of tensors in `b ` to FP16.\"\n",
    "    return apply(lambda x: x.half() if torch.is_floating_point(x) else x, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq([tensor(1.), tensor([1,2.])][0].dtype, torch.float32)\n",
    "test_eq(to_half([tensor(1.), tensor([1,2.])])[0].dtype, torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```to_float(b)```\n",
    "Recursively map lists of int tensors in `b ` to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastai.torch_core import to_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_float??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def to_float(b):\n",
    "    \"Recursively map lists of int tensors in `b ` to float.\"\n",
    "    return apply(lambda x: x.float() if torch.is_floating_point(x) else x, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(torch.is_floating_point(tensor(1,2)), False)\n",
    "test_eq(torch.is_floating_point(tensor(1.,2)), True)\n",
    "test_eq([t.float() for t in [tensor(1,2), tensor([3,4])]], [tensor([1., 2.]), tensor([3., 4.])])\n",
    "test_eq(to_float([tensor(1,2), tensor([3,4])]), [tensor([1, 2]), tensor([3, 4])]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```_has_mps()```\n",
    "whether `torch.backends.mps` is available or not. what is [mps](https://huggingface.co/docs/accelerate/usage_guides/mps)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check(torch.backends.mps.is_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "def _has_mps(): return nested_attr(torch, 'backends.mps.is_available', noop)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nested_attr??\n",
    "test_eq(torch.backends.mps.is_available(), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```default_device(use_cuda=-1)```\n",
    "use `default_device(True)` for torch to work in cuda or mps, and `default_device(False)` or `default_device(-1)` to work in cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# None: True if available; True: error if not available; False: use CPU\n",
    "defaults.use_cuda = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "def default_device(use_cuda=-1):\n",
    "    \"Return or set default device; `use_cuda`: None - CUDA if available; True - error if not available; False - CPU\"\n",
    "    if use_cuda != -1: defaults.use_cuda=use_cuda\n",
    "    use = defaults.use_cuda or (torch.cuda.is_available() and defaults.use_cuda is None)\n",
    "    assert torch.cuda.is_available() or not use\n",
    "    return torch.device(torch.cuda.current_device()) if use else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "def default_device(use=-1):\n",
    "    \"Return or set default device; `use_cuda`: -1 - CUDA/mps if available; True - error if not available; False - CPU\"\n",
    "    if use == -1: use = defaults.use_cuda\n",
    "    else: defaults.use_cuda=use\n",
    "    if use is None:\n",
    "        if torch.cuda.is_available(): use = True\n",
    "    if use:\n",
    "        if torch.cuda.is_available(): return torch.device(torch.cuda.current_device())\n",
    "        if _has_mps(): return torch.device('mps')\n",
    "    return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|cuda\n",
    "# %%snoop # to work need to remove the line above, which is  #|cuda\n",
    "if torch.cuda.is_available():\n",
    "    _td = torch.device(torch.cuda.current_device())\n",
    "    test_eq(default_device(-1), _td)\n",
    "    test_eq(default_device(True), _td)\n",
    "else:\n",
    "    test_eq(default_device(False), torch.device('cpu'))\n",
    "    test_eq(default_device(-1), torch.device('cpu'))\n",
    "    test_eq(default_device(True), torch.device('mps'))    \n",
    "    test_eq(_has_mps(), True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```to_device(b, device=None, non_blocking=False)```\n",
    "Recursively put `b` (a list of tensors) on `device` (on cpu or cuda or mps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check(Tensor.to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "def to_device(b, device=None, non_blocking=False):\n",
    "    \"Recursively put `b` on `device`.\"\n",
    "    if defaults.use_cuda==False: device='cpu'\n",
    "    elif device is None: device=default_device()\n",
    "    def _inner(o):\n",
    "        if isinstance(o,Tensor): return o.to(device, non_blocking=non_blocking)\n",
    "#         if hasattr(o, \"to_device\"): return o.to_device(device)\n",
    "        return o\n",
    "    return apply(_inner, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults.use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = to_device((3,(tensor(3),tensor(2))))\n",
    "t1,(t2,t3) = t\n",
    "test_eq(str(t2.device), 'mps:0')\n",
    "test_eq(str(t3.device), 'mps:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|cuda\n",
    "if torch.cuda.is_available():\n",
    "    test_eq_type(t,(3,(tensor(3).cuda(),tensor(2).cuda())))\n",
    "    test_eq(t2.type(), \"torch.cuda.LongTensor\")\n",
    "    test_eq(t3.type(), \"torch.cuda.LongTensor\")\n",
    "else: \n",
    "    print(\"cuda is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```to_cpu(b)```\n",
    "Recursively map lists of tensors in `b ` to the cpu exclusively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def to_cpu(b):\n",
    "    \"Recursively map lists of tensors in `b ` to the cpu.\"\n",
    "    return to_device(b,'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(t3.type(), \"torch.mps.LongTensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = to_cpu(t3)\n",
    "test_eq(t3.type(), \"torch.LongTensor\")\n",
    "test_eq(t3, 2)\n",
    "test_eq(str(t3.device), 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```to_np(x)```\n",
    "Convert a tensor to a numpy array recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def to_np(x):\n",
    "    \"Convert a tensor to a numpy array.\"\n",
    "    return apply(lambda o: o.data.cpu().numpy(), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    to_np(array(1))\n",
    "except AttributeError:\n",
    "    print('array has no attr: cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = to_np(t3)\n",
    "test_eq(type(t3), np.ndarray)\n",
    "test_eq(t3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_np([tensor(1), tensor([2,3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = map(lambda t: type(t), to_np([tensor(1), tensor([2,3])]))\n",
    "test_eq(a, numpy.ndarray)\n",
    "test_eq(b, numpy.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```range_of(a, b=None, step=None)```\n",
    "All indices of collection `a`, if `a` is a collection, otherwise `range`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check(range_of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_of(tensor([[1,2]]))\n",
    "range_of(tensor([[1,2],[3,4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L(1,2,3)\n",
    "sum([1,2,3], L())\n",
    "sum([[1],[2],[3]], L())\n",
    "sum([[1],[2],[3]], list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```to_concat(xs, dim=0)```\n",
    "Concat the element in `xs` (recursively if they are tuples/lists of tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop(depth=2)\n",
    "# @snoop\n",
    "def to_concat(xs, dim=0):\n",
    "    \"Concat the element in `xs` (recursively if they are tuples/lists of tensors)\"\n",
    "    if not xs: return xs\n",
    "    if is_listy(xs[0]): return type(xs[0])([to_concat([x[i] for x in xs], dim=dim) for i in range_of(xs[0])])\n",
    "    if isinstance(xs[0],dict):  return {k: to_concat([x[k] for x in xs], dim=dim) for k in xs[0].keys()}\n",
    "    #We may receive xs that are not concatenable (inputs of a text classifier for instance),\n",
    "    #   in this case we return a big list\n",
    "    try:    return retain_type(torch.cat(xs, dim=dim), xs[0])\n",
    "    except: \n",
    "        lst = []\n",
    "        for o_ in xs:\n",
    "            for i in range_of(o_):\n",
    "#                 pp.deep(lambda: o_.index_select(dim, tensor(i)).squeeze(dim))\n",
    "                item = retain_type(o_.index_select(dim, tensor(i)).squeeze(dim), xs[0]) # squeeze out one dim for text classifier\n",
    "                lst.append(L(item))\n",
    "        return sum(lst, L()) # bring a list of things into a single L list\n",
    "#         return sum([L(retain_type(o_.index_select(dim, tensor(i)).squeeze(dim), xs[0])\n",
    "#                           for i in range_of(o_)) for o_ in xs], L())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(tensor([1,2]).shape, torch.Size([2]))\n",
    "test_eq(tensor([1,2]).dim(), 1)\n",
    "test_eq(tensor([1,2,3,4]).dim(), 1)\n",
    "test_eq(to_concat([tensor([1,2]), tensor([3,4])]), tensor([1,2,3,4])) # dim = 1 won't work as the tensors have only 0 dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(to_concat([tensor([[1,2]]), tensor([[3,4]])], dim=0).shape[0], 2) # concat on rows, as it has 2 dims\n",
    "test_eq(to_concat([tensor([[1,2]]), tensor([[3,4]])], dim=1).shape[1], 4) # concat on cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_concat([(tensor([1,2]), tensor([3,4])), (tensor([3,4]), tensor([5,6]))]) \n",
    "to_concat([(tensor([1,2]), tensor([3,4])), (tensor([3,4]), tensor([5,6]))], dim=-1) # when tensor has 1 dim, then dim=0 or -1 same\n",
    "to_concat([(tensor([[1,2]]), tensor([[3,4]])), (tensor([[3,4]]), tensor([[5,6]]))]) # when each tensor has 2 dims, dim=1 is working\n",
    "to_concat([(tensor([[1,2]]), tensor([[3,4]])), (tensor([[3,4]]), tensor([[5,6]]))], dim=1) # or dim=1 or -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when tensors have different dims, meaning can't concat, then sum(L(...)) get run to squeeze them out, and put into a L list\n",
    "# tensor([3,4]) is squeezed into tensor(3), tensor(4)\n",
    "# tensor([[5,6]]) is squeezed into tensor([5,6])\n",
    "to_concat([tensor([3,4]), tensor([[5,6]])]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim can only be 0 or -1, as tensor([3,4]) and tensor([7,8]) have just 1 dim\n",
    "# now xs is a list of two lists, it uses a different formula, which concat the first parts of two lists, then 2nd parts of two lists\n",
    "# as as tensor([3,4]) and tensor([7,8]) have just 1 dim, so they can only concat as a long row\n",
    "to_concat([[tensor([3,4]), tensor([[5,6]])], [tensor([7,8]), tensor([[9,10]])]], dim=0) \n",
    "to_concat([[tensor([3,4]), tensor([[5,6]])], [tensor([7,8]), tensor([[9,10]])]], dim=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_concat([(tensor([1,2]),), (tensor([3,4]),)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# although xs[0] is not a list, but both tensors in xs share the same dim e.g., 2 here, torch.cat(xs,dim=0) can apply directly\n",
    "# test_eq(tensor([[1,2]]).dim(), tensor([[3,4], [5,6]]).dim())\n",
    "# tensor([[1,2]]).shape, tensor([[3,4], [5,6]]).shape\n",
    "to_concat([tensor([[1,2]]), tensor([[3,4], [5,6]])], dim=0) # concat on rows, ok because both tensors have same num of cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to concat tensors on cols, require they having same num of rows, as this example below\n",
    "to_concat([tensor([[1,2],[7,8]]), tensor([[3,4], [5,6]])], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to concat tensors on cols, require they having same num of rows, but the tensors below don't match on rows, so \n",
    "# we have to squeeze them and put into a long L list\n",
    "# only the tensor(1) get out of tensor([[1,2]]) is due to range_of(tensor([[1,2]])) returns [0]\n",
    "to_concat([tensor([[1,2]]), tensor([[3,4], [5,6]])], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_concat([tensor([[1,2]]), tensor([[3,4], [5,6]])], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st part of two lists have different dims, so they are squeezed and put into a long L list as [tensor([1, 2]),tensor(3),tensor(4)]\n",
    "# 2nd parts of two lists have same dims, so they can concat as above\n",
    "to_concat([(tensor([[1,2]]), tensor([[3,4]])), (tensor([3,4]), tensor([[5,6]]))])\n",
    "to_concat([(tensor([[1,2]]), tensor([[3,4]])), (tensor([3,4]), tensor([[5,6]]))], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(foo=tensor([1,2]), bar=tensor(3,4))\n",
    "to_concat([dict(foo=tensor([1,2]), bar=tensor(3,4))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(type(to_concat([dict(foo=tensor([1,2]), bar=tensor(3,4))])), dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Tensor.index_select(dim, index)```\n",
    "- select part of a tensor to become a new tensor, ```dim=0``` to select row, ```dim=1``` to select col, \n",
    "- so when `dim=1` and `index=0` will select first col\n",
    "- `dim=0` and `index=1` will select second row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check(Tensor.index_select)\n",
    "tensor([[1,2]]).shape\n",
    "tensor([[1,2]]).dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([[1,2],[3,4]]).index_select(1, tensor(0))\n",
    "tensor([[1,2],[3,4]]).index_select(1, tensor(1))\n",
    "tensor([[1,2],[3,4]]).index_select(0, tensor(0))\n",
    "tensor([[1,2]]).index_select(1, tensor(0))\n",
    "tensor([[1,2]]).index_select(1, tensor(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor subtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```t1.set_meta(t2, as_copy=False)```\n",
    "pass tensor t2's `__dict__` to Tensor t1, either as a copy or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def set_meta(self:Tensor, x, as_copy=False):\n",
    "    \"Set all metadata in `__dict__`\"\n",
    "    if not hasattr(x,'__dict__'): return\n",
    "    # XXX: change to `deepcopy` once PyTorch 1.7.1 is out, and check nb 23 segmentation fit works\n",
    "    self.__dict__ = copy(x.__dict__) if as_copy else x.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = tensor(1,2)\n",
    "t2.__dict__ = {'a': 1}\n",
    "test_eq(t2.__dict__, {'a': 1})\n",
    "t1 = tensor(3,4)\n",
    "t1.set_meta(t2)\n",
    "test_eq(t1.__dict__, {'a': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pass ```torch.Tensor.as_subclass``` to ```torch.as_subclass```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "if not hasattr(torch,'as_subclass'): torch.as_subclass = torch.Tensor.as_subclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```retain_meta(x, res, as_copy=False)```\n",
    "- copy `x.__dict__` to `res.__dict__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_meta(x, res, as_copy=False):\n",
    "    \"Call `res.set_meta(x)`, if it exists\"\n",
    "    if hasattr(res,'set_meta'): \n",
    "        res.set_meta(x, as_copy=as_copy)\n",
    "#         if hasattr(x, '__dict__'): print(\"{} of type {} .__dict__: {}\".format(x, type(x), x.__dict__))\n",
    "#         if hasattr(res, '__dict__'): print(\"{} of type {} .__dict__: {}\".format(res, type(res), res.__dict__))            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```t2 = t.as_subclass(_T)```\n",
    "create another tensor `t2` as an instance of `_T` and copy `__dict__` from `t`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "# @snoop(depth=2)\n",
    "def as_subclass(self:Tensor, typ):\n",
    "    \"Cast to `typ` and include `__dict__` and meta\"\n",
    "    return retain_meta(self, torch.as_subclass(self, typ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check(torch.as_subclass)\n",
    "class _T(Tensor): pass\n",
    "t = tensor(1.).requires_grad_()\n",
    "t.img_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.as_subclass(t, _T) only makes a new tensor instance of the class _T\n",
    "t1 = torch.as_subclass(t, _T) \n",
    "test_eq(isinstance(t1, _T), True)\n",
    "test_eq(isinstance(t, _T), False)\n",
    "test_eq(t1.__dict__, {})\n",
    "test_eq(t.__dict__, {'img_size':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastai's Tensor.as_subclass(self, _T): \n",
    "# not only create an new tensor from _T, but also pass self (another tensor)'s __dict__ to the new tensor\n",
    "t2 = t.as_subclass(_T) \n",
    "test_eq(isinstance(t2, _T), True)\n",
    "test_eq(t2.__dict__, t.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tensor.set_meta` and `Tensor.as_subclass` work together to maintain `__dict__` after casting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _T(Tensor): pass\n",
    "t = tensor(1.).requires_grad_()\n",
    "t.img_size = 1\n",
    "t2 = t.as_subclass(_T) # create \n",
    "test_eq(t.img_size, t2.img_size)\n",
    "test_eq(t2.img_size, 1)\n",
    "assert(t2.requires_grad_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```_torch_handled(args, opt, func)```\n",
    "False, if `func` is not in `opt`; True, if func in `opt` and `args` is an instance of `opt[func]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(all([True, True, True]), True)\n",
    "test_eq(all([True, False, True]), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _torch_handled(args, opt, func):\n",
    "    if func not in opt: return False\n",
    "    for oks in opt[func]:\n",
    "        if all(isinstance(arg,ok) for arg,ok in zip(args,oks) if ok): return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```_rebuild_from_type(func, type, args, dict)```\n",
    "`t = _rebuild_from_type(tensor, torch.Tensor, [1,2,3], {'b':1})`, to create an instance using a func, a class, args and a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# from https://github.com/pytorch/pytorch/blob/13c975684a220ec096216ec6468ccd0dc90ff50a/torch/_tensor.py#L34\n",
    "def _rebuild_from_type(func, type, args, dict):\n",
    "    ret = func(*args).as_subclass(type)\n",
    "    ret.__dict__ = dict\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = _rebuild_from_type(tensor, torch.Tensor, [1,2,3], {'b':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(t.__dict__, {'b':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```_find_args(x)```\n",
    "`_find_args([\"a\", \"b\", tensor(2)])` find the arg with `__dict__` from a list of args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _find_args(x):   \n",
    "    x0 = x[0] if is_listy(x[0]) and x[0] else x\n",
    "    return [a for a in x0 if hasattr(a,'__dict__')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(_find_args([\"a\", \"b\", tensor(2)]), [tensor(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TensorBase(Tensor)```\n",
    "Many methods inside are overriding those inside Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```TensorBase.__new__(cls, x, **kwargs)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TensorBase(Tensor):\n",
    "    \"A `Tensor` which support subclass pickling, and maintains metadata when casting or after methods\"\n",
    "    debug,_opt = False,defaultdict(list)\n",
    "    def __new__(cls, x, **kwargs):\n",
    "        res = cast(tensor(x), cls)\n",
    "        for k,v in kwargs.items(): setattr(res, k, v)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = TensorBase(1)\n",
    "test_eq(type(a), TensorBase)\n",
    "a1 = TensorBase.__new__(TensorBase, 1)\n",
    "a2 = a.__new__(TensorBase, 1)\n",
    "test_eq_type(a, a1)\n",
    "test_eq_type(a2, a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `TensorBase` and its subclasses also allow for passing through metadata size as img_size...\n",
    "a = TensorBase(1,img_size=(128,128))\n",
    "test_eq(a.img_size,(128,128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TensorBase._before_cast(cls, x)```\n",
    "just return `tensor(x)` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TensorBase(Tensor):\n",
    "    \"A `Tensor` which support subclass pickling, and maintains metadata when casting or after methods\"\n",
    "    debug,_opt = False,defaultdict(list)\n",
    "    def __new__(cls, x, **kwargs):\n",
    "        res = cast(tensor(x), cls)\n",
    "        for k,v in kwargs.items(): setattr(res, k, v)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _before_cast(cls, x): return tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(TensorBase._before_cast(1), tensor(1))\n",
    "test_eq(TensorBase(2)._before_cast(1), tensor(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TensorBase.__repr__(self)```\n",
    "represent the tensor in `TensorBase`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TensorBase(Tensor):\n",
    "    \"A `Tensor` which support subclass pickling, and maintains metadata when casting or after methods\"\n",
    "    debug,_opt = False,defaultdict(list)\n",
    "    def __new__(cls, x, **kwargs):\n",
    "        res = cast(tensor(x), cls)\n",
    "        for k,v in kwargs.items(): setattr(res, k, v)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _before_cast(cls, x): return tensor(x)\n",
    "    def __repr__(self): \n",
    "#         pp(self.__class__, self.__class__.__name__, super().__repr__())\n",
    "        return re.sub('tensor', self.__class__.__name__, super().__repr__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorBase(1).__repr__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TensorBase.__reduce_ex__(self, proto)```\n",
    "handling pickle.dump and pickle.loads, but no idea how this method get triggered (question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump??\n",
    "# pickle.loads??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TensorBase(Tensor):\n",
    "    \"A `Tensor` which support subclass pickling, and maintains metadata when casting or after methods\"\n",
    "    debug,_opt = False,defaultdict(list)\n",
    "    def __new__(cls, x, **kwargs):\n",
    "        res = cast(tensor(x), cls)\n",
    "        for k,v in kwargs.items(): setattr(res, k, v)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _before_cast(cls, x): return tensor(x)\n",
    "    def __repr__(self): \n",
    "#         pp(self.__class__, self.__class__.__name__, super().__repr__())\n",
    "        return re.sub('tensor', self.__class__.__name__, super().__repr__())\n",
    "\n",
    "#     @snoop\n",
    "    def __reduce_ex__(self,proto):\n",
    "        torch.utils.hooks.warn_if_has_hooks(self)\n",
    "        args = (self.storage(), self.storage_offset(), tuple(self.size()), self.stride())\n",
    "        if self.is_quantized: args = args + (self.q_scale(), self.q_zero_point())\n",
    "        args = args + (self.requires_grad, OrderedDict())\n",
    "        f = torch._utils._rebuild_qtensor if self.is_quantized else  torch._utils._rebuild_tensor_v2\n",
    "        return (_rebuild_from_type, (f, type(self), args, self.__dict__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _T(TensorBase): pass\n",
    "t = _T(range(5))\n",
    "test_eq(type(pickle.loads(pickle.dumps(t))), _T) # trigger __reduce_ex__(self,proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TensorBase.register_func(cls, func, *oks)```\n",
    "add a list of functions stored in `oks` to `cls._opt[func]`. There is no code example in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TensorBase(Tensor):\n",
    "    \"A `Tensor` which support subclass pickling, and maintains metadata when casting or after methods\"\n",
    "    debug,_opt = False,defaultdict(list)\n",
    "    def __new__(cls, x, **kwargs):\n",
    "        res = cast(tensor(x), cls)\n",
    "        for k,v in kwargs.items(): setattr(res, k, v)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _before_cast(cls, x): return tensor(x)\n",
    "    def __repr__(self): \n",
    "#         pp(self.__class__, self.__class__.__name__, super().__repr__())\n",
    "        return re.sub('tensor', self.__class__.__name__, super().__repr__())\n",
    "\n",
    "#     @snoop\n",
    "    def __reduce_ex__(self,proto):\n",
    "        torch.utils.hooks.warn_if_has_hooks(self)\n",
    "        args = (self.storage(), self.storage_offset(), tuple(self.size()), self.stride())\n",
    "        if self.is_quantized: args = args + (self.q_scale(), self.q_zero_point())\n",
    "        args = args + (self.requires_grad, OrderedDict())\n",
    "        f = torch._utils._rebuild_qtensor if self.is_quantized else  torch._utils._rebuild_tensor_v2\n",
    "        return (_rebuild_from_type, (f, type(self), args, self.__dict__))\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "#     @snoop\n",
    "    def register_func(cls, func, *oks): cls._opt[func].append(oks)\n",
    "#         pp.deep(lambda: cls._opt[func].append(oks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run this example, need to run cells below which define TensorMask etc\n",
    "# for o in Tensor.__getitem__, Tensor.__ne__,Tensor.__eq__,Tensor.add,Tensor.sub,Tensor.mul,Tensor.div,Tensor.__rsub__,Tensor.__radd__,Tensor.matmul,Tensor.bmm:\n",
    "#     TensorBase.register_func(o, TensorMask, TensorImageBase)\n",
    "#     TensorBase.register_func(o, TensorImageBase, TensorMask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TensorBase.__torch_function__```\n",
    "- performing operation like addition, repr, multiplication etc \n",
    "- and take meta info from tensors and give it to the resulting tensor of the operation; \n",
    "- set `TensorBase.debug=True` to print out more info; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TensorBase(Tensor):\n",
    "    \"A `Tensor` which support subclass pickling, and maintains metadata when casting or after methods\"\n",
    "    debug,_opt = False,defaultdict(list)\n",
    "    def __new__(cls, x, **kwargs):\n",
    "        res = cast(tensor(x), cls)\n",
    "        for k,v in kwargs.items(): setattr(res, k, v)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _before_cast(cls, x): return tensor(x)\n",
    "    def __repr__(self): \n",
    "#         pp(self.__class__, self.__class__.__name__, super().__repr__())\n",
    "        return re.sub('tensor', self.__class__.__name__, super().__repr__())\n",
    "\n",
    "#     @snoop\n",
    "    def __reduce_ex__(self,proto):\n",
    "        torch.utils.hooks.warn_if_has_hooks(self)\n",
    "        args = (self.storage(), self.storage_offset(), tuple(self.size()), self.stride())\n",
    "        if self.is_quantized: args = args + (self.q_scale(), self.q_zero_point())\n",
    "        args = args + (self.requires_grad, OrderedDict())\n",
    "        f = torch._utils._rebuild_qtensor if self.is_quantized else  torch._utils._rebuild_tensor_v2\n",
    "        return (_rebuild_from_type, (f, type(self), args, self.__dict__))\n",
    "\n",
    "    @classmethod\n",
    "#     @snoop\n",
    "    def register_func(cls, func, *oks): cls._opt[func].append(oks)\n",
    "\n",
    "    @classmethod\n",
    "#     @snoop\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        if cls.debug and func.__name__ not in ('__str__','__repr__'): print(func, types, args, kwargs)\n",
    "        if _torch_handled(args, cls._opt, func): types = (torch.Tensor,)\n",
    "        res = super().__torch_function__(func, types, args, ifnone(kwargs, {}))\n",
    "        dict_objs = _find_args(args) if args else _find_args(list(kwargs.values()))\n",
    "        if issubclass(type(res),TensorBase) and dict_objs: res.set_meta(dict_objs[0],as_copy=True)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBase.__torch_function__??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TensorBase` hooks into `__torch_function__` to ensure metadata is not lost. To see all functions being called, set `debug`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = TensorBase(1)\n",
    "# a.debug=True\n",
    "TensorBase.debug=True\n",
    "# a.debug=False\n",
    "1/(a+1)\n",
    "# a + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ```default_collate(listy)```\n",
    "to prepare a batch of data or tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data._utils.collate import default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_collate??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with a batch of `int`s:\n",
    "test_eq(default_collate([0, 1, 2, 3]), tensor([0, 1, 2, 3]))\n",
    "# Example with a batch of `str`s:\n",
    "test_eq(default_collate(['a', 'b', 'c']), ['a', 'b', 'c'])\n",
    "# Example with `Map` inside the batch:\n",
    "default_collate([{'A':0, 'B':1}, {'A': 100, 'B': 100}]) # {'A': tensor([  0, 100]), 'B': tensor([  1, 100])}\n",
    "# Example with `NamedTuple` inside the batch:\n",
    "Point = namedtuple('Point', ['x', 'y'])\n",
    "test_eq(default_collate([Point(0, 0), Point(1, 1)]), Point(x=tensor([0, 1]), y=tensor([0, 1])))\n",
    "# Example with `Tuple` inside the batch:\n",
    "test_eq(default_collate([(0, 1), (2, 3)]), [tensor([0, 2]), tensor([1, 3])])\n",
    "# Example with `List` inside the batch:\n",
    "test_eq(default_collate([[0, 1], [2, 3]]), [tensor([0, 2]), tensor([1, 3])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TensorBase.new_tensor(size, dtype=None, device=None, requires_grad=False)```\n",
    "- `t.new_tensor(1)`\n",
    "- first, use `tensor.new_tensor` to create a new tensor with data 1, then use `as_subclass` to make the new tensor of the same class as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the original pytorch Tensor.new_tensor docs\n",
    "# Tensor.new_tensor??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor(1).new_tensor((1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TensorBase(Tensor):\n",
    "    \"A `Tensor` which support subclass pickling, and maintains metadata when casting or after methods\"\n",
    "    debug,_opt = False,defaultdict(list)\n",
    "    def __new__(cls, x, **kwargs):\n",
    "        res = cast(tensor(x), cls)\n",
    "        for k,v in kwargs.items(): setattr(res, k, v)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _before_cast(cls, x): return tensor(x)\n",
    "    def __repr__(self): \n",
    "        return re.sub('tensor', self.__class__.__name__, super().__repr__())\n",
    "\n",
    "    def __reduce_ex__(self,proto):\n",
    "        torch.utils.hooks.warn_if_has_hooks(self)\n",
    "        args = (self.storage(), self.storage_offset(), tuple(self.size()), self.stride())\n",
    "        if self.is_quantized: args = args + (self.q_scale(), self.q_zero_point())\n",
    "        args = args + (self.requires_grad, OrderedDict())\n",
    "        f = torch._utils._rebuild_qtensor if self.is_quantized else  torch._utils._rebuild_tensor_v2\n",
    "        return (_rebuild_from_type, (f, type(self), args, self.__dict__))\n",
    "\n",
    "    @classmethod\n",
    "    def register_func(cls, func, *oks): cls._opt[func].append(oks)\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        if cls.debug and func.__name__ not in ('__str__','__repr__'): print(func, types, args, kwargs)\n",
    "        if _torch_handled(args, cls._opt, func): types = (torch.Tensor,)\n",
    "        res = super().__torch_function__(func, types, args, ifnone(kwargs, {}))\n",
    "        dict_objs = _find_args(args) if args else _find_args(list(kwargs.values()))\n",
    "        if issubclass(type(res),TensorBase) and dict_objs: res.set_meta(dict_objs[0],as_copy=True)\n",
    "        return res\n",
    "\n",
    "#     @snoop\n",
    "    def new_tensor(self, size, dtype=None, device=None, requires_grad=False):\n",
    "        cls = type(self)\n",
    "        return self.as_subclass(Tensor).new_tensor(size, dtype=dtype, device=device, requires_grad=requires_grad).as_subclass(cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastnbs(\"as_subclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBase.new_tensor??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _T(TensorBase): pass\n",
    "t = _T(range(5))\n",
    "test_eq(t.new_tensor(1), _T(1))\n",
    "# test_eq_type(t.new_tensor([1,2]), _T([1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TensorBase.new_ones(self, data, dtype=None, device=None, requires_grad=False)```\n",
    "- `t.new_ones((2,3))`\n",
    "- make `t` a subclass of `torch.Tensor` and use `Tensor.new_ones` to create a tensor of value with shape (2,3)\n",
    "- then make the new tensor a subclass as that of `t`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor.new_ones?\n",
    "# torch.as_subclass??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor((), dtype=torch.int32)\n",
    "t1.new_ones((2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TensorBase(Tensor):\n",
    "    \"A `Tensor` which support subclass pickling, and maintains metadata when casting or after methods\"\n",
    "    debug,_opt = False,defaultdict(list)\n",
    "    def __new__(cls, x, **kwargs):\n",
    "        res = cast(tensor(x), cls)\n",
    "        for k,v in kwargs.items(): setattr(res, k, v)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _before_cast(cls, x): return tensor(x)\n",
    "    def __repr__(self): \n",
    "#         pp(self.__class__, self.__class__.__name__, super().__repr__())\n",
    "        return re.sub('tensor', self.__class__.__name__, super().__repr__())\n",
    "\n",
    "    def __reduce_ex__(self,proto):\n",
    "        torch.utils.hooks.warn_if_has_hooks(self)\n",
    "        args = (self.storage(), self.storage_offset(), tuple(self.size()), self.stride())\n",
    "        if self.is_quantized: args = args + (self.q_scale(), self.q_zero_point())\n",
    "        args = args + (self.requires_grad, OrderedDict())\n",
    "        f = torch._utils._rebuild_qtensor if self.is_quantized else  torch._utils._rebuild_tensor_v2\n",
    "        return (_rebuild_from_type, (f, type(self), args, self.__dict__))\n",
    "\n",
    "    @classmethod\n",
    "    def register_func(cls, func, *oks): cls._opt[func].append(oks)\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        if cls.debug and func.__name__ not in ('__str__','__repr__'): print(func, types, args, kwargs)\n",
    "        if _torch_handled(args, cls._opt, func): types = (torch.Tensor,)\n",
    "        res = super().__torch_function__(func, types, args, ifnone(kwargs, {}))\n",
    "        dict_objs = _find_args(args) if args else _find_args(list(kwargs.values()))\n",
    "        if issubclass(type(res),TensorBase) and dict_objs: res.set_meta(dict_objs[0],as_copy=True)\n",
    "        return res\n",
    "\n",
    "    def new_tensor(self, size, dtype=None, device=None, requires_grad=False):\n",
    "        cls = type(self)\n",
    "        return self.as_subclass(Tensor).new_tensor(size, dtype=dtype, device=device, requires_grad=requires_grad).as_subclass(cls)\n",
    "\n",
    "#     @snoop\n",
    "    def new_ones(self, data, dtype=None, device=None, requires_grad=False):\n",
    "        cls = type(self)\n",
    "        return self.as_subclass(Tensor).new_ones(data, dtype=dtype, device=device, requires_grad=requires_grad).as_subclass(cls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _T(TensorBase): pass\n",
    "t = _T(range(5))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.new_ones(3)\n",
    "t.new_ones((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq_type(t.new_ones(1), _T([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TensorBase.new((self, x=None)```\n",
    "- use `Tensor.new` to create a new tensor\n",
    "- use `as_subclass` to make the new tensor to receive `t`'s class but **not** `t.__dict__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = Tensor([1,2])\n",
    "t2.__dict__ = {'a':1}\n",
    "t2.new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = t2.new((2,3))\n",
    "getattr(t3, \"__dict__\")\n",
    "# help(t2.new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TensorBase(Tensor):\n",
    "    \"A `Tensor` which support subclass pickling, and maintains metadata when casting or after methods\"\n",
    "    debug,_opt = False,defaultdict(list)\n",
    "    def __new__(cls, x, **kwargs):\n",
    "        res = cast(tensor(x), cls)\n",
    "        for k,v in kwargs.items(): setattr(res, k, v)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _before_cast(cls, x): return tensor(x)\n",
    "    def __repr__(self): \n",
    "#         pp(self.__class__, self.__class__.__name__, super().__repr__())\n",
    "        return re.sub('tensor', self.__class__.__name__, super().__repr__())\n",
    "\n",
    "    def __reduce_ex__(self,proto):\n",
    "        torch.utils.hooks.warn_if_has_hooks(self)\n",
    "        args = (self.storage(), self.storage_offset(), tuple(self.size()), self.stride())\n",
    "        if self.is_quantized: args = args + (self.q_scale(), self.q_zero_point())\n",
    "        args = args + (self.requires_grad, OrderedDict())\n",
    "        f = torch._utils._rebuild_qtensor if self.is_quantized else  torch._utils._rebuild_tensor_v2\n",
    "        return (_rebuild_from_type, (f, type(self), args, self.__dict__))\n",
    "\n",
    "    @classmethod\n",
    "    def register_func(cls, func, *oks): cls._opt[func].append(oks)\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        if cls.debug and func.__name__ not in ('__str__','__repr__'): print(func, types, args, kwargs)\n",
    "        if _torch_handled(args, cls._opt, func): types = (torch.Tensor,)\n",
    "        res = super().__torch_function__(func, types, args, ifnone(kwargs, {}))\n",
    "        dict_objs = _find_args(args) if args else _find_args(list(kwargs.values()))\n",
    "        if issubclass(type(res),TensorBase) and dict_objs: res.set_meta(dict_objs[0],as_copy=True)\n",
    "        return res\n",
    "\n",
    "    def new_tensor(self, size, dtype=None, device=None, requires_grad=False):\n",
    "        cls = type(self)\n",
    "        return self.as_subclass(Tensor).new_tensor(size, dtype=dtype, device=device, requires_grad=requires_grad).as_subclass(cls)\n",
    "\n",
    "\n",
    "    def new_ones(self, data, dtype=None, device=None, requires_grad=False):\n",
    "        cls = type(self)\n",
    "        return self.as_subclass(Tensor).new_ones(data, dtype=dtype, device=device, requires_grad=requires_grad).as_subclass(cls)\n",
    "\n",
    "#     @snoop(depth=3)\n",
    "    def new(self, x=None):\n",
    "        cls = type(self)\n",
    "        res = self.as_subclass(Tensor).new() if x is None else self.as_subclass(Tensor).new(x)\n",
    "        return res.as_subclass(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T(TensorBase): pass\n",
    "t = T(1)\n",
    "t.__dict__ = {'a':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t.as_subclass??\n",
    "# retain_meta??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = t.new()\n",
    "t1.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tensor)\n",
    "# tensor??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastnbs(\"as_subclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T(1).new((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T(1.).new((1.,2.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TensorBase.requires_grad_(self, requires_grad=True)```\n",
    "- set a tensor's `requires_grad` to be True or False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TensorBase(Tensor):\n",
    "    \"A `Tensor` which support subclass pickling, and maintains metadata when casting or after methods\"\n",
    "    debug,_opt = False,defaultdict(list)\n",
    "    def __new__(cls, x, **kwargs):\n",
    "        res = cast(tensor(x), cls)\n",
    "        for k,v in kwargs.items(): setattr(res, k, v)\n",
    "        return res\n",
    "\n",
    "    @classmethod\n",
    "    def _before_cast(cls, x): return tensor(x)\n",
    "    def __repr__(self): \n",
    "#         pp(self.__class__, self.__class__.__name__, super().__repr__())\n",
    "        return re.sub('tensor', self.__class__.__name__, super().__repr__())\n",
    "\n",
    "    def __reduce_ex__(self,proto):\n",
    "        torch.utils.hooks.warn_if_has_hooks(self)\n",
    "        args = (self.storage(), self.storage_offset(), tuple(self.size()), self.stride())\n",
    "        if self.is_quantized: args = args + (self.q_scale(), self.q_zero_point())\n",
    "        args = args + (self.requires_grad, OrderedDict())\n",
    "        f = torch._utils._rebuild_qtensor if self.is_quantized else  torch._utils._rebuild_tensor_v2\n",
    "        return (_rebuild_from_type, (f, type(self), args, self.__dict__))\n",
    "\n",
    "    @classmethod\n",
    "    def register_func(cls, func, *oks): cls._opt[func].append(oks)\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        if cls.debug and func.__name__ not in ('__str__','__repr__'): print(func, types, args, kwargs)\n",
    "        if _torch_handled(args, cls._opt, func): types = (torch.Tensor,)\n",
    "        res = super().__torch_function__(func, types, args, ifnone(kwargs, {}))\n",
    "        dict_objs = _find_args(args) if args else _find_args(list(kwargs.values()))\n",
    "        if issubclass(type(res),TensorBase) and dict_objs: res.set_meta(dict_objs[0],as_copy=True)\n",
    "        return res\n",
    "\n",
    "    def new_tensor(self, size, dtype=None, device=None, requires_grad=False):\n",
    "        cls = type(self)\n",
    "        return self.as_subclass(Tensor).new_tensor(size, dtype=dtype, device=device, requires_grad=requires_grad).as_subclass(cls)\n",
    "\n",
    "    def new_ones(self, data, dtype=None, device=None, requires_grad=False):\n",
    "        cls = type(self)\n",
    "        return self.as_subclass(Tensor).new_ones(data, dtype=dtype, device=device, requires_grad=requires_grad).as_subclass(cls)\n",
    "\n",
    "    def new(self, x=None):\n",
    "        cls = type(self)\n",
    "        res = self.as_subclass(Tensor).new() if x is None else self.as_subclass(Tensor).new(x)\n",
    "        return res.as_subclass(cls)\n",
    "    \n",
    "#     @snoop\n",
    "    def requires_grad_(self, requires_grad=True):\n",
    "        # Workaround https://github.com/pytorch/pytorch/issues/50219\n",
    "        self.requires_grad = requires_grad\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# test of https://github.com/pytorch/pytorch/issues/50219\n",
    "x = TensorBase(torch.rand(4,3,16,16))\n",
    "with torch.no_grad():\n",
    "    y = x.requires_grad_()\n",
    "    assert y.requires_grad and x.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More demonstration of ```TensorBase```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastnbs(\"cast(x\", filter_folder=\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = TensorBase(1,img_size=(128,128))\n",
    "test_eq(a.img_size,(128,128))\n",
    "b = cast(a,TensorBase)\n",
    "test_eq(b.img_size,(128,128))\n",
    "test_eq(torch.stack([a,b],0), TensorBase([1, 1]))\n",
    "test_eq(torch.stack([a,b],0).img_size,(128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TImage(TensorBase): pass\n",
    "class _TImage2(_TImage): pass\n",
    "t1 = _TImage([1.])\n",
    "t2 = _TImage2([1.])\n",
    "t2+t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _T(TensorBase): pass\n",
    "\n",
    "t = _T(range(5))\n",
    "test_eq(repr(t), '_T([0, 1, 2, 3, 4])')\n",
    "test_eq(t[0], 0)\n",
    "test_eq_type(t+1, _T(range(1,6)))\n",
    "\n",
    "test_eq_type(t[_T([False,False,True,True,True])], _T([2,3,4]))\n",
    "test_eq_type(t[_T([2,3,4])], _T([2,3,4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tensor([1,2,3])\n",
    "m = TensorBase([False,True,True])\n",
    "test_eq(t[m], tensor([2,3]))\n",
    "t = tensor([[1,2,3],[1,2,3]])\n",
    "m = cast(tensor([[False,True,True],\n",
    "                 [False,True,True]]), TensorBase)\n",
    "test_eq(t[m], tensor([2,3,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tensor([[1,2,3],[1,2,3]])\n",
    "t.img_size = 1\n",
    "t2 = cast(t, TensorBase)\n",
    "test_eq(t2.img_size, t.img_size)\n",
    "x = retain_type(tensor([4,5,6]), t2)\n",
    "test_eq(x.img_size, t.img_size)\n",
    "t3 = TensorBase([[1,2,3],[1,2,3]], img_size=1)\n",
    "test_eq(t3.img_size, t.img_size)\n",
    "t4 = t2+1\n",
    "t4.img_size = 2\n",
    "test_eq(t2.img_size, 1)\n",
    "test_eq(t4.img_size, 2)\n",
    "# this will fail with `Tensor` but works with `TensorBase`\n",
    "test_eq(pickle.loads(pickle.dumps(t2)).img_size, t2.img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "# test of https://github.com/pytorch/pytorch/issues/47186\n",
    "class _T(TensorBase): ...\n",
    "t = _T([1.])\n",
    "test_eq_type(t.new([1,2]), _T([1.,2.]))\n",
    "test_eq_type(t.new(), _T([]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TensorImageBase(TensorBase)```\n",
    "- add `ArrayImageBase._show_args` to ```TensorImageBase._show_args```\n",
    "- add method `show(self, ctx=None, **kwargs)` to enable ```show_image``` here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TensorImageBase(TensorBase):\n",
    "    _show_args = ArrayImageBase._show_args\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        return show_image(self, ctx=ctx, **{**self._show_args, **kwargs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TensorImage(TensorImageBase)```\n",
    "- empty subclass of ```TensorImageBase```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TensorImage(TensorImageBase): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(TEST_IMAGE)\n",
    "im_t = cast(array(im), TensorImage)\n",
    "test_eq(type(im_t), TensorImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = im_t.show(figsize=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fig_exists(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TensorImageBW(TensorImage)```\n",
    "- a subclass of ```TensorImage```\n",
    "- add ```ArrayImageBW._show_args``` to ```TensorImageBW._show_args```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TensorImageBW(TensorImage): _show_args = ArrayImageBW._show_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TensorMask(TensorImageBase)```\n",
    "- a subclass of TensorImageBase\n",
    "- use ```ArrayMask._show_args``` as its own `_show_args`\n",
    "- ```TensorMask.show``` is a wrapper around ```TensorImageBase.show``` with attr `codes` setting up the `vmin` and `vmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TensorMask(TensorImageBase):\n",
    "    _show_args = ArrayMask._show_args\n",
    "\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        codes = getattr(self, 'codes', None)\n",
    "        if codes is not None: kwargs = merge({'vmin': 0, 'vmax': len(codes)}, kwargs)\n",
    "        return super().show(ctx=ctx, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_t2 = cast(tensor(1), TensorMask)\n",
    "test_eq(type(im_t2), TensorMask)\n",
    "test_eq(im_t2, tensor(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### register many subclasses into ```TensorBase``` and ```TensorMask```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "for o in Tensor.__getitem__, Tensor.__ne__,Tensor.__eq__,Tensor.add,Tensor.sub,Tensor.mul,Tensor.div,Tensor.__rsub__,Tensor.__radd__,Tensor.matmul,Tensor.bmm:\n",
    "    TensorBase.register_func(o, TensorMask, TensorImageBase)\n",
    "    TensorBase.register_func(o, TensorImageBase, TensorMask)\n",
    "\n",
    "TensorMask.register_func(torch.einsum, str, TensorImageBase, TensorMask)\n",
    "TensorMask.register_func(torch.einsum, str, TensorMask, TensorImageBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### when ```TensorMask``` operate with ```TensorImageBase```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations between `TensorMask` and `TensorImageBase` objects return the type of the `TensorImageBase` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = TensorMask([1,2])\n",
    "test_eq_type(TensorImage(1)+a, TensorImage([2,3]))\n",
    "test_eq_type(1-a, TensorMask([0,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide (last test of to_concat)\n",
    "test_eq_type(to_concat([TensorImage([1,2]), TensorImage([3,4])]), TensorImage([1,2,3,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TensorFlowField(TensorBase)```\n",
    "- a subclass of ```TensorBase```\n",
    "- store ```TensorImageBase``` and ```TensorFlowField``` under ```TensorImage._opt[F.grid_sample]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TensorFlowField(TensorBase): pass\n",
    "TensorImage.register_func(F.grid_sample, TensorImageBase, TensorFlowField)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorImage._opt[F.grid_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = TensorImage([1.]).view(1,1,1,1)\n",
    "test_eq(t1.shape, torch.Size([1, 1, 1, 1]))\n",
    "t2 = TensorFlowField([1.,1.]).view(1,1,1,2)\n",
    "test_eq(t2.shape, torch.Size([1, 1, 1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(F.grid_sample(t1, t2), TensorImage([[[[0.2500]]]]))\n",
    "test_eq(F.grid_sample(t1, t2).shape, torch.Size([1, 1, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq_type(F.grid_sample(t1, t2), TensorImage([[[[0.25]]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorImage.register_func??\n",
    "# F.grid_sample?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TensorCategory(TensorBase)```\n",
    "- a subclass of ```TensorBase```\n",
    "- register ```TensorImageBase, TensorCategory``` under ```TensorBase._opt[Tensor.__getitem__]```\n",
    "- ```TensorMask, TensorImage``` can use ```TensorCategory``` as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "class TensorCategory(TensorBase): pass\n",
    "TensorBase.register_func(Tensor.__getitem__, TensorImageBase, TensorCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TensorCategory([1,2,3])\n",
    "mask_t = TensorMask([0,2,4,5])\n",
    "im_t = TensorImage([0,2,4,5])\n",
    "test_eq(mask_t[tc], tensor([2,4,5]))\n",
    "test_eq(im_t[tc], tensor([2,4,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TensorMultiCategory(TensorCategory)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "class TensorMultiCategory(TensorCategory): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```TitledTensorScalar(TensorBase)```\n",
    "- ```show_title``` not defined (question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class TitledTensorScalar(TensorBase):\n",
    "    \"A tensor containing a scalar that has a `show` method\"\n",
    "    def show(self, **kwargs): show_title(self.item(), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TitledTensorScalar(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```tensored(self:L)```\n",
    "- turn a L list of lists into a list of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "# @snoop\n",
    "def tensored(self:L):\n",
    "    \"`mapped(tensor)`\"\n",
    "    ### original source\n",
    "#     return self.map(tensor)\n",
    "\n",
    "    ### fastdebug source\n",
    "    sp1 = tensor\n",
    "    sp2 = self.map(sp1)\n",
    "    return sp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = L(([1,2],[3,4]))\n",
    "test_eq(t, [[1, 2],[3, 4]])\n",
    "test_eq(t.tensored(), [tensor(1,2),tensor(3,4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```stack(self:L, dim=0)```\n",
    "- run `L.tensored` to turn a L list of lists into a list of tensors\n",
    "- stack a list of tenors on top of each other (on the same cols extending rows) into a large tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "# @snoop\n",
    "def stack(self:L, dim=0):\n",
    "    \"Same as `torch.stack`\"\n",
    "    ### original source\n",
    "#     return torch.stack(list(self.tensored()), dim=dim)\n",
    "\n",
    "    ### fastdebug source\n",
    "    sp1 = self.tensored()\n",
    "    sp2 = list(sp1)\n",
    "    sp3 = torch.stack(sp2, dim=dim)\n",
    "    return sp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(t.stack(), tensor([[1,2],[3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.stack??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```cat  (self:L, dim=0)```\n",
    "- - run `L.tensored` to turn a L list of lists into a list of tensors\n",
    "- stack a list of tenors on the same rows (extend cols) into a large tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "# @snoop\n",
    "def cat  (self:L, dim=0):\n",
    "    \"Same as `torch.cat`\"\n",
    "    ### original source\n",
    "#     return torch.cat  (list(self.tensored()), dim=dim)\n",
    "    \n",
    "    ### fastdebug\n",
    "    sp1 = self.tensored()\n",
    "    sp2 = list(sp1) # convert a L into a list\n",
    "    sp3 = torch.cat(sp2, dim=dim)\n",
    "    return sp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(t.cat(), tensor([1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cat?? vs torch.stack??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(L.tensored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```concat(*ls)```\n",
    "- merge a num of lists, tuples, arrays, tensors, TensorBases into a longer ones\n",
    "- the output type is the first item's type (different types allowed)\n",
    "- if the first item is not those types above, then make it a L type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "def concat(*ls):\n",
    "    \"Concatenate tensors, arrays, lists, or tuples\"\n",
    "    ### original source\n",
    "#     if not len(ls): return []\n",
    "#     it = ls[0]\n",
    "#     if isinstance(it,torch.Tensor): res = torch.cat(ls)\n",
    "#     elif isinstance(it,ndarray): res = np.concatenate(ls)\n",
    "#     else:\n",
    "#         res = itertools.chain.from_iterable(map(L,ls))\n",
    "#         if isinstance(it,(tuple,list)): res = type(it)(res)\n",
    "#         else: res = L(res)\n",
    "#     return retain_type(res, it)\n",
    "\n",
    "    ### fastdebug source\n",
    "    if not len(ls): \n",
    "        return []\n",
    "    it = ls[0]\n",
    "    if isinstance(it,torch.Tensor): \n",
    "        res = torch.cat(ls)\n",
    "    elif isinstance(it,ndarray): \n",
    "        res = np.concatenate(ls)\n",
    "    else:\n",
    "        sp1 = map(L,ls) # make a tuple of lists into a map object\n",
    "        res = itertools.chain.from_iterable(sp1) # make a map object into a itertool chain object\n",
    "        if isinstance(it,(tuple,list)): \n",
    "            res = type(it)(res) # convert a itertool chain object into a longer/merged list\n",
    "        else: \n",
    "            res = L(res)\n",
    "    return retain_type(res, it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = [1],[1,2],[1,1,2]\n",
    "test_eq(concat(a,b), c) # concat 2 lists into a longer list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq_type(concat(tuple (a),tuple (b)), tuple (c)) # concat 2 tuples into a long tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq_type(concat(array (a),array (b)), array (c)) # concat 2 arrays into a long array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq_type(concat(tensor(a),tensor(b)), tensor(c)) # concat 2 tensors into a long tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq_type(concat(TensorBase(a),TensorBase(b)), TensorBase(c)) # concat 2 TensorBase into a long TensorBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq_type(concat([1,1],1), [1,1,1]) # concat a list and an int into a long list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq_type(concat(1,1,1), L(1,1,1)) # concat 3 int into a long L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq_type(concat(L(1,2),1), L(1,2,1)) # concat a L and an int into a long L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ```Chunks(docs)```\n",
    "- docs is a L list of lists\n",
    "- merge a list of lists into an iterable and indexable (like a long list)\n",
    "- merge a list of tensors or TensorBase(of different len) into a long list of single number tensors (iterable and indexable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Chunks.__init__(self, chunks, lens=None)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Chunks:\n",
    "    \"Slice and int indexing into a list of lists\"\n",
    "#     @snoop\n",
    "    def __init__(self, chunks, lens=None):\n",
    "        ### original source\n",
    "#         self.chunks = chunks\n",
    "#         self.lens = L(map(len,self.chunks) if lens is None else lens)\n",
    "#         self.cumlens = np.cumsum(0+self.lens)\n",
    "#         self.totlen = self.cumlens[-1]\n",
    "        \n",
    "        ### fastdebug\n",
    "        self.chunks = chunks\n",
    "#         pp.deep(lambda: L(map(len,self.chunks) if lens is None else lens))\n",
    "        self.lens = L(map(len,self.chunks) if lens is None else lens)\n",
    "#         pp.deep(lambda: np.cumsum(0+self.lens))\n",
    "        self.cumlens = np.cumsum(0+self.lens)\n",
    "        self.totlen = self.cumlens[-1]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### original example\n",
    "docs = L(list(string.ascii_lowercase[a:b]) for a,b in ((0,3),(3,7),(7,8),(8,16),(16,24),(24,26)))\n",
    "docs\n",
    "### fastdebug example\n",
    "# docs = pp.deep(lambda: L(list(string.ascii_lowercase[a:b]) for a,b in ((0,3),(3,7),(7,8),(8,16),(16,24),(24,26))))\n",
    "# docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Chunks(docs) # merge a list of lists into an iterable (like a long list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(type(b), Chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```np.searchsorted(a, v, side='left', sorter=None)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.searchsorted??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.searchsorted([1,2,3,4,5],-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.searchsorted([1,2,3,4,5], [-10, 10, 12, 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Chunks.__getitem__(self, i), getslice(self, i), doc_idx(self, i)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Chunks:\n",
    "    \"Slice and int indexing into a list of lists\"\n",
    "    def __init__(self, chunks, lens=None):\n",
    "        self.chunks = chunks\n",
    "        self.lens = L(map(len,self.chunks) if lens is None else lens)\n",
    "        self.cumlens = np.cumsum(0+self.lens)\n",
    "        self.totlen = self.cumlens[-1]\n",
    "\n",
    "#     @snoop(depth=1)\n",
    "    def __getitem__(self,i):\n",
    "        ### original source\n",
    "#         if isinstance(i,slice): return retain_type(self.getslice(i), old=self.chunks[0])\n",
    "#         di,idx = self.doc_idx(i)\n",
    "#         return retain_type(self.chunks[di][idx], old=self.chunks[0])\n",
    "    \n",
    "        ### fastdebug source\n",
    "        if isinstance(i,slice): \n",
    "            return retain_type(self.getslice(i), old=self.chunks[0])\n",
    "        di,idx = self.doc_idx(i)\n",
    "        ret = retain_type(self.chunks[di][idx], old=self.chunks[0])\n",
    "        return ret\n",
    "\n",
    "#     @snoop\n",
    "    def getslice(self, i):\n",
    "        \"get the elements of the selected lists concatenated into a longer list. Not sure about the logic of the source\"\n",
    "        ### original source\n",
    "#         st_d,st_i = self.doc_idx(ifnone(i.start,0))\n",
    "#         en_d,en_i = self.doc_idx(ifnone(i.stop,self.totlen+1))\n",
    "#         res = [self.chunks[st_d][st_i:(en_i if st_d==en_d else sys.maxsize)]]\n",
    "#         for b in range(st_d+1,en_d): res.append(self.chunks[b])\n",
    "#         if st_d!=en_d and en_d<len(self.chunks): res.append(self.chunks[en_d][:en_i])\n",
    "#         return concat(*res)\n",
    "\n",
    "        ### fastdebug source\n",
    "        st_d,st_i = self.doc_idx(ifnone(i.start,0))\n",
    "        en_d,en_i = self.doc_idx(ifnone(i.stop,self.totlen+1))\n",
    "#         pp.deep(lambda: [self.chunks[st_d][st_i:(en_i if st_d==en_d else sys.maxsize)]])\n",
    "        res = [self.chunks[st_d][st_i:(en_i if st_d==en_d else sys.maxsize)]]\n",
    "        for b in range(st_d+1,en_d): \n",
    "            res.append(self.chunks[b])\n",
    "        if st_d!=en_d and en_d<len(self.chunks): \n",
    "            res.append(self.chunks[en_d][:en_i])\n",
    "        return concat(*res)\n",
    "    \n",
    "#     @snoop\n",
    "    def doc_idx(self, i):\n",
    "        \"get index for outer and inner lists. Still not clear about the logic of the source \"\n",
    "        ### original source\n",
    "#         if i<0: i=self.totlen+i # count from end\n",
    "#         docidx = np.searchsorted(self.cumlens, i+1)-1\n",
    "#         cl = self.cumlens[docidx]\n",
    "#         return docidx,i-cl\n",
    "\n",
    "        ### fastdebug source\n",
    "        if i<0: \n",
    "            i = self.totlen+i # count from end\n",
    "#         pp.deep(lambda: np.searchsorted(self.cumlens, i+1)-1)\n",
    "        docidx = np.searchsorted(self.cumlens, i+1)-1\n",
    "#         pp.deep(lambda: self.cumlens[docidx])\n",
    "        cl = self.cumlens[docidx]\n",
    "        return docidx,i-cl     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth=1\n",
    "\n",
    "docs = L(list(string.ascii_lowercase[a:b]) for a,b in ((0,3),(3,7),(7,8),(8,16),(16,24),(24,26)))\n",
    "docs\n",
    "b = Chunks(docs)\n",
    "test_eq([b[ o] for o in range(0,1)], ['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth=2\n",
    "docs = L(list(string.ascii_lowercase[a:b]) for a,b in ((0,3),(3,7),(7,8),(8,16),(16,24),(24,26)))\n",
    "docs\n",
    "b = Chunks(docs)\n",
    "test_eq([b[ o] for o in range(0,1)], ['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq([b[ o] for o in range(0,2)], ['a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq([b[ o] for o in range(0,5)], ['a','b','c','d','e'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq([b[-o] for o in range(1,6)], ['z','y','x','w','v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(b[6:13], 'g,h,i,j,k,l,m'.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(b[20:77], 'u,v,w,x,y,z'.split(','))\n",
    "test_eq(b[:5], 'a,b,c,d,e'.split(','))\n",
    "test_eq(b[:2], 'a,b'.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(26)\n",
    "test_eq(t, tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
    "        18, 19, 20, 21, 22, 23, 24, 25]))\n",
    "docs = L(t[a:b] for a,b in ((0,3),(3,7),(7,8),(8,16),(16,24),(24,26)))\n",
    "test_eq(docs, [tensor([0, 1, 2]),tensor([3, 4, 5, 6]),tensor([7]),\\\n",
    "               tensor([ 8,  9, 10, 11, 12, 13, 14, 15]),tensor([16, 17, 18, 19, 20, 21, 22, 23]),tensor([24, 25])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Chunks(docs) # merge a list of tensors (of different len) into a long list of single number tensors (iterable and indexable)\n",
    "test_eq(list(b), list(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[b[-o] for o in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq([b[ o] for o in range(0,5)], [tensor(0), tensor(1), tensor(2), tensor(3), tensor(4)])\n",
    "test_eq([b[ o] for o in range(0,5)], range(0,5))\n",
    "test_eq([b[-o] for o in range(1,6)], [tensor(25), tensor(24), tensor(23), tensor(22), tensor(21)])\n",
    "test_eq([b[-o] for o in range(1,6)], [25,24,23,22,21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(b[6:13], torch.arange(6,13))\n",
    "test_eq(b[20:77], torch.arange(20,26))\n",
    "test_eq(b[:5], torch.arange(5))\n",
    "test_eq(b[:2], torch.arange(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = L(TensorBase(t[a:b]) for a,b in ((0,3),(3,7),(7,8),(8,16),(16,24),(24,26)))\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Chunks(docs)\n",
    "# list(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq_type(b[:2], TensorBase(range(2)))\n",
    "test_eq_type(b[:5], TensorBase(range(5)))\n",
    "test_eq_type(b[9:13], TensorBase(range(9,13)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### more demos of ```Chunks```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck = Chunks([[1,3], [2,4]])\n",
    "test_eq(ck[2], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```show_title(o, ax=None, ctx=None, label=None, color='black', **kwargs)```\n",
    "- why do we need `show_title`?\n",
    "- add title to pd.series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "def show_title(o, ax=None, ctx=None, label=None, color='black', **kwargs):\n",
    "    \"Set title of `ax` to `o`, or print `o` if `ax` is `None`\"\n",
    "    ax = ifnone(ax,ctx)\n",
    "    if ax is None: print(o)\n",
    "    elif hasattr(ax, 'set_title'):\n",
    "        t = ax.title.get_text()\n",
    "        if len(t) > 0: \n",
    "            o = t+'\\n'+str(o)\n",
    "        ax.set_title(o, color=color)\n",
    "    elif isinstance(ax, pd.Series):\n",
    "        while label in ax: \n",
    "            label += '_'\n",
    "        ax = pd.concat([ax,pd.Series({label: o})])\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_title(\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stdout(lambda: show_title(\"title\"), \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dict(a=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_title(\"title\", ctx=pd.Series(dict(a=1)), label='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dict(a=1,a_='title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that col names are unique when showing to a pandas series\n",
    "assert show_title(\"title\", ctx=pd.Series(dict(a=1)), label='a').equals(pd.Series(dict(a=1,a_='title')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```ShowTitle as superclass to TitledInt, TitledFloat, TitledStr, TitledTuple```\n",
    "- why having all these classes?\n",
    "- to give int, float, str, tuple a title and print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ShowTitle:\n",
    "    \"Base class that adds a simple `show`\"\n",
    "    _show_args = {'label': 'text'}\n",
    "    @delegates(show_title, keep=True)    \n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        \"Show self\"\n",
    "        return show_title(str(self), ctx=ctx, **merge(self._show_args, kwargs))\n",
    "\n",
    "class TitledInt(Int, ShowTitle):\n",
    "    _show_args = {'label': 'text'}\n",
    "    @delegates(show_title, keep=True)    \n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        \"Show self\"\n",
    "        return show_title(str(self), ctx=ctx, **merge(self._show_args, kwargs))\n",
    "\n",
    "class TitledFloat(Float, ShowTitle):\n",
    "    _show_args = {'label': 'text'}\n",
    "    @delegates(show_title, keep=True)\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        \"Show self\"\n",
    "        return show_title(str(self), ctx=ctx, **merge(self._show_args, kwargs))\n",
    "\n",
    "class TitledStr(Str, ShowTitle):\n",
    "    _show_args = {'label': 'text'}\n",
    "    @delegates(show_title, keep=True)    \n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        \"Show self\"\n",
    "        return show_title(str(self), ctx=ctx, **merge(self._show_args, kwargs))\n",
    "\n",
    "class TitledTuple(fastuple, ShowTitle):\n",
    "    _show_args = {'label': 'text'}\n",
    "    @delegates(show_title, keep=True)    \n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        \"Show self\"\n",
    "        return show_title(str(self), ctx=ctx, **merge(self._show_args, kwargs))\n",
    "\n",
    "add_docs(TitledInt, \"An `int` with `show`\"); add_docs(TitledStr, \"An `str` with `show`\");\n",
    "add_docs(TitledFloat, \"A `float` with `show`\"); add_docs(TitledTuple, \"A `fastuple` with `show`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stdout(lambda: TitledStr('s').show(), 's')\n",
    "test_stdout(lambda: TitledInt(1).show(), '1')\n",
    "test_stdout(lambda: TitledFloat(1.).show(), '1.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "df = pd.DataFrame(index = range(2,5))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TitledStr('s').show()\n",
    "TitledStr('s').show(\"title\")\n",
    "TitledInt(1).show()\n",
    "TitledFloat(1.).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row1 = df.iloc[0]\n",
    "row2 = df.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = TitledFloat(2.56)\n",
    "x2 = TitledInt(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TitledFloat(1.)\n",
    "TitledFloat(1.).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(TitledTuple, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1show = x1.show(ctx=row1, label='lbl')\n",
    "x2show = x2.show(ctx=row2, label='lbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1show\n",
    "x2show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1show, type(x1show), x1show.lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```truncate(self:TitledStr, n)```\n",
    "- why need ```TitledStr.truncate(n)```?\n",
    "- to cut the first n words from the string and print them out together as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def truncate(self:TitledStr, n):\n",
    "    \"Truncate self to `n`\"\n",
    "    words = self.split(' ')[:n]\n",
    "    return TitledStr(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TitledStr(\"this is me\")\n",
    "TitledStr(\"this is me\").truncate(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "if not hasattr(pd.DataFrame,'_old_init'): pd.DataFrame._old_init = pd.DataFrame.__init__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```pd.DataFrame.__init__(self:pd.DataFrame, data=None, index=None, columns=None, dtype=None, copy=None)```\n",
    "- why do we need a new ```__init__``` for ```pd.DataFrame```? because\n",
    "- 1. to first turn a tensor to a np.ndarray first using ```to_np```\n",
    "- 2. and then use original ```pd.DataFrame.__init__``` above to turn array into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def __init__(self:pd.DataFrame, data=None, index=None, columns=None, dtype=None, copy=None):\n",
    "    if data is not None and isinstance(data, Tensor): data = to_np(data)\n",
    "    self._old_init(data, index=index, columns=columns, dtype=dtype, copy=copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([1,2])\n",
    "to_np(tensor(1,2))\n",
    "pd.DataFrame(tensor(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```get_empty_df(n)```\n",
    "- Return `n` empty rows of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_empty_df(n):\n",
    "    \"Return `n` empty rows of a dataframe\"\n",
    "    df = pd.DataFrame(index = range(n))\n",
    "    return [df.iloc[i] for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_empty_df(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```display_df(df)```\n",
    "- print out in HTML of a dataframe or just print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def display_df(df):\n",
    "    \"Display `df` in a notebook or defaults to print\"\n",
    "    try: from IPython.display import display, HTML\n",
    "    except: return print(df)\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_df(pd.DataFrame(tensor(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```get_first(c)```\n",
    "- Get the first element of c (anything listy), even if c is a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_first(c):\n",
    "    \"Get the first element of c, even if c is a dataframe\"\n",
    "    return getattr(c, 'iloc', c)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_first([1,2,3])\n",
    "get_first((1,2,3))\n",
    "get_first(range(5))\n",
    "get_first(array([1,2,3]))\n",
    "get_first(tensor([1,2,3]))\n",
    "get_first(pd.DataFrame(tensor(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```one_param(m)```\n",
    "- get the first parameter of a model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def one_param(m):\n",
    "    \"First parameter in `m`\"\n",
    "    return first(m.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```item_find(x, idx=0)```\n",
    "- Recursively takes the `idx`-th element of `x`, and take the 1st element of that\n",
    "- and `x` can be a list of lists or dict of dict or list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "def item_find(x, idx=0):\n",
    "    \"Recursively takes the `idx`-th element of `x`\"\n",
    "    if is_listy(x): return item_find(x[idx])\n",
    "#     if is_listy(x): return item_find(x[idx], idx) # another way of interpreting recursively take the idx-th element   \n",
    "    if isinstance(x,dict):\n",
    "        key = list(x.keys())[idx] if isinstance(idx, int) else idx\n",
    "        return item_find(x[key])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_find({'a':[1,2,9], 'b':[3,4,8], 'c':[5,6,7]}, idx=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_find([[1,2,9], [3,4,8],[5,6,7]], idx=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```find_device(b)```\n",
    "Recursively search the device of `b`. the first of the first device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop(depth=2)\n",
    "def find_device(b):\n",
    "    \"Recursively search the device of `b`. the first of the first device\"\n",
    "    return item_find(b).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = to_device(tensor(1))\n",
    "t2 = to_device(tensor(0))\n",
    "dev = default_device()\n",
    "t1\n",
    "t2\n",
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_device(t2)\n",
    "find_device([t1,t2])\n",
    "find_device({'a':t1,'b':t2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```finds_bs(b)```\n",
    "- Recursively search the batch size of `b`.\n",
    "- b can be a list (single array) or a 2d matrix or a tuple of 2 matricies, or a dict of many matricies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "def find_bs(b):\n",
    "    \"Recursively search the batch size of `b`.\"\n",
    "    res = item_find(b)\n",
    "    if not hasattr(res, \"shape\"): return len(b)\n",
    "    return res.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [1,2,3]\n",
    "test_eq(find_bs(x1), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = array([1,2,3])\n",
    "test_eq(find_bs(x2), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4,5)\n",
    "test_eq(find_bs(x), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(find_bs((x,x)), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(find_bs([x, x]), 4)\n",
    "test_eq(find_bs({'a':x,'b':x}), 4)\n",
    "test_eq(find_bs({'a':[[x],[x]],'b':x}), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```np_func(f)```\n",
    "- Convert a function taking and returning numpy arrays to one taking and returning tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def np_func(f):\n",
    "    \"Convert a function taking and returning numpy arrays to one taking and returning tensors\"\n",
    "    def _inner(*args, **kwargs):\n",
    "        nargs = [to_np(arg) if isinstance(arg,Tensor) else arg for arg in args]\n",
    "        return tensor(f(*nargs, **kwargs))\n",
    "    functools.update_wrapper(_inner, f)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This decorator is particularly useful for using numpy functions as fastai metrics, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@np_func\n",
    "def f1(inp,targ): return f1_score(targ, inp)\n",
    "\n",
    "a1,a2 = array([0,1,1]),array([1,0,1])\n",
    "t = f1(tensor(a1),tensor(a2))\n",
    "test_eq(f1_score(a1,a2), t)\n",
    "assert isinstance(t,Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Module(nn.Module, metaclass=PrePostInitMeta)```\n",
    "- Same as `nn.Module`, but no need for subclasses to call `super().__init__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Module(nn.Module, metaclass=PrePostInitMeta):\n",
    "    \"Same as `nn.Module`, but no need for subclasses to call `super().__init__`\"\n",
    "    def __pre_init__(self, *args, **kwargs): super().__init__()\n",
    "    def __init__(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(Module, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _T(Module):\n",
    "    def __init__(self): self.f = nn.Linear(1,1)\n",
    "    def forward(self,x): return self.f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = _T()\n",
    "t(tensor([1.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```get_model(model)```\n",
    "Return the model even when the model is wrapped inside `model` through `model.module` and `model` is an instance of ```DistributedDataParallel, nn.DataParallel```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from torch.nn.parallel import DistributedDataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_model(model):\n",
    "    \"Return the model maybe wrapped inside `model`.\"\n",
    "    return model.module if isinstance(model, (DistributedDataParallel, nn.DataParallel)) else model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```one_hot(x, c)```\n",
    "- why need one_hot(x, c)? \n",
    "- one_hot(3, 5) can give us the 3rd class out of 5 classes in one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def one_hot(x, c):\n",
    "    \"One-hot encode `x` with `c` classes.\"\n",
    "    res = torch.zeros(c, dtype=torch.uint8)\n",
    "    if isinstance(x, Tensor) and x.numel()>0: \n",
    "        res[x] = 1.\n",
    "    else: res[list(L(x, use_list=None))] = 1.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(L)\n",
    "# L([1,2], use_list=False)\n",
    "# L((1,2), use_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor(3).numel()\n",
    "tensor(2,3).numel() # number of elements in the tensor?\n",
    "tensor([[2,3],[3,4]]).numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot([1,2], 5)\n",
    "one_hot([1,4,3,0,0,0], 6)\n",
    "one_hot([0], 5)\n",
    "one_hot(tensor(2,3,4), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor(0,1,0,0,1).byte()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(one_hot([1,4], 5), tensor(0,1,0,0,1).byte())\n",
    "test_eq(one_hot(torch.tensor([]), 5), tensor(0,0,0,0,0).byte())\n",
    "test_eq(one_hot(2, 5), tensor(0,0,1,0,0).byte())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```one_hot_decode(x, vocab=None)```\n",
    "- reverse from one_hot to classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def one_hot_decode(x, vocab=None):\n",
    "    return L(vocab[i] if vocab else i for i,x_ in enumerate(x) if x_==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(one_hot_decode(tensor(0,1,0,0,1)), [1,4])\n",
    "test_eq(one_hot_decode(tensor(0,0,0,0,0)), [   ])\n",
    "test_eq(one_hot_decode(tensor(0,0,1,0,0)), [2  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_decode(tensor(0,1,0,0,1), vocab=['a', 'b', 'c', 'd', 'e'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```params(m)```\n",
    "- return a list of parameters in a model `m`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def params(m):\n",
    "    \"Return all parameters of `m`\"\n",
    "    return [p for p in m.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```trainable_params(m)```\n",
    "- Return all trainable parameters of `m` into a list, meaning their `requires_grad` is True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def trainable_params(m):\n",
    "    \"Return all trainable parameters of `m`\"\n",
    "    return [p for p in m.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Linear(20, 30)\n",
    "input = torch.randn(128, 20)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Linear(4,5)\n",
    "test_eq(trainable_params(m), [m.weight, m.bias])\n",
    "m.weight.requires_grad_(False)\n",
    "test_eq(trainable_params(m), [m.bias])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```norm_types``` \n",
    "- have all normalization functions stored inside a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "norm_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d, nn.LayerNorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```norm_bias_params(m, with_bias=True)```\n",
    "- Return all bias and BatchNorm parameters, not other weights parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def norm_bias_params(m, with_bias=True):\n",
    "    \"Return all bias and BatchNorm parameters\"\n",
    "    if isinstance(m, norm_types): return L(m.parameters())\n",
    "    res = L(m.children()).map(norm_bias_params, with_bias=with_bias).concat()\n",
    "    if with_bias and getattr(m, 'bias', None) is not None: res.append(m.bias)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Linear(10, 20)\n",
    "m.weight.shape\n",
    "m.bias.shape\n",
    "# m.weight.grad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(10,20), nn.BatchNorm1d(20), nn.Conv1d(3,4, 3))\n",
    "model\n",
    "# model[0].weight.shape\n",
    "model[0].bias.shape\n",
    "model[1].weight.shape\n",
    "model[1].bias.shape\n",
    "# model[2].weight.shape\n",
    "model[2].bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.shape for i in norm_bias_params(model)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for norm_func in [nn.BatchNorm1d, partial(nn.InstanceNorm1d, affine=True)]:\n",
    "    model = nn.Sequential(nn.Linear(10,20), norm_func(20), nn.Conv1d(3,4, 3))\n",
    "    test_eq(norm_bias_params(model), [model[0].bias, model[1].weight, model[1].bias, model[2].bias])\n",
    "    model = nn.ModuleList([nn.Linear(10,20, bias=False), nn.Sequential(norm_func(20), nn.Conv1d(3,4,3))])\n",
    "    test_eq(norm_bias_params(model), [model[1][0].weight, model[1][0].bias, model[1][1].bias])\n",
    "    model = nn.ModuleList([nn.Linear(10,20), nn.Sequential(norm_func(20), nn.Conv1d(3,4,3))])\n",
    "    test_eq(norm_bias_params(model, with_bias=False), [model[1][0].weight, model[1][0].bias])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```batch_to_samples(b, max_n=10)```\n",
    "- 'Transposes' a batch to (at most `max_n`) samples\n",
    "- it's like to slice a batch of data by n cols for a smaller sample data, and transpose it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "def batch_to_samples(b, max_n=10):\n",
    "    \"'Transposes' a batch to (at most `max_n`) samples\"\n",
    "    if isinstance(b, Tensor): \n",
    "        return retain_types(list(b[:max_n]), [b])\n",
    "    else:\n",
    "#         pp.deep(lambda: L(b).map(partial(batch_to_samples,max_n=max_n)))\n",
    "        res = L(b).map(partial(batch_to_samples,max_n=max_n))\n",
    "        return retain_types(res.zip(), [b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_to_samples(tensor(1,2,3), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tensor([1,2,3])\n",
    "[t,t+1]\n",
    "batch_to_samples([t,t+1], max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_to_samples([tensor([1,2,3]), tensor([4,5,6])], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(batch_to_samples([t,t+1], max_n=2), ([1,2],[2,3]))\n",
    "test_eq(batch_to_samples(tensor([1,2,3]), 10), [1, 2, 3])\n",
    "test_eq(batch_to_samples([tensor([1,2,3]), tensor([4,5,6])], 10), [(1, 4), (2, 5), (3, 6)])\n",
    "test_eq(batch_to_samples([tensor([1,2,3]), tensor([4,5,6])], 2), [(1, 4), (2, 5)])\n",
    "test_eq(batch_to_samples([tensor([1,2,3]), [tensor([4,5,6]),tensor([7,8,9])]], 10), \n",
    "        [(1, (4, 7)), (2, (5, 8)), (3, (6, 9))])\n",
    "test_eq(batch_to_samples([tensor([1,2,3]), [tensor([4,5,6]),tensor([7,8,9])]], 2), [(1, (4, 7)), (2, (5, 8))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastnbs(\"fastuple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = fastuple(tensor([1,2,3]),TensorBase([2,3,4]))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq_type(batch_to_samples(t)[0][1], TensorBase(2))\n",
    "test_eq(batch_to_samples(t).map(type), [fastuple]*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```interp_1d(x:Tensor, xp, fp)```\n",
    "- Same as `np.interp`, run `np.interp??`\n",
    "- a quick [video](https://youtu.be/nGwg5MrbZxo?t=82) about what 1d linear interpolation mean and for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def interp_1d(x:Tensor, xp, fp):\n",
    "    \"Same as `np.interp`\"\n",
    "    slopes = (fp[1:]-fp[:-1])/(xp[1:]-xp[:-1])\n",
    "    incx = fp[:-1] - (slopes*xp[:-1])\n",
    "    locs = (x[:,None]>=xp[None,:]).long().sum(1)-1\n",
    "    locs = locs.clamp(0,len(slopes)-1)\n",
    "    return slopes[locs]*x + incx[locs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%snoop\n",
    "brks = tensor(0,1,2,4,8,64).float()\n",
    "ys = tensor(range_of(brks)).float()\n",
    "ys /= ys[-1].item()\n",
    "pts = tensor(0.2,0.5,0.8,3,5,63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%snoop\n",
    "preds = pts.interp_1d(brks, ys)\n",
    "preds\n",
    "test_close(preds.numpy(), np.interp(pts.numpy(), brks.numpy(), ys.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(brks,ys)\n",
    "plt.scatter(pts,preds)\n",
    "plt.legend(['breaks','preds']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```pca(x:Tensor, k=2)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def pca(x:Tensor, k=2):\n",
    "    \"Compute PCA of `x` with `k` dimensions.\"\n",
    "    x = x-torch.mean(x,0)\n",
    "    U,S,V = torch.svd(x.t())\n",
    "    return torch.mm(x,U[:,:k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```logit(x)```\n",
    "- Logit of `x`, clamped to avoid inf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def logit(x):\n",
    "    \"Logit of `x`, clamped to avoid inf.\"\n",
    "    x = x.clamp(1e-7, 1-1e-7)\n",
    "    return -(1/x-1).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit(tensor(1e-18))\n",
    "logit(tensor(1-1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```num_distrib()```\n",
    "- Return the number of processes in distributed training (if applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def num_distrib():\n",
    "    \"Return the number of processes in distributed training (if applicable).\"\n",
    "    return int(os.environ.get('WORLD_SIZE', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_distrib()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```rank_distrib()```\n",
    "Return the distributed rank of this process (if applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def rank_distrib():\n",
    "    \"Return the distributed rank of this process (if applicable).\"\n",
    "    return int(os.environ.get('RANK', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_distrib()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```distrib_barrier()```\n",
    "Place a synchronization barrier in distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def distrib_barrier():\n",
    "    \"Place a synchronization barrier in distributed training\"\n",
    "    if num_distrib() > 1 and torch.distributed.is_initialized(): torch.distributed.barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calling this, ALL sub-processes in the pytorch process group must arrive here before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Saving arrays requires pytables - optional dependency\n",
    "try: import tables\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _comp_filter(lib='lz4',lvl=3): return tables.Filters(complib=f'blosc:{lib}', complevel=lvl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```save_array(p:Path, o, complib='lz4', lvl=3)```\n",
    "- turn a tensor into an array\n",
    "- save/write the array in a pytable file under the Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def save_array(p:Path, o, complib='lz4', lvl=3):\n",
    "    \"Save numpy array to a compressed `pytables` file, using compression level `lvl`\"\n",
    "    if isinstance(o,Tensor): o = to_np(o)\n",
    "    with tables.open_file(p, mode='w', filters=_comp_filter(lib=complib,lvl=lvl)) as f: f.create_carray('/', 'data', obj=o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compression lib can be any of: blosclz, lz4, lz4hc, snappy, zlib or zstd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```load_array(p:Path)```\n",
    "- add a method ```load_array``` to Path\n",
    "- to open and read from a pytable file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@patch\n",
    "def load_array(p:Path):\n",
    "    \"Save numpy array to a `pytables` file\"\n",
    "    with tables.open_file(p, 'r') as f: return f.root.data.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```base_doc(elt), doc(elt)```\n",
    "- how to use ```doc``` and what goes on behind the scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def base_doc(elt):\n",
    "    \"Print a base documentation of `elt`\"\n",
    "    name = getattr(elt, '__qualname__', getattr(elt, '__name__', ''))\n",
    "    print(f'{name}{inspect.signature(elt)}\\n{inspect.getdoc(elt)}\\n')\n",
    "    print('To get a prettier result with hyperlinks to source code and documentation, install nbdev: pip install nbdev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def doc(elt):\n",
    "    \"Try to use doc form nbdev and fall back to `base_doc`\"\n",
    "    try:\n",
    "        from nbdev.showdoc import doc\n",
    "        doc(elt)\n",
    "    except: base_doc(elt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```nested_reorder(t, idxs)```\n",
    "- Reorder all tensors in `t` using `idxs`\n",
    "- all the tensors reordered will be put back to list or tuple where they were from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def nested_reorder(t, idxs):\n",
    "    \"Reorder all tensors in `t` using `idxs`\"\n",
    "    if isinstance(t, (Tensor,L)): return t[idxs]\n",
    "    elif is_listy(t): return type(t)(nested_reorder(t_, idxs) for t_ in t)\n",
    "    if t is None: return t\n",
    "    raise TypeError(f\"Expected tensor, tuple, list or L but got {type(t)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tensor([0,1,2,3,4,5])\n",
    "idxs = tensor([2,5,1,0,3,4])\n",
    "nested_reorder(x, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_reorder([[x], x], idxs)\n",
    "test_eq_type(nested_reorder(([x], x), idxs), ([idxs], idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = L(0,1,2,3,4,5)\n",
    "# pp.deep(lambda: L(i.item() for i in idxs))\n",
    "z = L(i.item() for i in idxs)\n",
    "nested_reorder((y, x), idxs)\n",
    "test_eq_type(nested_reorder((y, x), idxs), (z,idxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```flatten_check(inp, targ)```\n",
    "- Check that `inp` and `targ` have the same number of elements and flatten them into a single row or 1d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def flatten_check(inp, targ):\n",
    "    \"Check that `inp` and `targ` have the same number of elements and flatten them.\"\n",
    "    inp,targ = TensorBase(inp.contiguous()).view(-1),TensorBase(targ.contiguous()).view(-1)\n",
    "    test_eq(len(inp), len(targ))\n",
    "    return inp,targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = torch.randn(5,4),torch.randn(20)\n",
    "x1,x2 = flatten_check(x1,x2)\n",
    "test_eq(x1.shape, [20])\n",
    "test_eq(x2.shape, [20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = torch.randn(5,4),torch.randn(21)\n",
    "test_fail(lambda: flatten_check(x1,x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```make_cross_image(bw=True)```\n",
    "Create a tensor containing a cross image, either `bw` (True) or color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def make_cross_image(bw=True):\n",
    "    \"Create a tensor containing a cross image, either `bw` (True) or color\"\n",
    "    if bw:\n",
    "        im = torch.zeros(5,5)\n",
    "        im[2,:] = 1.\n",
    "        im[:,2] = 1.\n",
    "    else:\n",
    "        im = torch.zeros(3,5,5)\n",
    "        im[0,2,:] = 1.\n",
    "        im[1,:,2] = 1.\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(make_cross_image()) # imshow's cmap default is with color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(make_cross_image(), cmap=\"Greys\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(make_cross_image(False).permute(1,2,0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```show_image_batch(b, show=show_titled_image, items=9, cols=3, figsize=None, **kwargs)```\n",
    "- Display batch `b` in a grid of size `items` with `cols` width\n",
    "- images with titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# @snoop\n",
    "def show_image_batch(b, show=show_titled_image, items=9, cols=3, figsize=None, **kwargs):\n",
    "    \"Display batch `b` in a grid of size `items` with `cols` width\"\n",
    "    if items<cols: cols=items\n",
    "    rows = (items+cols-1) // cols\n",
    "    if figsize is None: figsize = (cols*3, rows*3)\n",
    "    fig,axs = plt.subplots(rows, cols, figsize=figsize)\n",
    "    for *o,ax in zip(*to_cpu(b), axs.flatten()): \n",
    "        show(o, ax=ax, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastnbs(\"show_titled_image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_cpu(([Image.open(TEST_IMAGE_BW),Image.open(TEST_IMAGE)],['bw','color']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_batch(([Image.open(TEST_IMAGE_BW),Image.open(TEST_IMAGE)],['bw','color']), items=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```requires_grad(m)```\n",
    "\n",
    "- Check if the first parameter of `m` requires grad or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def requires_grad(m):\n",
    "    \"Check if the first parameter of `m` requires grad or not\"\n",
    "    ps = list(m.parameters())\n",
    "    return ps[0].requires_grad if len(ps)>0 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = nn.Linear(4,5)\n",
    "requires_grad(tst)\n",
    "assert requires_grad(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastnbs(\"requires_grad_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in tst.parameters(): p.requires_grad_(False)\n",
    "assert not requires_grad(tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```init_default(m, func=nn.init.kaiming_normal_)```\n",
    "- Initialize `m` weights with `func` and set `bias` to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def init_default(m, func=nn.init.kaiming_normal_):\n",
    "    \"Initialize `m` weights with `func` and set `bias` to 0.\"\n",
    "    if func:\n",
    "        if hasattr(m, 'weight'): func(m.weight)\n",
    "        if hasattr(m, 'bias') and hasattr(m.bias, 'data'): m.bias.data.fill_(0.)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = nn.Linear(4,5)\n",
    "tst.weight.data\n",
    "tst.weight.data.uniform_(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst.bias.data\n",
    "tst.bias.data.uniform_(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = init_default(tst, func = lambda x: x.data.fill_(1.))\n",
    "tst.weight.data\n",
    "tst.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(tst.weight, torch.ones(5,4))\n",
    "test_eq(tst.bias, torch.zeros(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```cond_init(m, func)```\n",
    "- Apply `init_default` to `m` as long as it is not a batchnorm module and first parameters require grads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def cond_init(m, func):\n",
    "    \"Apply `init_default` to `m` unless it's a batchnorm module\"\n",
    "    if (not isinstance(m, norm_types)) and requires_grad(m): init_default(m, func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_types\n",
    "# requires_grad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = nn.Linear(4,5)\n",
    "tst.weight.data.uniform_(-1,1)\n",
    "tst.bias.data.uniform_(-1,1)\n",
    "cond_init(tst, func = lambda x: x.data.fill_(1.))\n",
    "test_eq(tst.weight, torch.ones(5,4))\n",
    "test_eq(tst.bias, torch.zeros(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = nn.BatchNorm2d(5)\n",
    "init = [tst.weight.clone(), tst.bias.clone()]\n",
    "cond_init(tst, func = lambda x: x.data.fill_(1.))\n",
    "test_eq(tst.weight, init[0])\n",
    "test_eq(tst.bias, init[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```apply_leaf(m, f)```\n",
    "- Apply `f` to children of `m` (every layers of a model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def apply_leaf(m, f):\n",
    "    \"Apply `f` to children of `m`.\"\n",
    "    c = m.children()\n",
    "    if isinstance(m, nn.Module): f(m)\n",
    "    for l in c: apply_leaf(l,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = nn.Sequential(nn.Linear(4,5), nn.Sequential(nn.Linear(4,5), nn.Linear(4,5)))\n",
    "tst[0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tst.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_leaf(tst, partial(init_default, func=lambda x: x.data.fill_(1.)))\n",
    "for l in [tst[0], *tst[1]]: \n",
    "    print(f\"l.weight.shape: {l.weight.shape}\")\n",
    "    test_eq(l.weight, torch.ones(5,4))\n",
    "for l in [tst[0], *tst[1]]: test_eq(l.bias,   torch.zeros(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```apply_init(m, func=nn.init.kaiming_normal_)```\n",
    "- Initialize all non-batchnorm layers of `m` with `func`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def apply_init(m, func=nn.init.kaiming_normal_):\n",
    "    \"Initialize all non-batchnorm layers of `m` with `func`.\"\n",
    "    apply_leaf(m, partial(cond_init, func=func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = nn.Sequential(nn.Linear(4,5), nn.Sequential(nn.Linear(4,5), nn.BatchNorm1d(5)))\n",
    "init = [tst[1][1].weight.clone(), tst[1][1].bias.clone()]\n",
    "apply_init(tst, func=lambda x: x.data.fill_(1.))\n",
    "for l in [tst[0], tst[1][0]]: test_eq(l.weight, torch.ones(5,4))\n",
    "for l in [tst[0], tst[1][0]]: test_eq(l.bias,   torch.zeros(5))\n",
    "test_eq(tst[1][1].weight, init[0])\n",
    "test_eq(tst[1][1].bias,   init[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autograd jit functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```script_use_ctx(f), script_save_ctx(static, *argidx), script_fwd(*argidx), script_bwd(f), grad_module(cls)```\n",
    "- funcs to automate autograd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def script_use_ctx(f):\n",
    "    \"Decorator: create jit script and pass everything in `ctx.saved_variables to `f`, after `*args`\"\n",
    "    sf = torch.jit.script(f)\n",
    "    def _f(ctx, *args, **kwargs): return sf(*args, *ctx.saved_variables, **kwargs)\n",
    "    return update_wrapper(_f,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def script_save_ctx(static, *argidx):\n",
    "    \"Decorator: create jit script and save args with indices `argidx` using `ctx.save_for_backward`\"\n",
    "    def _dec(f):\n",
    "        sf = torch.jit.script(f)\n",
    "        def _f(ctx, *args, **kwargs):\n",
    "            if argidx:\n",
    "                save = [args[o] for o in argidx]\n",
    "                ctx.save_for_backward(*save)\n",
    "            if not argidx: args = [ctx]+args\n",
    "            return sf(*args, **kwargs)\n",
    "        if static: _f = staticmethod(_f)\n",
    "        return update_wrapper(_f,f)\n",
    "    return _dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def script_fwd(*argidx):\n",
    "    \"Decorator: create static jit script and save args with indices `argidx` using `ctx.save_for_backward`\"\n",
    "    return script_save_ctx(True, *argidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def script_bwd(f):\n",
    "    \"Decorator: create static jit script and pass everything in `ctx.saved_variables to `f`, after `*args`\"\n",
    "    return staticmethod(script_use_ctx(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def grad_module(cls):\n",
    "    \"Decorator: convert `cls` into an autograd function\"\n",
    "    class _c(nn.Module):\n",
    "        def forward(self, *args, **kwargs): return cls.apply(*args, **kwargs)\n",
    "    return _c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Version Checks -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```ismin_torch(min_version), notmax_torch(max_version)```\n",
    "- check whether the torch version greater than the minimum version; check whether is less than the maximum version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def ismin_torch(min_version):\n",
    "    \"Check if `torch.__version__` >= `min_version` using packaging.version\"\n",
    "    return parse(torch.__version__) >= parse(min_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def notmax_torch(max_version):\n",
    "    \"Check if `torch.__version__` < `max_version` using packaging.version\"\n",
    "    return parse(torch.__version__) < parse(max_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
