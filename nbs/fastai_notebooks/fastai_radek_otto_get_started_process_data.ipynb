{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "934b96d0",
   "metadata": {},
   "source": [
    "# process_data_OTTO\n",
    "\n",
    "[Original process_data.ipynb](https://www.kaggle.com/competitions/otto-recommender-system/discussion/363843) is given by @radek1 to make the original OTTO dataset easier to process and faster to access.\n",
    "\n",
    "This notebook is to experiment in order to understand what the original code does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac59de",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4171c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure fastkaggle is install and imported\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11795920",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    }
   ],
   "source": [
    "try: import fastkaggle\n",
    "except ModuleNotFoundError:\n",
    "    os.system(\"pip install -Uq fastkaggle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29abe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastkaggle import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad6aad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nbdev\n",
      "  Downloading nbdev-2.3.9-py3-none-any.whl (64 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.1/64.1 kB 492.2 kB/s eta 0:00:00\n",
      "Collecting snoop\n",
      "  Downloading snoop-0.4.2-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: fastcore>=1.5.27 in /opt/conda/lib/python3.7/site-packages (from nbdev) (1.5.27)\n",
      "Collecting asttokens\n",
      "  Downloading asttokens-2.1.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from nbdev) (6.0)\n",
      "Requirement already satisfied: astunparse in /opt/conda/lib/python3.7/site-packages (from nbdev) (1.6.3)\n",
      "Collecting ghapi>=1.0.3\n",
      "  Downloading ghapi-1.0.3-py3-none-any.whl (58 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.1/58.1 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting execnb>=0.1.4\n",
      "  Downloading execnb-0.1.4-py3-none-any.whl (13 kB)\n",
      "Collecting watchdog\n",
      "  Downloading watchdog-2.1.9-py3-none-manylinux2014_x86_64.whl (78 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.4/78.4 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting cheap-repr>=0.4.0\n",
      "  Downloading cheap_repr-0.5.1-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from snoop) (1.15.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from snoop) (2.12.0)\n",
      "Collecting executing\n",
      "  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from execnb>=0.1.4->nbdev) (7.33.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from fastcore>=1.5.27->nbdev) (21.3)\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (from fastcore>=1.5.27->nbdev) (22.1.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse->nbdev) (0.37.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->execnb>=0.1.4->nbdev) (5.3.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython->execnb>=0.1.4->nbdev) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->execnb>=0.1.4->nbdev) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->execnb>=0.1.4->nbdev) (3.0.30)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython->execnb>=0.1.4->nbdev) (0.18.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->execnb>=0.1.4->nbdev) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->execnb>=0.1.4->nbdev) (5.1.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython->execnb>=0.1.4->nbdev) (59.8.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython->execnb>=0.1.4->nbdev) (0.1.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->fastcore>=1.5.27->nbdev) (3.0.9)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython->execnb>=0.1.4->nbdev) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython->execnb>=0.1.4->nbdev) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->execnb>=0.1.4->nbdev) (0.2.5)\n",
      "Installing collected packages: executing, cheap-repr, watchdog, asttokens, snoop, ghapi, execnb, nbdev\n",
      "Successfully installed asttokens-2.1.0 cheap-repr-0.5.1 execnb-0.1.4 executing-1.2.0 ghapi-1.0.3 nbdev-2.3.9 snoop-0.4.2 watchdog-2.1.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    }
   ],
   "source": [
    "# use fastdebug.utils \n",
    "if iskaggle: os.system(\"pip install nbdev snoop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3431ac28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if iskaggle:\n",
    "    path = \"../input/fastdebugutils0\"\n",
    "    import sys\n",
    "    sys.path\n",
    "    sys.path.insert(1, path)\n",
    "    import utils as fu\n",
    "    from utils import *\n",
    "else: \n",
    "    from fastdebug.utils import *\n",
    "    import fastdebug.utils as fu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c31abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = \"otto-recommender-system\"\n",
    "path = setup_comp(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85ac3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('../input/otto-recommender-system/sample_submission.csv'),\n",
       " Path('../input/otto-recommender-system/test.jsonl'),\n",
       " Path('../input/otto-recommender-system/train.jsonl')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(path.ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4647619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('../input/otto-recommender-system/train.jsonl')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Path('../input/otto-recommender-system/test.jsonl')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = path.ls()[2]\n",
    "test_path = path.ls()[1]\n",
    "train_path\n",
    "test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c581dd4d",
   "metadata": {},
   "source": [
    "### jn: revisit process_data notebook and change get started to process_data in title. /2022-11-14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273e256",
   "metadata": {},
   "source": [
    "## The original code start from below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83011ce7",
   "metadata": {},
   "source": [
    "### rd: recsys - otto - process data - save a list or dict into pkl and load them - id2type = ['clicks', 'carts', 'orders'] - type2id = {a: i for i, a in enumerate(id2type)} - pd.to_pickle(id2type, 'id2type.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d8344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27220d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['clicks', 'carts', 'orders'], {'clicks': 0, 'carts': 1, 'orders': 2})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2type = ['clicks', 'carts', 'orders'] # I have analyzed the data\n",
    "                                          # and so I know we can expect these event types\n",
    "type2id = {a: i for i, a in enumerate(id2type)}\n",
    "\n",
    "id2type, type2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(id2type, 'id2type.pkl')\n",
    "pd.to_pickle(type2id, 'type2id.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2624fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402773df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pickle5\n",
    "\n",
    "import pickle5 as pickle\n",
    "\n",
    "with open('../input/otto-full-optimized-memory-footprint/id2type.pkl', \"rb\") as fh:\n",
    "    id2type = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7ecc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5ad3dd8",
   "metadata": {},
   "source": [
    "### rd: recsys - otto - process data - how to process jsonl file to df (pd.read_json, chunk.iterrows) - chunks = pd.read_json(fn, lines=True, chunksize=2) - for chunk in chunks: - for row_idx, session_data in chunk.iterrows(): - sessions = [] - num_events = len(session_data.events) - sessions += ([session_data.session] * num_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd9488d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 2, 3, 4]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2] * 2 + [3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe536e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonl_to_df(fn):\n",
    "    sessions = []\n",
    "    aids = []\n",
    "    tss = []\n",
    "    types = []\n",
    "\n",
    "    # lines: True => Read the file as a json object per line\n",
    "    # chunksize=100_000 => Return JsonReader object for iteration; If this is None, the file will be read into memory all at once.\n",
    "    # 100_000 == 100000, I guess _ is for easy view; each chunk will have 100,000 lines/objects\n",
    "    chunks = pd.read_json(fn, lines=True, chunksize=2) # you can change to chunksize=2 to experiment\n",
    "    \n",
    "    for chunk in chunks: # each chunk will have 2 items (if chunksize=2)\n",
    "        info = \"each item is a session, each session has two items 'session' and 'events'\"\n",
    "        info1 = \"each session has variable amount of events\"\n",
    "        pp(info, info1)\n",
    "        for row_idx, session_data in chunk.iterrows():\n",
    "            # each session_data is a pd.Series object and contain two columns: 'session' (int) and 'events' (list of dicts)\n",
    "            # each dict of 'events' has keys: 'aid', 'ts', 'type', 'clicks'\n",
    "            num_events = len(session_data.events) # total num of events of each session_data or each item in a chunk\n",
    "            sessions += ([session_data.session] * num_events) # sessions is a list contains the same session value for every event\n",
    "\n",
    "            pp(session_data.session, len(session_data.events), session_data.events[0], session_data.events[1])\n",
    "            for event in session_data.events: # each session_data.events actually have different num of events\n",
    "                aids.append(event['aid']) # aids is a list containing value of `aid` of every event\n",
    "                tss.append(event['ts']) # tss is a list containing value of `ts` of every event\n",
    "                types.append(type2id[event['type']]) # types is a list containing value of `type`|`id` of every event \n",
    "                # (`id` is created by Radek from above)\n",
    "        return\n",
    "    # now we can combine all the data info `session`, `aids`, `tss` and `types` into a DataFrame for each session_data\n",
    "    return pd.DataFrame(data={'session': sessions, 'aid': aids, 'ts': tss, 'type': types})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29338f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:13:26.20 LOG:\n",
      "11:13:26.21 .... info = \"each item is a session, each session has two items 'session' and 'events'\"\n",
      "11:13:26.21 .... info1 = 'each session has variable amount of events'\n",
      "11:13:26.21 LOG:\n",
      "11:13:26.22 .... session_data.session = 0\n",
      "11:13:26.22 .... len(session_data.events) = 276\n",
      "11:13:26.23 .... session_data.events[0] = {'aid': 1517085, 'ts': 1659304800025, 'type': 'clicks'}\n",
      "11:13:26.23 .... session_data.events[1] = {'aid': 1563459, 'ts': 1659304904511, 'type': 'clicks'}\n",
      "11:13:26.23 LOG:\n",
      "11:13:26.23 .... session_data.session = 1\n",
      "11:13:26.23 .... len(session_data.events) = 32\n",
      "11:13:26.23 .... session_data.events[0] = {'aid': 424964, 'ts': 1659304800025, 'type': 'carts'}\n",
      "11:13:26.23 .... session_data.events[1] = {'aid': 1492293, 'ts': 1659304852871, 'type': 'clicks'}\n"
     ]
    }
   ],
   "source": [
    "train_df = jsonl_to_df(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # train_df = jsonl_to_df('data/train.jsonl')\n",
    "# train_df = jsonl_to_df(train_path)\n",
    "# train_df.type = train_df.type.astype(np.uint8) # a tiny bit of further memory footprint optimization (original type is just int class)\n",
    "# # train_df.to_parquet('train.parquet', index=False)\n",
    "# train_df.to_csv('train.csv', index=False)\n",
    "\n",
    "# del train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b174bcff",
   "metadata": {},
   "source": [
    "### rd: src - recsys - otto - process data - jsonl_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb1a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonl_to_df(fn):\n",
    "    sessions = []\n",
    "    aids = []\n",
    "    tss = []\n",
    "    types = []\n",
    "\n",
    "    # lines: True => Read the file as a json object per line\n",
    "    # chunksize=100_000 => Return JsonReader object for iteration; If this is None, the file will be read into memory all at once.\n",
    "    # 100_000 == 100000, I guess _ is for easy view; each chunk will have 100,000 lines/objects\n",
    "    chunks = pd.read_json(fn, lines=True, chunksize=2) # you can change to chunksize=2 to experiment\n",
    "    \n",
    "    for chunk in chunks: # each chunk will have 2 items (if chunksize=2)\n",
    "        info = \"each item is a session, each session has two items 'session' and 'events'\"\n",
    "        info1 = \"each session has variable amount of events\"\n",
    "        pp(info, info1)\n",
    "        for row_idx, session_data in chunk.iterrows():\n",
    "            # each session_data is a pd.Series object and contain two columns: 'session' (int) and 'events' (list of dicts)\n",
    "            # each dict of 'events' has keys: 'aid', 'ts', 'type', 'clicks'\n",
    "            num_events = len(session_data.events) # total num of events of each session_data or each item in a chunk\n",
    "            sessions += ([session_data.session] * num_events) # sessions is a list contains the same session value for every event\n",
    "\n",
    "            pp(session_data.session, len(session_data.events), session_data.events[0], session_data.events[1])\n",
    "            for event in session_data.events: # each session_data.events actually have different num of events\n",
    "                aids.append(event['aid']) # aids is a list containing value of `aid` of every event\n",
    "                tss.append(event['ts']) # tss is a list containing value of `ts` of every event\n",
    "                types.append(type2id[event['type']]) # types is a list containing value of `type`|`id` of every event \n",
    "                # (`id` is created by Radek from above)\n",
    "        return\n",
    "    # now we can combine all the data info `session`, `aids`, `tss` and `types` into a DataFrame for each session_data\n",
    "    return pd.DataFrame(data={'session': sessions, 'aid': aids, 'ts': tss, 'type': types})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c547178e",
   "metadata": {},
   "source": [
    "### rd: recsys - otto - process data - 400MB parquet file takes up nearly 4GB ram on Kaggle\n",
    "\n",
    "see more detailed findings of mine in the discussion [here](https://www.kaggle.com/competitions/otto-recommender-system/discussion/363843#2024279)\n",
    "\n",
    "### rd: recsys - otto - process data - use parquet over csv, why and how - test_df.type = test_df.type.astype(np.uint8) - test_df.to_parquet('test.parquet', index=False) - test_df.to_csv('test.csv', index=False)\n",
    "\n",
    "Summary of technical features of parquet files\n",
    "- Apache Parquet is column-oriented and designed to provide efficient columnar storage compared to row-based file types such as CSV.\n",
    "- Parquet files were designed with complex nested data structures in mind.\n",
    "- Apache Parquet is designed to support very efficient compression and encoding schemes.\n",
    "- Apache Parquet generates lower storage costs for data files and maximizes the effectiveness of data queries with current cloud technologies such as Amazon Athena, Redshift Spectrum, BigQuery and Azure Data Lakes.\n",
    "- Licensed under the Apache license and available for any project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 49s, sys: 1.5 s, total: 2min 51s\n",
      "Wall time: 2min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# test_df = jsonl_to_df('data/test.jsonl')\n",
    "test_df = jsonl_to_df(test_path)\n",
    "test_df.type = test_df.type.astype(np.uint8)\n",
    "test_df.to_parquet('test.parquet', index=False)\n",
    "test_df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc85d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f6dbc2",
   "metadata": {},
   "source": [
    "### rd: recsys - otto - process data - use parquet to instead of jsonl or csv to save space on disk - os.path.getsize(path)\n",
    "\n",
    "for details see discussion [here](https://www.kaggle.com/code/radek1/howto-full-dataset-as-parquet-csv-files/comments#2025116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae995f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonl_to_df_type_str(fn):\n",
    "    sessions = []\n",
    "    aids = []\n",
    "    tss = []\n",
    "    types = []\n",
    "\n",
    "    chunks = pd.read_json(fn, lines=True, chunksize=100_000) \n",
    "    \n",
    "    for chunk in chunks:\n",
    "        for row_idx, session_data in chunk.iterrows():\n",
    "            num_events = len(session_data.events) \n",
    "            sessions += ([session_data.session] * num_events) \n",
    "            for event in session_data.events:\n",
    "                aids.append(event['aid']) \n",
    "                tss.append(event['ts']) \n",
    "                types.append(event['type'])\n",
    "\n",
    "    return pd.DataFrame(data={'session': sessions, 'aid': aids, 'ts': tss, 'type': types})\n",
    "\n",
    "%%time\n",
    "test_df_str = jsonl_to_df_type_str(test_path)\n",
    "test_df_str.to_parquet('test_keep_str.parquet', index=False)\n",
    "test_df_str.to_csv('test_keep_str.csv', index=False)\n",
    "\n",
    "test_df = jsonl_to_df(test_path)\n",
    "test_df.to_parquet('test_str2int.parquet', index=False)\n",
    "test_df.to_csv('test0_str2int.csv', index=False)\n",
    "\n",
    "test_df.type = test_df.type.astype(np.uint8)\n",
    "test_df.to_parquet('test_str2uint8.parquet', index=False)\n",
    "test_df.to_csv('test_str2uint8.csv', index=False)\n",
    "\n",
    "def filesize(path): # path: str or path\n",
    "    import os\n",
    "    return os.path.getsize(path)\n",
    "\n",
    "[(\"jsonl=>{}\".format(path), filesize(path)) for path in ['../input/otto-recommender-system/test.jsonl','test_keep_str.parquet', 'test_keep_str.csv', 'test_str2int.parquet', 'test0_str2int.csv', 'test_str2uint8.parquet', 'test_str2uint8.csv']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61587dee",
   "metadata": {},
   "source": [
    "It seems that by converting jsonl to csv can half the size, to parquet can quarter the size;\n",
    "\n",
    "Converting type from string to int using type2id can save 34MB which is about 10% size of original test dataset during conversion from jsonl to csv; but it has no observable effect during convertion to parquet file.\n",
    "\n",
    "Although converting type further from int to uint8 in a parquet file can shrink the size, but the amount is barely observable, about 200byte.\n",
    "\n",
    "So, it seems that conversion from jsonl to parquet alone does the most heavy lifting in reducing size, converting from string to int and then uint8 is helpful but in this case has no significant effect in reducing the size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69b566e",
   "metadata": {},
   "source": [
    "### rd: recsys - otto - process data - use `uint8` instead of `int` or `str` to reduce RAM usage by 9 times - test_df.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d272789",
   "metadata": {},
   "source": [
    "read the discussion [here](https://www.kaggle.com/code/radek1/howto-full-dataset-as-parquet-csv-files/comments#2025129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13681cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.memory_usage()\n",
    "test_df_str.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0a4f7",
   "metadata": {},
   "source": [
    "### jn: process_data revisited and get the name straight for search (done) /2022-11-14"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
