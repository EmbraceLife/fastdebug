{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import my utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try: import fastkaggle\n",
    "except ModuleNotFoundError:\n",
    "    os.system(\"pip install -Uq fastkaggle\")\n",
    "\n",
    "from fastkaggle import *\n",
    "\n",
    "# use fastdebug.utils \n",
    "if iskaggle: os.system(\"pip install nbdev snoop\")\n",
    "\n",
    "if iskaggle:\n",
    "    path = \"../input/fastdebugutils0\"\n",
    "    import sys\n",
    "    sys.path\n",
    "    sys.path.insert(1, path)\n",
    "    import utils as fu\n",
    "    from utils import *\n",
    "else: \n",
    "    from fastdebug.utils import *\n",
    "    import fastdebug.utils as fu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate ReRank Model using Handcrafted Rules\n",
    "https://www.kaggle.com/code/cdeotte/candidate-rerank-model-lb-0-575?scriptVersionId=111214204"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rd: recsys - otto - candidate rerank - what is candidate rerank model\n",
    "In this notebook, we present a \"candidate rerank\" model using handcrafted rules. \n",
    "\n",
    "### rd: recsys - otto - candidate rerank - how to improve this candidate rerank model\n",
    "We can improve this model by engineering features, merging them unto items and users, and training a reranker model (such as XGB) to choose our final 20. \n",
    "\n",
    "### rd: recsys - otto - candidate rerank - how to further tune and improve this notebook\n",
    "\n",
    "we should build a local CV scheme to experiment new logic and/or models.\n",
    "\n",
    "UPDATE: I published a notebook to compute validation score [here][https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-565?scriptVersionId=111214251] using Radek's scheme described [here][https://www.kaggle.com/competitions/otto-recommender-system/discussion/364991].\n",
    "\n",
    "### rd: recsys - otto - candidate rerank - what does a session mean in this competition\n",
    "\n",
    "Note in this competition, a \"session\" actually means a unique \"user\". So our task is to predict what each of the `1,671,803` test \"users\" (i.e. \"sessions\") will do in the future. For each test \"user\" (i.e. \"session\") we must predict what they will `click`, `cart`, and `order` during the remainder of the week long test period.\n",
    "\n",
    "## rd: recsys - otto - candidate rerank - The candidate rerank model of this notebook\n",
    "\n",
    "### rd: recsys - otto - candidate rerank - Step 1 - Generate Candidates from 5 sources\n",
    "For each test user, we generate possible choices, i.e. candidates. In this notebook, we generate candidates from 5 sources:\n",
    "* User history of clicks, carts, orders\n",
    "* Most popular 20 clicks, carts, orders during test week\n",
    "* Co-visitation matrix of click/cart/order to cart/order with type weighting\n",
    "* Co-visitation matrix of cart/order to cart/order called buy2buy\n",
    "* Co-visitation matrix of click/cart/order to clicks with time weighting\n",
    "\n",
    "### rd: recsys - otto - candidate rerank - Step 2 - ReRank and Choose 20 - what are those rules and what are their relationship between the candidates and XGBoost model\n",
    "Given the list of candidates, we must select 20 to be our predictions. In this notebook, we do this with a set of handcrafted rules. We can improve our predictions by training an XGBoost model to select for us. Our handcrafted rules give priority to:\n",
    "* Most recent previously visited items\n",
    "* Items previously visited multiple times\n",
    "* Items previously in cart or order\n",
    "* Co-visitation matrix of cart/order to cart/order\n",
    "* Current popular items\n",
    "\n",
    "![candidate rerank model visualization](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Nov-2022/c_r_model.png)\n",
    "  \n",
    "## rd: recsys - otto - candidate rerank - what are learnt from other kagglers\n",
    "We thank many Kagglers who have shared ideas. We use co-visitation matrix idea from Vladimir [here][1]. \n",
    "\n",
    "We use groupby sort logic from Sinan in comment section [here][4]. \n",
    "\n",
    "We use duplicate prediction removal logic from Radek [here][5]. \n",
    "\n",
    "We use multiple visit logic from Pietro [here][2]. \n",
    "\n",
    "We use type weighting logic from Ingvaras [here][3]. \n",
    "\n",
    "We use leaky test data from my previous notebook [here][4]. \n",
    "\n",
    "And some ideas may have originated from Tawara [here][6] and KJ [here][7]. \n",
    "\n",
    "We use Colum2131's parquets [here][8]. \n",
    "\n",
    "Above image is from Ravi's discussion about candidate rerank models [here][9]\n",
    "\n",
    "[1]: https://www.kaggle.com/code/vslaykovsky/co-visitation-matrix\n",
    "[2]: https://www.kaggle.com/code/pietromaldini1/multiple-clicks-vs-latest-items\n",
    "[3]: https://www.kaggle.com/code/ingvarasgalinskas/item-type-vs-multiple-clicks-vs-latest-items\n",
    "[4]: https://www.kaggle.com/code/cdeotte/test-data-leak-lb-boost\n",
    "[5]: https://www.kaggle.com/code/radek1/co-visitation-matrix-simplified-imprvd-logic\n",
    "[6]: https://www.kaggle.com/code/ttahara/otto-mors-aid-frequency-baseline\n",
    "[7]: https://www.kaggle.com/code/whitelily/co-occurrence-baseline\n",
    "[8]: https://www.kaggle.com/datasets/columbia2131/otto-chunk-data-inparquet-format\n",
    "[9]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364721\n",
    "[10]: https://www.kaggle.com/cdeotte/compute-validation-score-cv-564\n",
    "[11]: https://www.kaggle.com/competitions/otto-recommender-system/discussion/364991"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rd: recsys - otto - candidate rerank - changes and improvements on difference versions\n",
    "Below are notes about versions:\n",
    "* **Version 1 LB 0.573** Uses popular ideas from public notebooks and adds additional co-visitation matrices and additional logic. Has CV `0.563`. See validation notebook version 2 [here][1].\n",
    "* **Version 2 LB 573** Refactor logic for `suggest_buys(df)` to make it clear how new co-visitation matrices are reranking the candidates by adding to candidate weights. Also new logic boosts CV by `+0.0003`. Also LB is slightly better too. See validation notebook version 3 [here][1]\n",
    "* **Version 3** is the same as version 2 but 1.5x faster co-visitation matrix computation!\n",
    "* **Version 4 LB 575** Use top20 for clicks and top15 for carts and buys (instead of top40 and top40). This boosts CV `+0.0015` hooray! New CV is `0.5647`. See validation version 5 [here][1]\n",
    "* **Version 5** is the same as version 4 but 2x faster co-visitation matrix computation! (and 3x faster than version 1)\n",
    "* **Version 6** Stay tuned for more versions...\n",
    "\n",
    "[1]: https://www.kaggle.com/code/cdeotte/compute-validation-score-cv-564"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rd: recsys - otto - candidate rerank - Step 1 - Candidate Generation with 3 co-visitation matrices and RAPIDS cuDF GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rd: recsys - otto - candidate rerank - what are the 3 co-visitation matrices\n",
    "For candidate generation, we build three co-visitation matrices. \n",
    "\n",
    "One computes the popularity of cart/order given a user's previous click/cart/order. We apply type weighting to this matrix. \n",
    "\n",
    "One computes the popularity of cart/order given a user's previous cart/order. We call this \"buy2buy\" matrix. \n",
    "\n",
    "One computes the popularity of clicks given a user previously click/cart/order.  We apply time weighting to this matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rd: recsys - otto - candidate rerank - what are RAPIDS cuDF GPU for\n",
    "We will use RAPIDS cuDF GPU to compute these matrices quickly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rd: recsys - otto - candidate rerank - imports needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VER = 5\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "import cudf, itertools\n",
    "print('We will use RAPIDS version',cudf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rd: recsys - otto - candidate rerank - Tricks to Compute Three Co-visitation Matrices super fast\n",
    "We will compute 3 co-visitation matrices using RAPIDS cuDF on GPU. This is 30x faster than using Pandas CPU like other public notebooks! \n",
    "\n",
    "For maximum speed, set the variable `DISK_PIECES` to the smallest number possible based on the GPU you are using without incurring memory errors. If you run this code offline with 32GB GPU ram, then you can use `DISK_PIECES = 1` and compute each co-visitation matrix in almost 1 minute! Kaggle's GPU only has 16GB ram, so we use `DISK_PIECES = 4` and it takes an amazing 3 minutes each! \n",
    "\n",
    "Below are some of the tricks to speed up computation (question: does this notebook do all these below?)\n",
    "* Use RAPIDS cuDF GPU instead of Pandas CPU\n",
    "* Read disk once and save in CPU RAM for later GPU multiple use\n",
    "* Process largest amount of data possible on GPU at one time\n",
    "* Merge data in two stages. Multiple small to single medium. Multiple medium to single large.\n",
    "* Write result as parquet instead of dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rd: recsys - otto - candidate rerank - How to Cache data on CPU before processing on GPU\n",
    "\n",
    "load all files into dataframes whose sizes are reduced by changing its dtypes and save those dataframes into a dict in CPU RAM for access later on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# CACHE FUNCTIONS\n",
    "def read_file(f):\n",
    "    return cudf.DataFrame( data_cache[f] )\n",
    "def read_file_to_cache(f):\n",
    "    df = pd.read_parquet(f)\n",
    "    df.ts = (df.ts/1000).astype('int32')\n",
    "    df['type'] = df['type'].map(type_labels).astype('int8')\n",
    "    return df\n",
    "\n",
    "# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\n",
    "data_cache = {}\n",
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}\n",
    "files = glob.glob('../input/otto-chunk-data-inparquet-format/*_parquet/*') # all parquet filenames into a list\n",
    "for f in files: data_cache[f] = read_file_to_cache(f)\n",
    "\n",
    "# CHUNK PARAMETERS\n",
    "READ_CT = 5 # based on the notebook, it means a number of files (5) to process each time inside a CHUNK of files (20). \n",
    "# If so, why READ_CT not READ_NF?\n",
    "# question: Where does 6 come from? if it relates to READ_CT then why not use READ_CT + 1?\n",
    "CHUNK = int( np.ceil( len(files)/6 )) # CHUNK == 20 here\n",
    "print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) \"Carts Orders\" Co-visitation Matrix - Type Weighted\n",
    "### rd: recsys - otto - candidate rerank - what does DISK_PIECES = 4 mean? \n",
    "\n",
    "yes we iterate through 4 times because the GPU memory in Kaggle's P100 GPU can only handle making 1/4th of the co-visitation matrix at a time. The variable DISK_PIECES refers to how many output files we make when producing one co-visitation matrix.\n",
    "\n",
    "As you make new different co-visitation matrices and/or use a different GPU offline with more GPU RAM, you can try to use a smaller DISK_PIECES. For me, i use DISK_PIECES=1 offline and everything is much faster.\n",
    "\n",
    "### rd: recsys - otto - candidate rerank - why use only the last 30 aids not all of them? \n",
    "\n",
    "This is mainly done to save memory and make things faster. You can try using all data to see if it improves CV score. Note if you use more data, you may need to increase DISK_PIECES to avoid memory error.\n",
    "\n",
    "Using more data might not necessarily help because the last 30 rows of each user is the more recent data and the more recent data might help predict the future better.\n",
    "\n",
    "The main reason is that it uses less memory. However, note that a users' tail is their most recent and meaningful data to help predict the future and capture recent co-visitation trends.\n",
    "\n",
    "You can experiment with changing tail and seeing how it affects CV and LB. Note if you increase tail, you may need to increase DISK_PIECES to avoid memory error.\n",
    "\n",
    "\n",
    "### rd: recsys - otto - candidate rerank - why type_weight is set to {0:1, 1:6, 2:3} vs {0:1, 1:3, 2:6} \n",
    "\n",
    "I'm not trying to align with weights in competition metric. These weights were chosen by Ingvaras in his notebook [here](https://www.kaggle.com/code/ingvarasgalinskas/item-type-vs-multiple-clicks-vs-latest-items). We are trying to determine how important previous behavior is.\n",
    "\n",
    "The basic idea is this. We want to predict future behavior, so the question is what is more important \"someone previously clicked an item OR someone previously put an item in their cart\". I would say the second is more important. That means the user will most likely click this item again or order this item. So we give lots of weight to previous behavior of \"cart\".\n",
    "\n",
    "Next, we wonder what is more important \"someone previously put an item in their cart OR someone previously ordered an item\". In both cases, the user might buy the item. But it is more likely that a user will buy an item if they put it in their cart versus buy an item if they have already bought the item. People do buy items multiple times so previously buying an item is more important than previously clicking an item (when predicting a future purchase).\n",
    "\n",
    "\n",
    "### rd: recsys - otto - candidate rerank - how to use cudf in kaggle\n",
    "\n",
    "When you use a Kaggle notebook with GPU, then RAPIDS comes already installed by Kaggle.\n",
    "\n",
    "Note that Kaggle uses version 21.10.01 which is one year old but still works well. Due to drivers and Python version, i don't think we can install the latest RAPIDS into Kaggle.\n",
    "\n",
    "Here is one important bug to be aware of. When using df['n'] = df.groupby(COL1)[COL2].cumcount() make sure that you have reset index with df = df.reset_index(drop=True) before the cumcount call otherwise, it does cumcount incorrectly. This is fixed in newer version of RAPIDS cuDF. Also if you use df.groupby(COL1)[COL2].transform(FUNC) then we need to reset index first too.\n",
    "\n",
    "\n",
    "### rd: recsys - otto - candidate rerank - why and how to computer only a quarter of dataframe for memory - df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "\n",
    "\n",
    "This line does split the dataframe into 4 parts. If we don't break the construction of the dataframe into 4 parts, then we will get GPU memory error. How would you split the dataframe into 4 parts differently?\n",
    "\n",
    "(by doing it the way above, all the dictionary entries for top_20[aid_x][ANYTHING] will be together and saved in the dictionary piece together.)\n",
    "\n",
    "### rd: recsys - otto - candidate rerank - why READ_CT = 5, why splitting all files into 6 parts (results in CHUNK) \n",
    "(based on my current understanding, READ_CT=5 can be experimented first on Kaggle GPU to figure out, and then we can experiment to see how big of merged dataframe can GPU handle to figure out how many more parts to merge in the end. To read and comtemplate everyday)\n",
    "\n",
    "Here's a diagram of what's happening. We can adjust READ_CT to the biggest number our GPU will allow before memory error because we always want GPUs doing as much work as possible (without memory error). That is step 1.\n",
    "\n",
    "After processing, we have a small dataframe of results (this is blue square in left column of diagram below). If READ_CT is 5 and there are 146 files on disk, then we will have 30 small dataframe. We might think, \"why not just merge all 30 dataframes together?\".\n",
    "\n",
    "When we merge \"small dataframe A\" into \"small dataframe B\" and both have 1e6 rows, then it might take 10 seconds. If \"dataframe A\" has 10e6 rows and \"dataframe B\" has 1e6 rows, it might take 40 seconds. And if \"dataframe A\" has 10e6 rows and \"dataframe B\" has 10e6 rows, it might take 50 seconds. Therefore we do not change CHUNK variable. Given number of rows of train data, I have found that we should merge CHUNK = 1/6 of all data small dataframes into large dataframes. Then merge the large dataframes together. The choice of 6 maximizes speed for our competition data and does not need to be changed.\n",
    "\n",
    "![](https://raw.githubusercontent.com/cdeotte/Kaggle_Images/main/Nov-2022/merge2.png)\n",
    "\n",
    "### rd: recsys - otto - candidate rerank - How do we know whether we should care about whether aid_y comes before or after aid_x within a day\n",
    "\n",
    "This is for you to explore and find out :-) \n",
    "\n",
    "Co-visitation matrices are pairs of items: ITEM_A to ITEM_B. Depending on what type of items ITEM_A and ITEM_B are, i have found different things are better. When ITEM_A and ITEM_B are both the same, using both forward and backward is better (because when exploring what sneakers to buy the order you click sneakers doesn't matter). However if ITEM_A is \"click\" and ITEM_B is \"cart\" or \"order\" then forward in time is better. (Because why click for sneakers after you already bought sneakers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "type_weight = {0:1, 1:6, 2:3}\n",
    "\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 4 # divide the kaggle disk into 4 parts or num of output files used to form one co-visitation matrix\n",
    "SIZE = 1.86e6/DISK_PIECES # 1.855.603 aids, each part of disk should handle SIZE num of aids # question: SIZE of what? size of a quarter of df\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES): \n",
    "    print()\n",
    "    print('### DISK PART',PART+1) # which part of the disk we are using at the moment\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6): # outer chunks have 6 parts to merge\n",
    "        a = j*CHUNK # the starting file index for a part of the disk\n",
    "        b = min( (j+1)*CHUNK, len(files) ) # the ending file index for a part of the disk\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...') # split the 20 files into groups and 5 files each\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT): # inner chunk has 20 files, and we take 5 files together in one step\n",
    "            # READ FILE: convert a READ_CT number of files into a list of 5 dataframes \n",
    "            df = [read_file(files[k])] # first file \n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) ) # start to add second file\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0) # stack the dataframes vertically\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False]) # sort df from small session id to large, then inside a sesion sort from large ts to small\n",
    "            # USE TAIL OF SESSION: use the latest 30 aids \n",
    "            df = df.reset_index(drop=True) # make sure the index is clean (because of using an old cudf version by kaggle)\n",
    "            df['n'] = df.groupby('session').cumcount() # give index to the aids of each session\n",
    "            df = df.loc[df.n<30].drop('n',axis=1) # keep the rows where n is < 30 and remove the column 'n'\n",
    "            # CREATE PAIRS: aid pair occurred in a day and must not be the same (question: it does not matter whether aid_y can come before or after aid_x?)\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # no more use for ts, so removed later\n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS: use only a quarter of the dataframe to compute\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            # ASSIGN WEIGHTS: \n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y']) # drop rows which are the same on those columns\n",
    "            df['wgt'] = df.type_y.map(type_weight) # add a column which give specific weights to specific types\n",
    "            df = df[['aid_x','aid_y','wgt']] # keep only 3 columns\n",
    "            df.wgt = df.wgt.astype('float32') # make weight a float 32 not int\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum() # accumulate weights for each pair (use df.sort_values())\n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df # if we just start in the inner chunk\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0) # add the next 5 files dataframe\n",
    "            print(k,', ',end='') # track the index of the files be processed\n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2 # if the outer chunk just started\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0) # add the next 20 files dataframe merged from inner chunks\n",
    "        del tmp2, df # delete to save RAM\n",
    "        gc.collect() # Use this method to force the system to try to reclaim the maximum amount of available memory.\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index() # get index clean and restore normal index\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False]) # under each aid_x, order the rows by wgt from high to low\n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True) # index is reordered due to sorting, drop it and restore the normal index\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount() # use column n to assign the index to aid_y under each aid_x group\n",
    "    tmp = tmp.loc[tmp.n<15].drop('n',axis=1) # keep the first 15 rows of each aid_x group\n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'top_15_carts_orders_v{VER}_{PART}.pqt') # this is one part of the dataframe and save to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) \"Buy2Buy\" Co-visitation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 1\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<15].drop('n',axis=1)\n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'top_15_buy2buy_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) \"Clicks\" Co-visitation Matrix - Time Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 4\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<20].drop('n',axis=1)\n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'top_20_clicks_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREE MEMORY\n",
    "del data_cache, tmp\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - ReRank (choose 20) using handcrafted rules\n",
    "For description of the handcrafted rules, read this notebook's intro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test():    \n",
    "    dfs = []\n",
    "    for e, chunk_file in enumerate(glob.glob('../input/otto-chunk-data-inparquet-format/test_parquet/*')):\n",
    "        chunk = pd.read_parquet(chunk_file)\n",
    "        chunk.ts = (chunk.ts/1000).astype('int32')\n",
    "        chunk['type'] = chunk['type'].map(type_labels).astype('int8')\n",
    "        dfs.append(chunk)\n",
    "    return pd.concat(dfs).reset_index(drop=True) #.astype({\"ts\": \"datetime64[ms]\"})\n",
    "\n",
    "test_df = load_test()\n",
    "print('Test data has shape',test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def pqt_to_dict(df):\n",
    "    return df.groupby('aid_x').aid_y.apply(list).to_dict()\n",
    "# LOAD THREE CO-VISITATION MATRICES\n",
    "top_20_clicks = pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_0.pqt') )\n",
    "for k in range(1,DISK_PIECES): \n",
    "    top_20_clicks.update( pqt_to_dict( pd.read_parquet(f'top_20_clicks_v{VER}_{k}.pqt') ) )\n",
    "top_20_buys = pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_0.pqt') )\n",
    "for k in range(1,DISK_PIECES): \n",
    "    top_20_buys.update( pqt_to_dict( pd.read_parquet(f'top_15_carts_orders_v{VER}_{k}.pqt') ) )\n",
    "top_20_buy2buy = pqt_to_dict( pd.read_parquet(f'top_15_buy2buy_v{VER}_0.pqt') )\n",
    "\n",
    "# TOP CLICKS AND ORDERS IN TEST\n",
    "top_clicks = test_df.loc[test_df['type']=='clicks','aid'].value_counts().index.values[:20]\n",
    "top_orders = test_df.loc[test_df['type']=='orders','aid'].value_counts().index.values[:20]\n",
    "\n",
    "print('Here are size of our 3 co-visitation matrices:')\n",
    "print( len( top_20_clicks ), len( top_20_buy2buy ), len( top_20_buys ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type_weight_multipliers = {'clicks': 1, 'carts': 6, 'orders': 3}\n",
    "type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n",
    "\n",
    "def suggest_clicks(df):\n",
    "    # USER HISTORY AIDS AND TYPES\n",
    "    aids=df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids)>=20:\n",
    "        weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n",
    "        return sorted_aids\n",
    "    # USE \"CLICKS\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_20_clicks[aid] for aid in unique_aids if aid in top_20_clicks]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(20) if aid2 not in unique_aids]    \n",
    "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "    # USE TOP20 TEST CLICKS\n",
    "    return result + list(top_clicks)[:20-len(result)]\n",
    "\n",
    "def suggest_buys(df):\n",
    "    # USER HISTORY AIDS AND TYPES\n",
    "    aids=df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    # UNIQUE AIDS AND UNIQUE BUYS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
    "    df = df.loc[(df['type']==1)|(df['type']==2)]\n",
    "    unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids)>=20:\n",
    "        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "        aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
    "        for aid in aids3: aids_temp[aid] += 0.1\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(20)]\n",
    "        return sorted_aids\n",
    "    # USE \"CART ORDER\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[top_20_buys[aid] for aid in unique_aids if aid in top_20_buys]))\n",
    "    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "    aids3 = list(itertools.chain(*[top_20_buy2buy[aid] for aid in unique_buys if aid in top_20_buy2buy]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2+aids3).most_common(20) if aid2 not in unique_aids] \n",
    "    result = unique_aids + top_aids2[:20 - len(unique_aids)]\n",
    "    # USE TOP20 TEST ORDERS\n",
    "    return result + list(top_orders)[:20-len(result)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Submission CSV\n",
    "Inferring test data with Pandas groupby is slow. We need to accelerate the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred_df_clicks = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
    "    lambda x: suggest_clicks(x)\n",
    ")\n",
    "\n",
    "pred_df_buys = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).apply(\n",
    "    lambda x: suggest_buys(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n",
    "orders_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\n",
    "carts_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\n",
    "pred_df.columns = [\"session_type\", \"labels\"]\n",
    "pred_df[\"labels\"] = pred_df.labels.apply(lambda x: \" \".join(map(str,x)))\n",
    "pred_df.to_csv(\"submission.csv\", index=False)\n",
    "pred_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
