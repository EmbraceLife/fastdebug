{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# process_data_OTTO\n\nOriginal code is given by @radek1 to make the original OTTO dataset easier to process and faster to access.\n\nThis notebook is to experiment in order to understand what the original code does.","metadata":{}},{"cell_type":"markdown","source":"## rd: recsys - otto - process data - How to convert dataset from jsonl file into parquet file to save disk tremendously, and convert type column from string to uint8 and ts column from int64 to int32 by dividing 1000 first to reduce RAM usage significantly","metadata":{}},{"cell_type":"code","source":"# make sure fastkaggle is install and imported\nimport os","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try: import fastkaggle\nexcept ModuleNotFoundError:\n    os.system(\"pip install -Uq fastkaggle\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastkaggle import *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use fastdebug.utils \nif iskaggle: os.system(\"pip install nbdev snoop\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if iskaggle:\n    path = \"../input/fastdebugutils0\"\n    import sys\n    sys.path\n    sys.path.insert(1, path)\n    import utils as fu\n    from utils import *\nelse: \n    from fastdebug.utils import *\n    import fastdebug.utils as fu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comp = \"otto-recommender-system\"\npath = setup_comp(comp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(path.ls())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = path.ls()[2]\ntest_path = path.ls()[1]\ntrain_path\ntest_path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### rd: recsys - otto - process data - create vocab or map between id and type using dict and list - id2type = ['clicks', 'carts', 'orders'] - type2id = {a: i for i, a in enumerate(id2type)}","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id2type = ['clicks', 'carts', 'orders'] # I have analyzed the data\n                                          # and so I know we can expect these event types\ntype2id = {a: i for i, a in enumerate(id2type)}\n\nid2type, type2id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.to_pickle(id2type, 'id2type.pkl')\npd.to_pickle(type2id, 'type2id.pkl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### rd: recsys - otto - process data - detail annotated source code of json_to_df","metadata":{}},{"cell_type":"code","source":"def jsonl_to_df(fn):\n    sessions = []\n    aids = []\n    tss = []\n    types = []\n\n    # lines: True => Read the file as a json object per line\n    # chunksize=100_000 => Return JsonReader object for iteration; If this is None, the file will be read into memory all at once.\n    # 100_000 == 100000, I guess _ is for easy view; each chunk will have 100,000 lines/objects\n    chunks = pd.read_json(fn, lines=True, chunksize=2) # you can change to chunksize=2 to experiment\n    \n    for chunk in chunks: # each chunk will have 2 items (if chunksize=2)\n        info = \"each item is a session, each session has two items 'session' and 'events'\"\n        info1 = \"each session has variable amount of events\"\n        pp(info, info1)\n        for row_idx, session_data in chunk.iterrows():\n            # each session_data is a pd.Series object and contain two columns: 'session' (int) and 'events' (list of dicts)\n            # each dict of 'events' has keys: 'aid', 'ts', 'type', 'clicks'\n            num_events = len(session_data.events) # total num of events of each session_data or each item in a chunk\n            sessions += ([session_data.session] * num_events) # sessions is a list contains the same session value for every event\n\n            pp(session_data.session, len(session_data.events), session_data.events[0], session_data.events[1])\n            for event in session_data.events: # each session_data.events actually have different num of events\n                aids.append(event['aid']) # aids is a list containing value of `aid` of every event\n                tss.append(event['ts']) # tss is a list containing value of `ts` of every event\n                types.append(type2id[event['type']]) # types is a list containing value of `type`|`id` of every event \n                # (`id` is created by Radek from above)\n        return\n    # now we can combine all the data info `session`, `aids`, `tss` and `types` into a DataFrame for each session_data\n    return pd.DataFrame(data={'session': sessions, 'aid': aids, 'ts': tss, 'type': types})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jsonl_to_df_type_str(fn):\n    sessions = []\n    aids = []\n    tss = []\n    types = []\n\n    chunks = pd.read_json(fn, lines=True, chunksize=100_000) \n    \n    for chunk in chunks:\n        for row_idx, session_data in chunk.iterrows():\n            num_events = len(session_data.events) \n            sessions += ([session_data.session] * num_events) \n            for event in session_data.events:\n                aids.append(event['aid']) \n                tss.append(event['ts']) \n#                 types.append(type2id[event['type']])\n                types.append(event['type']) \n                \n    return pd.DataFrame(data={'session': sessions, 'aid': aids, 'ts': tss, 'type': types})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame.memory_usage??","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### rd: recsys - otto - process data - chunks = pd.read_json(fn, lines=True, chunksize=100_000) - for chunk in chunks: - for row_idx, session_data in chunk.iterrows(): - session_data.session - for event in session_data.events: - aids.append(event['aid']) - tss.append(event['ts'])","metadata":{}},{"cell_type":"code","source":"def jsonl_to_df(fn):\n    sessions = []\n    aids = []\n    tss = []\n    types = []\n\n    chunks = pd.read_json(fn, lines=True, chunksize=100_000) \n    \n    for chunk in chunks:\n        for row_idx, session_data in chunk.iterrows():\n            num_events = len(session_data.events) \n            sessions += ([session_data.session] * num_events) \n            for event in session_data.events:\n                aids.append(event['aid']) \n                tss.append(event['ts']) \n                types.append(type2id[event['type']])\n\n    return pd.DataFrame(data={'session': sessions, 'aid': aids, 'ts': tss, 'type': types})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### rd: recsys - otto - process data - and check RAM of a df and save df into parquet or csv file - test_df_str.memory_usage() - test_df_str.to_parquet('test_keep_str.parquet', index=False) - test_df_str.to_csv('test_keep_str.csv', index=False)","metadata":{}},{"cell_type":"code","source":"%%time\ntest_df_str = jsonl_to_df_type_str(test_path)\ntest_df_str.memory_usage()\ntest_df_str.to_parquet('test_keep_str.parquet', index=False)\ntest_df_str.to_csv('test_keep_str.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### rd: recsys - otto - process data - How much RAM does convert string to uint8 save? - test_df.type = test_df.type.astype(np.uint8)","metadata":{}},{"cell_type":"code","source":"%%time\ntest_df = jsonl_to_df(test_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.memory_usage()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.type = test_df.type.astype(np.uint8)\ntest_df.memory_usage()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.memory_usage()\ntest_df_str.memory_usage()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### rd: recsys - otto - process data - convert `ts` from int64 to int32 without `/1000` will lose a lot of info - (test_df_updated.ts/1000).astype(np.int32)","metadata":{}},{"cell_type":"code","source":"test_df_updated = jsonl_to_df(test_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[the difference between int64, int32](http://www.ece.northwestern.edu/local-apps/matlabhelp/techdoc/ref/int8.html)","metadata":{}},{"cell_type":"code","source":"test_df_updated.ts.dtype","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df_updated.ts[0], test_df_updated.ts.astype(np.int32)[0] \n# without dividing 1000, ts is too large to be contained by int 32, directly convert to int32 can result in info loss ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1_661_724_000_278 is beyond int32 range, but 1_661_724_000 (divided by 1000) is within int32\ntest_df_updated.ts[0],test_df_updated.ts[0]/1000, (test_df_updated.ts/1000).astype(np.int32)[0] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### rd: recsys - otto - process data - dividing ts by 1000 only affect on milisecond accuracy not second accuracy - datetime.datetime.fromtimestamp((test_df_updated.ts/1000).astype(np.int32)[100])","metadata":{}},{"cell_type":"code","source":"import datetime","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datetime.date.fromtimestamp(test_df_updated.ts[100]/1000)\ndatetime.datetime.fromtimestamp(test_df_updated.ts[100]/1000) # the milisecond is the last item 278000\ndatetime.datetime.fromtimestamp((test_df_updated.ts/1000).astype(np.int32)[100]) # the milisecond is removed","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### rd: recsys - otto - process data - How much RAM can be saved by dividing `ts` by 1000 - test_df_updated.ts = (test_df_updated.ts / 1000).astype(np.int32) ","metadata":{}},{"cell_type":"code","source":"test_df_updated = jsonl_to_df(test_path)\ntest_df_updated.type = test_df_updated.type.astype(np.uint8)\ntest_df_updated.ts = (test_df_updated.ts / 1000).astype(np.int32) \ntest_df_updated.memory_usage() # the RAM used by `ts` is halved","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### rd: recsys - otto - process data - use parquet to instead of jsonl or csv to save space on disk - os.path.getsize(path)\n\nfor details see discussion [here](https://www.kaggle.com/code/radek1/howto-full-dataset-as-parquet-csv-files/comments#2025116)","metadata":{}},{"cell_type":"code","source":"def jsonl_to_df_type_str(fn):\n    sessions = []\n    aids = []\n    tss = []\n    types = []\n\n    chunks = pd.read_json(fn, lines=True, chunksize=100_000) \n    \n    for chunk in chunks:\n        for row_idx, session_data in chunk.iterrows():\n            num_events = len(session_data.events) \n            sessions += ([session_data.session] * num_events) \n            for event in session_data.events:\n                aids.append(event['aid']) \n                tss.append(event['ts']) \n                types.append(event['type'])\n\n    return pd.DataFrame(data={'session': sessions, 'aid': aids, 'ts': tss, 'type': types})\n\n%%time\ntest_df_str = jsonl_to_df_type_str(test_path)\ntest_df_str.to_parquet('test_keep_str.parquet', index=False)\ntest_df_str.to_csv('test_keep_str.csv', index=False)\n\ntest_df = jsonl_to_df(test_path)\ntest_df.to_parquet('test_str2int.parquet', index=False)\ntest_df.to_csv('test0_str2int.csv', index=False)\n\ntest_df.type = test_df.type.astype(np.uint8)\ntest_df.to_parquet('test_str2uint8.parquet', index=False)\ntest_df.to_csv('test_str2uint8.csv', index=False)\n\ndef filesize(path): # path: str or path\n    import os\n    return os.path.getsize(path)\n\n[(\"jsonl=>{}\".format(path), filesize(path)) for path in ['../input/otto-recommender-system/test.jsonl',\\\n                                                         'test_keep_str.parquet', 'test_keep_str.csv', \\\n                                                         'test_str2int.parquet', 'test0_str2int.csv', \\\n                                                         'test_str2uint8.parquet', 'test_str2uint8.csv']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that by converting jsonl to csv can half the size, to parquet can quarter the size; \n\nConverting `type` from string to int using `type2id` can save 34MB which is about 10% size of original test dataset during conversion from jsonl to csv; but it has no observable effect during convertion to parquet file.\n\nAlthough converting `type` further from `int` to `uint8` in a parquet file can shrink the size, but the amount is barely observable, about 200byte.\n\nSo, it seems that conversion from jsonl to parquet alone does the most heavy lifting in reducing size, converting from string to int and then uint8 is helpful but in this case has no significant effect in reducing the size. ","metadata":{}},{"cell_type":"markdown","source":"### rd: recsys - otto - process data - 400MB parquet file takes up nearly 4GB ram on Kaggle\n\nsee more detailed findings of mine in the discussion [here](https://www.kaggle.com/competitions/otto-recommender-system/discussion/363843#2024279)\n\n### rd: recsys - otto - process data - use parquet over csv, why and how - test_df.type = test_df.type.astype(np.uint8) - test_df.to_parquet('test.parquet', index=False) - test_df.to_csv('test.csv', index=False)\n\nSummary of technical features of parquet files\n- Apache Parquet is column-oriented and designed to provide efficient columnar storage compared to row-based file types such as CSV.\n- Parquet files were designed with complex nested data structures in mind.\n- Apache Parquet is designed to support very efficient compression and encoding schemes.\n- Apache Parquet generates lower storage costs for data files and maximizes the effectiveness of data queries with current cloud technologies such as Amazon Athena, Redshift Spectrum, BigQuery and Azure Data Lakes.\n- Licensed under the Apache license and available for any project.","metadata":{}},{"cell_type":"markdown","source":"### jn: process_data revisited and get the name straight for search (done) /2022-11-14","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}